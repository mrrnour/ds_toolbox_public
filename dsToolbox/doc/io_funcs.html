<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>io_funcs API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>io_funcs</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import os ,sys ,re, io
import datetime as dt

from importlib import resources as res
import pandas as pd
import numpy as np
import yaml

import dsToolbox.common_funcs   as cfuncs
import dsToolbox.default_values as par

__all__ = [ &#34;query_synapse&#34;,
           &#39;query_synapse_db&#39;,
           &#39;query_synapse_local&#39;,
          &#34;query_deltaTable&#34;,
          &#34;query_template_run&#34;,
          
          &#34;dbfs2blob&#34;,
          &#34;spark2deltaTable&#34;,
          &#39;deltaTable_check&#39;,
          
          &#34;blob2spark&#34;,            
          &#34;spark2blob&#34;,
          &#34;blob2pd&#34;,
          &#34;pd2blob&#34;,
          &#34;pd2blob_batch&#34;,
          &#34;blob_check&#34;,
          &#39;xls2blob&#39;,
          
          &#34;pi2pd_interpolate&#34;,
          &#34;pi2pd_rawData&#34;,
          &#39;pi2pd_seconds&#39;
          ]

# upath=&#34;./dsToolbox/config.yml&#34;
# upath=&#39;./sql_template.yml&#39;
# config_yaml = yaml.safe_load(Path(upath).read_text())

def get_spark():
  import pyspark
  spark = pyspark.sql.SparkSession.builder.getOrCreate()
  sqlContext = pyspark.SQLContext(spark.sparkContext)
  return spark, sqlContext

def get_dbutils():
  import IPython
  dbutils = IPython.get_ipython().user_ns[&#34;dbutils&#34;]
  return dbutils

def load_config(custom_config=None):
  if custom_config is None:
    with res.open_binary(&#39;dsToolbox&#39;, &#39;config.yml&#39;) as fp:
      config_yaml = yaml.load(fp, Loader=yaml.Loader)
  elif isinstance(custom_config, dict):  
    if (&#39;key_vault_dictS&#39; not in custom_config.keys())&amp;\
      (&#39;key_vault_name&#39; in custom_config.keys())&amp;\
      (&#39;secret_name&#39; in custom_config.keys())&amp;\
      (&#39;storage_account&#39; in custom_config.keys())  :
      custom_config[&#39;key_vault_dictS&#39;]={}
      custom_config[&#39;key_vault_dictS&#39;][custom_config[&#39;storage_account&#39;]]= {&#39;key_vault_name&#39; : custom_config[&#39;key_vault_name&#39;],
                                                                          &#34;secret_name&#34;    : custom_config[&#39;secret_name&#39;]
                                                                          }
      for k in (&#39;storage_account&#39;, &#39;key_vault_name&#39;, &#39;secret_name&#39;):
          custom_config.pop(k)

    config_yaml =custom_config
  else:
    from pathlib import Path
    config_yaml = yaml.safe_load(Path(custom_config).read_text())

  key_vault_dictS      = config_yaml.get(&#39;key_vault_dictS&#39;)
  KV_access_local      = config_yaml.get(&#39;KV_access_local&#39;)
  synapse_cred_dict    = config_yaml.get(&#39;synapse_cred_dict&#39;)
  azure_ml_appID       = config_yaml.get(&#39;azure_ml_appID&#39;)
  pi_server_dict       = config_yaml.get(&#39;pi_server&#39;)
  # print(key_vault_dictS, KV_access_local, synapse_cred_dict, azure_ml_appID)
  return config_yaml, key_vault_dictS, KV_access_local, synapse_cred_dict, azure_ml_appID, pi_server_dict

io_config_dict, _, _, _, _, _=load_config(custom_config=None)

class cred_strings():
  def __init__(self, key_vault_dict, custom_config=None, platform=&#39;databricks&#39;):
    
    _, key_vault_dictS, KV_access_local, synapse_cred_dict, azure_ml_appID, pi_server_dict=load_config(custom_config)
    
    self.key_vault_dict  = key_vault_dict
    cred_dict            = key_vault_dictS[key_vault_dict]
    self.key_vault_name  = cred_dict.get(&#39;key_vault_name&#39;)
    self.secret_name     = cred_dict.get(&#39;secret_name&#39;)

    self.platform        = platform
    self.azure_ml_appID  = azure_ml_appID
    self.KV_access_local = KV_access_local
    
    self.synapse_cred_dict = synapse_cred_dict
    
    self.pi_server_dict  = pi_server_dict
    
    self.password        = fetch_key_value(self.key_vault_name,
                                            self.secret_name,
                                            self.azure_ml_appID,
                                            self.KV_access_local,
                                            self.platform)

  def blob_connector(self, filename, container):
    self.storage_account = self.key_vault_dict
    blob_host= f&#34;fs.azure.account.key.{self.storage_account}.blob.core.windows.net&#34;

    path = f&#39;{container}@{self.storage_account}&#39;
    blob_path = f&#34;wasbs://{path}.blob.core.windows.net/{filename}&#34;
    blob_connectionStr= (f&#39;DefaultEndpointsProtocol=https;AccountName={self.storage_account};&#39;
                          f&#39;AccountKey={self.password};EndpointSuffix=core.windows.net&#39;)
    
    return blob_host, blob_path, blob_connectionStr
    
  def spark_host(self):
    return f&#34;fs.azure.account.key.{self.key_vault_name}.dfs.core.windows.net&#34;

  def synapse_connector(self):
    hostname = self.synapse_cred_dict[&#39;hostname&#39;]
    database = self.synapse_cred_dict[&#39;database&#39;]
    port     = self.synapse_cred_dict[&#39;port&#39;]
    username = self.synapse_cred_dict[&#39;username&#39;]
    driver   = self.synapse_cred_dict.get(&#39;driver&#39;)
    driver_odbc=self.synapse_cred_dict.get(&#39;driver_odbc&#39;)
    port       =self.synapse_cred_dict[&#39;port&#39;]
    
    properties = {&#34;user&#34;     : username,
                  &#34;password&#34; : self.password,
                  &#34;driver&#34;   : driver }

    url = f&#34;jdbc:sqlserver://{hostname}:{port};database={database}&#34;

    odbc_connector = (f&#34;DRIVER={driver_odbc};SERVER={hostname};PORT={port};&#34;
                      f&#34;DATABASE={database};UID={username};&#34;
                      f&#34;PWD={self.password}; MARS_Connection=yes&#34;)
        
    return url, properties, odbc_connector
    
  #Call to Azure to retrieve OAuth access token (required for all PI API calls through cloud services)
  def pi_server_connector(self):
    url = self.pi_server_dict[&#39;url&#39;]
    myobj = {&#39;grant_type&#39;  : self.pi_server_dict[&#39;grant_type&#39;],
            &#39;client_id&#39;    : self.pi_server_dict[&#39;client_id&#39;],
            &#39;scope&#39;        : self.pi_server_dict[&#39;client_secret&#39;],
            &#39;client_secret&#39;: self.password
            }
    import requests
    oAuthResponse = requests.post(url, data = myobj)
    accessToken = oAuthResponse.json().get(&#39;access_token&#39;)
    return accessToken

def clean_query(q,
                # n=100
                ):
    q = q.strip().lstrip(&#39;(&#39;)
    q = q.rstrip(&#39;query&#39;)
    q = q.strip().rstrip(&#39;)&#39;)
    # q = q.replace(&#39;SELECT&#39;, f&#39;SELECT TOP({n})&#39;)
    return q
  
def get_secret_KVUri(key_vault_name, secret_name, credential):
  from azure.keyvault.secrets import SecretClient    
  KVUri      = f&#34;https://{key_vault_name}.vault.azure.net&#34;
  client     = SecretClient(vault_url = KVUri, credential = credential)
  secret     = client.get_secret(secret_name).value
  return secret

def fetch_key_value(key_vault_name, secret_name,
                     azure_ml_appID, KV_access_local,
                     platform=&#39;databricks&#39;,):
  if platform==&#39;databricks&#39;:
    # print(&#39;i am databricks run&#39;)
    dbutils = get_dbutils()
    secret = dbutils.secrets.get(scope =key_vault_name, key = secret_name)

  elif platform == &#39;aml&#39;:
    print(&#34;using azure ML and managed identity authentication&#34;)
    #Auth Mechanism: Managed Identity
    if azure_ml_appID is None:
      sys.exit(&#34;Identity Application ID is not provided&#34;)
    from azure.identity import ManagedIdentityCredential
    client_id                  = f&#34;{azure_ml_appID}&#34;  ##EnterManagedIdentityApplicationID
    credential                 = ManagedIdentityCredential(client_id = client_id)
    credential.get_token(&#34;https://vault.azure.net/.default&#34;)
    secret=get_secret_KVUri(key_vault_name, secret_name, credential = credential)

  elif platform == &#39;local&#39; or &#39;vm_docker&#39;:
    print(&#39;i am locally run&#39;)

    from azure.identity import DefaultAzureCredential    
    try:    
      import os
      if os.environ.get(&#39;AZURE_TENANT_ID&#39;) is None:      
        os.environ[&#39;AZURE_TENANT_ID&#39;]     = KV_access_local[&#39;secret_TenantID&#39;]
      if os.environ.get(&#39;AZURE_CLIENT_ID&#39;) is None:    
        os.environ[&#39;AZURE_CLIENT_ID&#39;]     = KV_access_local[&#39;secret_ClientID__prd&#39;]
      if os.environ.get(&#39;AZURE_CLIENT_SECRET&#39;) is None:      
        os.environ[&#39;AZURE_CLIENT_SECRET&#39;] = KV_access_local[&#39;secret_ClientSecret__Prd&#39;]
    except Exception as e:
      print(f&#39;{str(e)}&#39;)

      if KV_access_local is None:
        sys.exit(&#34;&#34;&#34;set AZURE_TENANT_ID, AZURE_CLIENT_ID, and AZURE_CLIENT_SECRET environment variables 
                    or provide KV_access_local dictionary in config.yml file to extract them &#34;&#34;&#34;)


    from azure.identity import DefaultAzureCredential    
    secret=get_secret_KVUri(key_vault_name, secret_name, credential = DefaultAzureCredential())

  return secret

# -------------------------------------------------------------------------
# -------------------------------------------------------------------------
# ----------------------------Running queries -----------------------------
def query_synapse(query: str,
                  platform=&#39;databricks&#39;,
                  key_vault_dict: str =&#39;azure_synapse&#39;,
                  custom_config=None,
                  verbose=True):
  &#34;&#34;&#34;Run a Query in azure synapse

  Params:
    
    query: (string) SQL query string
    
    verbose:(Boolean)  an option for producing detailed information
    
  Returns: a spark dataframe in databricks or pandas dataframe in local/ vm_docker
  &#34;&#34;&#34;
  
  if platform==&#39;databricks&#39;:
    query_synapse_db(query,
                    key_vault_dict=key_vault_dict,
                    custom_config=custom_config,
                    verbose=verbose,
                    )
  elif (platform==&#39;local&#39;) or (platform==&#39;vm_docker&#39;):
    query_synapse_local(query,
                        key_vault_dict=key_vault_dict,
                        custom_config=custom_config,
                        verbose=verbose,
                        )

def query_synapse_db(query: str,
                    key_vault_dict: str =&#39;azure_synapse&#39;,
                    custom_config=None,
                    verbose=True,
                    ):
  &#34;&#34;&#34;Run a Query in azure synapse
  
    Params:
     query: (string) SQL query string
      
      key_vault_dict(string) dictionary name in config.yml
      
      verbose:(Boolean)  an option for producing detailed information
      
      custom_config(dict/filePath): a dictionary of configuration credentials or path of a yaml file, if it is not provided, dsToolbox.config_dict will be used instead
      
    Returns: a spark dataframe
  &#34;&#34;&#34;
  c=cred_strings(key_vault_dict=key_vault_dict,
                 custom_config=custom_config,
                 platform=&#39;databricks&#39;)
  url, properties, _=c.synapse_connector()

  query=f&#39;({query}) query&#39; if (query.strip()[-5:]!=&#39;query&#39;) or (query.strip()[0]!=&#39;(&#39;) else query 
  if verbose: print(&#34;pulling data from azure_synapse:\n&#34;, query) 

  spark, sqlContext=get_spark()
  df  = spark.read.jdbc(table=query, url=url, properties=properties)

  ###----for local:
  # query = clean_query(query)  
  # import pyodbc
  # con = pyodbc.connect(cnstr)
  # df  = pd.read_sql(query,con)
  # con.close()

  return df  

def query_synapse_local(query: str,
                        key_vault_dict: str =&#39;azure_synapse&#39;,
                        custom_config=None,
                        verbose=True,
                        ):
  &#34;&#34;&#34;Run a Query in azure synapse
  
    Params:
    
      query: (string) SQL query string
      
      key_vault_dict(string) dictionary name in config.yml
      
      custom_config(dict/filePath): a dictionary of configuration credentials or path of a yaml file, if it is not provided, dsToolbox.config_dict will be used instead

      verbose:(Boolean)  an option for producing detailed information
      
    Returns: a pandas dataframe
  &#34;&#34;&#34;
  # query=query.strip()
  # if (query[-5:]==&#39;query&#39;):
  #   query=query[1:-5]

  c=cred_strings(key_vault_dict=key_vault_dict,
                 custom_config=custom_config,
                 platform=&#39;local&#39;)
  _, _, cnstr=c.synapse_connector()

  query = clean_query(query)  
  if verbose: print(&#34;pulling data from azure_synapse:\n&#34;, query) 

  import pyodbc
  con = pyodbc.connect(cnstr)
  df  = pd.read_sql(query,con)
  con.close()
  return df

def query_deltaTable_db(query: str,
                        key_vault_dict: str =&#39;deltaTable&#39;,
                        verbose=True,
                        custom_config=None,
                        ):
  &#34;&#34;&#34;Run a Query in deltaTable
  
    Params:
     query: (string) SQL query string
      
      key_vault_dict(string) dictionary name in config.yml
      
      verbose:(Boolean)  an option for producing detailed information
      
      custom_config(dict/filePath): a dictionary of configuration credentials or path of a yaml file, if it is not provided, dsToolbox.config_dict will be used instead
      
    Returns: a spark dataframe 
  &#34;&#34;&#34;

  if verbose: print(&#34;pulling data from deltaTable:\n&#34;, query) 
  c = cred_strings(key_vault_dict=key_vault_dict,
                   custom_config=custom_config,
                   platform=&#39;databricks&#39;)
  spark, sqlContext=get_spark()
  spark.conf.set(c.spark_host(), c.password)
  df = spark.sql(query)
  return df

def query_template_reader(query_str: str,
                          replace_dict: dict={&#39;start___date&#39;:par.start_date,
                                              &#39;end___date&#39;  :par.end_date
                                              },
                          ):
    ##TODO: check required format based on yaml 
    if cfuncs.check_timestamps(replace_dict.get(&#39;start___date&#39;), replace_dict.get(&#39;end___date&#39;)):
      for key,value in replace_dict.items():
        query=query_str.replace(key,value)  
    return query

def query_template_run(query_temp_name: str,
                      replace_dict: dict={&#39;start___date&#39;:par.start_date,
                                          &#39;end___date&#39;  :par.end_date
                                          },
                      custom_config=None,
                      custom_sql_template_yml=None,
                      platform=&#39;databricks&#39;,
                      ):
  &#34;&#34;&#34;Run a Query in deltaTable or azure synapse
    Params:
      query_temp_name: (dictionary) the name of dictionary that contains query template
      
      custom_config(dict/filePath): a dictionary of configuration credentials or path of a yaml file, if it is not provided, dsToolbox.config_dict will be used instead
      
      custom_sql_template_yml(path): a path of predefined  SQL qureies, if it is not provided, dsToolbox.sql_template_dict will be used instead
      
      replace_dict:(dictionary) it is used to replace start and end date
      
    Returns: a spark dataframe
  &#34;&#34;&#34;
  
  if custom_sql_template_yml is None:
    with res.open_binary(&#39;dsToolbox&#39;, &#39;sql_template.yml&#39;) as fp:
      sql_template_dict = yaml.load(fp, Loader=yaml.Loader)
  else:
    from pathlib import Path
    sql_template_dict = yaml.safe_load(Path(custom_sql_template_yml).read_text())

  tmp=sql_template_dict.get(query_temp_name)
  host, query_str = tmp[&#39;db&#39;], tmp[&#39;query&#39;] 

  query=query_template_reader(query_str,
                            replace_dict=replace_dict
                            )
  if host==&#39;azure_synapse&#39;:
    if platform==&#39;databricks&#39;:
      df=query_synapse_db(
                          query,
                          key_vault_dict=host,
                          custom_config=custom_config,
                          verbose=True,
                          )
    elif (platform==&#39;local&#39;) or (platform==&#39;vm_docker&#39;):
      df=query_synapse_local(query,
                            key_vault_dict=host,
                            custom_config=custom_config,
                            verbose=True,
                            )
  else:
    df=query_deltaTable_db(
                          query,
                          key_vault_dict=host,
                          custom_config=custom_config,
                          )  
  return df

# -------------------------------------------------------------------------
# -------------------------------------------------------------------------
# ----------------------------Talking with databricks  ------------------
def dbfs2blob(ufile, blob_dict, custom_config=None):      
  &#34;&#34;&#34;Save a File from databricks into Azure blob storage
    Params:
      ufile  : (string) Path to file saved on Databricks File System (DBFS) starting with &#39;/dbfs/&#39;
      
      blob_dict:(Dictionary) TargetFile path   : {&#39;key_vault_dict&#39;:storage account_name(string) ,
                                                  &#39;container&#39;:container_name(string) ,
                                                  &#39;blob&#39;: blob_name(string) 
                                                  }     
                                                    
    Returns: None
  &#34;&#34;&#34;                       
  storage_account = blob_dict.get(&#39;storage_account&#39;)
  container       = blob_dict.get(&#39;container&#39;)  
  blob            = blob_dict.get(&#39;blob&#39;)

  c = cred_strings(key_vault_dict=storage_account,
                   custom_config=custom_config,
                   platform=&#39;databricks&#39;)
  blob_host, blob_path, blob_connectionStr=c.blob_connector(blob, container)

  ##TODO: setting spark with blob_host?
  spark, sqlContext=get_spark()
  spark.conf.set(blob_host, c.password)
  
  dbutils=get_dbutils()
  dbutils.fs.cp(ufile.replace(&#34;/dbfs&#34;,&#34;dbfs:&#34;),blob_path)
  print(f&#34;{ufile} saved in {blob_path}&#34;)

def spark2deltaTable(df, table_name: str, schema: str = &#39;xxx_analytics&#39;,
                    write_mode:str = &#39;append&#39;, partitionby:list = None, 
                    **options
                    ):
  &#34;&#34;&#34; Writes to databricks delta tables
  
    Params: 
          df           : Spark Dataframe to write to delta table
            
          database     : (string) Name of database
            
          table        : (string) Name of table. Creates a new table if it doesn&#39;t exist
            
          write_mode   : (string) Write mode (&#39;append&#39; or &#39;overwrite&#39;). Default is &#39;append&#39;
            
          partitionby  : (list) list of Column names to partition by.

          **options    : (dict) all other string options 
            
    Return: None
  &#34;&#34;&#34;
  spark, sqlContext=get_spark()
  spark.sql(f&#34;CREATE DATABASE IF NOT EXISTS {schema}&#34;)

  if partitionby is None:
    partitionBy=&#39;&#39;
  else:
    partitionby=[partitionby] if not isinstance(partitionby, list) else partitionby
    partitionBy=f&#39;, partitionby={partitionby}&#39;

  eval(f&#34;df.write.saveAsTable(&#39;{schema}.{table_name}&#39;, mode=&#39;{write_mode}&#39; {partitionBy} , **{options})&#34;)
  
  return

def deltaTable_check(delta_tableName: str,
                     ) -&gt; bool:

  &#34;&#34;&#34;check a delta table exist or not
  
    Params:
    
    delta_tableName:(string) the tablename

    Returns: (Boolean)
  &#34;&#34;&#34;  
  spark, sqlContext=get_spark()
  is_delta = spark._jsparkSession.catalog().tableExists(delta_tableName)
  ###https://kb.databricks.com/en_US/delta/programmatically-determine-if-a-table-is-a-delta-table-or-not
  # desc_table = spark.sql(f&#34;describe formatted {delta_tableName}&#34;).collect()
  # location = [i[1] for i in desc_table if i[0] == &#39;Location&#39;][0]
  # try:
  #   dir_check = dbutils.fs.ls(f&#34;{location}/_delta_log&#34;)
  #   is_delta = True
  # except Exception as e:
  #   is_delta = False

  if is_delta:
    print(f&#34;table {delta_tableName} exists!&#34;)
  return is_delta

# -------------------------------------------------------------------------
# -------------------------------------------------------------------------
# ----------------------------Talking with Azure Storage ------------------
##storage account--&gt;containers--&gt;blob
def blob2spark(blob_dict:dict,
              custom_config=None,
              platform=&#39;databricks&#39;
              ):
  &#34;&#34;&#34;read a blob file (csv or parquet file) as a spark dataframe
    Params:
    
      blob_dict:(Dictionary) TargetFile path   : {&#39;key_vault_dict&#39;:storage account_name(string) ,
                                                  &#39;container&#39;:container_name(string) ,
                                                  &#39;blob&#39;: blob_name(string) 
                                                  }  
                                                        
      verbose:(Boolean)  an option for producing detailed information   
      
      custom_config(dict/filePath): a dictionary of configuration credentials or path of a yaml file, if it is not provided, dsToolbox.config_dict will be used instead
                            
    Returns: spark dataframe
  &#34;&#34;&#34;  
  storage_account=blob_dict.get(&#39;storage_account&#39;)
  container=blob_dict.get(&#39;container&#39;)  
  blob=blob_dict.get(&#39;blob&#39;)
  
  c = cred_strings(key_vault_dict=storage_account,
                   custom_config=custom_config,
                   platform=platform)
  blob_host, blob_path, blob_connectionStr=c.blob_connector(blob, container)
  spark, sqlContext=get_spark()
  spark.conf.set(blob_host, c.password)
  extension=blob.split(&#39;.&#39;)[-1]
  if extension==&#39;csv&#39;:
    df = spark.read.format(&#39;csv&#39;)\
                  .option(&#39;header&#39;,&#39;true&#39;)\
                  .option(&#39;inferSchema&#39;,&#39;true&#39;)\
                  .load(blob_path)
  elif extension==&#39;parquet&#39;:
    df= spark.read.format(&#34;parquet&#34;)\
                  .load(blob_path)
  return df

def spark2blob(df,
              blob_dict:dict,
              write_mode:str = &#34;mode(&#39;append&#39;)&#34;,
              custom_config=None,
              platform=&#39;databricks&#39;
              ):
  &#34;&#34;&#34;Save spark dataframe (df) into Azure blob storage using df.write.format command
    Params:
    
    df: park dataframe
    
    blob_dict:(Dictionary) TargetFile path   : {&#39;key_vault_dict&#39;:storage account_name(string) ,
                                                  &#39;container&#39;:container_name(string) ,
                                                  &#39;blob&#39;: blob_name(string) 
                                                  }  

    write_mode(string)  attached like options to df.write.format command   
    
    custom_config(dict/filePath): a dictionary of configuration credentials or path of a yaml file, if it is not provided, dsToolbox.config_dict will be used instead

    Returns: None
  &#34;&#34;&#34;          
  storage_account = blob_dict.get(&#39;storage_account&#39;)
  container       = blob_dict.get(&#39;container&#39;)  
  blob            = blob_dict.get(&#39;blob&#39;)

  c = cred_strings(key_vault_dict=storage_account,
                   custom_config=custom_config,
                   platform=platform)
  blob_host, blob_path, blob_connectionStr=c.blob_connector(blob, container)
  spark, sqlContext=get_spark()
  spark.conf.set(blob_host, c.password)
  extension=blob.split(&#39;.&#39;)[-1]
  
  string_run=f&#34;&#34;&#34;df.write.format(&#39;{extension}&#39;)\
                .{write_mode}
              &#34;&#34;&#34;

  string_run=f&#34;&#34;&#34;{string_run}
                  .save(blob_path)&#34;&#34;&#34;  
  string_run=re.sub(r&#34;[\n\t\s]*&#34;, &#34;&#34;, string_run)
  print(&#34;running:\n&#34;, string_run)
  eval(string_run)

def blob2pd(blob_dict:dict,
            verbose=True,
            custom_config=None,
            platform=&#39;databricks&#39;,
            create_tmp_file=False,
            **kwargs
            )-&gt; pd.DataFrame:
  &#34;&#34;&#34;read a blob file (csv or parquet file) as a panadas dataframe
  
    Params:
    
    blob_dict:(Dictionary) TargetFile path   : {&#39;key_vault_dict&#39;:storage account_name(string) ,
                                                  &#39;container&#39;:container_name(string) ,
                                                  &#39;blob&#39;: blob_name(string) 
                                                  }

    verbose:(Boolean)  an option for print detailed information
    
    custom_config(dict/filePath): a dictionary of configuration credentials or path of a yaml file, if it is not provided, dsToolbox.config_dict will be used instead
    
    create_tmp_file:(Boolean)  

    **kwargs: options used in pd.read_csv or pd.read_parquet   

    Returns: panda dataframe
  &#34;&#34;&#34;   
  import inspect 
  csv_args = list(inspect.signature(pd.read_csv).parameters)
  kwargs_csv = {k: kwargs.pop(k) for k in dict(kwargs) if k in csv_args}
  # print(&#34;csv arugments:&#34;, kwargs_csv)
  parq_args = list(inspect.signature(pd.read_parquet).parameters)
  kwargs_parq = {k: kwargs.pop(k) for k in dict(kwargs) if k in parq_args}
  # print(f&#34;parquet arugments:&#34;, parq_args)

  from azure.storage.blob import BlobServiceClient
  # from azure.storage.blob import ContainerClient

  storage_account = blob_dict.get(&#39;storage_account&#39;)
  container       = blob_dict.get(&#39;container&#39;)  
  blob            = blob_dict.get(&#39;blob&#39;)

  c = cred_strings(key_vault_dict=storage_account,
                   custom_config=custom_config,
                   platform=platform)
  blob_host, blob_path, blob_connectionStr=c.blob_connector(blob, container)
  blob_service_client = BlobServiceClient.from_connection_string(blob_connectionStr)
  blob_client         = blob_service_client.get_blob_client(container = container, blob = blob)
  extension           = blob.split(&#39;.&#39;)[-1]

  if verbose: print(f&#34;Downloading from storage_account:&#39;{storage_account}&#39;, container:&#39;{container},&#39; and blob:&#39;{blob}&#39; as bytes&#34;)
  
  if create_tmp_file:
    with io.BytesIO() as blob_dest:
      blob_client.download_blob().readinto(blob_dest)
      blob_dest.seek(0)
      if extension==&#39;csv&#39;:
        df = pd.read_csv(blob_dest,**kwargs_csv)
      elif extension==&#39;parquet&#39;:
        df = pd.read_parquet(blob_dest,**kwargs_parq)  
      else:
        print(f&#34;file uploaded into the memory&#34;)
        df=blob_dest
    
  else:  
    tmp_file_locs={
                  &#39;databricks&#39; : f&#39;/tmp/tmp_download_blob.{extension}&#39;,
                  &#39;aml&#39;        : f&#39;{os.getcwd()}/tmp_download_blob.{extension}&#39;,
                  &#39;local&#39;      : f&#39;{os.getcwd()}/tmp_download_blob.{extension}&#39;,
                  &#39;vm_docker&#39;  : f&#39;{os.getcwd()}/tmp_download_blob.{extension}&#39;,
                  }
  
    tmp_file=tmp_file_locs[platform]
    with open(tmp_file, &#39;wb&#39;) as blob_dest:
      data = blob_client.download_blob()
      blob_dest.write(data.readall())
    if extension==&#39;csv&#39;:
      df = pd.read_csv(tmp_file,**kwargs_csv)
      os.remove(tmp_file)
    elif extension==&#39;parquet&#39;:
      df = pd.read_parquet(tmp_file,**kwargs_parq)  
      os.remove(tmp_file)
    else:
      print(f&#34;file uploaded in {tmp_file}&#34;)
      df=tmp_file
  
  return df

def pd2blob(data: pd.DataFrame,
            blob_dict:dict,
            append=False,
            overwrite=True,
            platform=&#39;databricks&#39;,
            custom_config=None,
            sheetName=&#39;dataframe1&#39;,
            **kwargs
          ):
  &#34;&#34;&#34;Save pandas dataframe (df) into Azure blob storage using blob_client
  
    Params:
    
    data: pandas dataframe
    
    blob_dict:(Dictionary) TargetFile path   : {&#39;key_vault_dict&#39;:storage account_name(string) ,
                                                  &#39;container&#39;:container_name(string) ,
                                                  &#39;blob&#39;: blob_name(string) 
                                                  }  

    append(boolean):       when append=True, append dataframe to existing file 
    
    sheetName(string):     sheet name in the excel file
    
    overwrite(boolean):    when overwrite=True, overwrite to  existing file  

    custom_config(dict/filePath): a dictionary of configuration credentials or path of a yaml file, if it is not provided, dsToolbox.config_dict will be used instead
    

    **kwargs: args for :
      blob_client.upload_blob see:https://docs.microsoft.com/en-us/python/api/azure-storage-blob/azure.storage.blob.blobclient?view=azure-python#azure-storage-blob-blobclient-upload-blob
      pandas.DataFrame.to_csv
      pandas.DataFrame.to_parquet
      pandas.DataFrame.to_excel
                    
    Returns: get_blob_properties
  &#34;&#34;&#34;      
  ##for more information please see 
  ##https://docs.microsoft.com/en-us/python/api/overview/azure/storage-blob-readme?view=azure-python#key-concepts
  ##https://docs.microsoft.com/en-us/azure/storage/blobs/storage-blobs-introduction
  ##https://github.com/Azure/azure-sdk-for-python/tree/azure-storage-blob_12.9.0/sdk/storage/azure-storage-blob/samples

  import sys, io
  import pandas as pd
  ###using BlobServiceClient    
  from azure.storage.blob import BlobServiceClient
  storage_account = blob_dict.get(&#39;storage_account&#39;)
  container       = blob_dict.get(&#39;container&#39;)  
  blob            = blob_dict.get(&#39;blob&#39;)

  c = cred_strings(key_vault_dict=storage_account,
                   custom_config=custom_config,
                   platform=platform)
  blob_host, blob_path, blob_connectionStr=c.blob_connector(blob, container)
  blob_service_client = BlobServiceClient.from_connection_string(blob_connectionStr)
  container_client    = blob_service_client.get_container_client(container)
  
  ##method1:
  # blob_client = container_client.upload_blob(name=blob,
  #                                            data=data,
  #                                            **kwargs
  #                                           )

  ##method2:
  blob_client=container_client.get_blob_client(blob)    

  if blob_client.exists():
    print(f&#34;File Exists!&#34;)
    if  (append==overwrite) :             ###(&#39;overwrite&#39; in kwargs)and (kwargs.get(&#34;overwrite&#34;)==append):
      print(f&#34;append and overwrite have values same = {append}&#34;,&#39;\n Append set to True and overwrite set to False&#39;)    
      append=True
      overwrite=False

  ##method3:
  ##https://medium.com/featurepreneur/parquet-on-azure-27725ab1246b
  #blob_client = blob_service_client.get_blob_client(container = container, blob = blob)

  # print(f&#34;Uploading file: {} to key_vault_dict:&#39;{key_vault_dict}&#39; container:&#39;{container},&#39; and blob:&#39;{blob}&#39; as bytes&#34;)
  import inspect
  
  csv_args = list(inspect.signature(pd.DataFrame.to_csv).parameters)
  kwargs_csv = {k: kwargs.pop(k) for k in dict(kwargs) if k in csv_args}

  parq_args = list(inspect.signature(pd.DataFrame.to_parquet).parameters)
  kwargs_parq = {k: kwargs.pop(k) for k in dict(kwargs) if k in parq_args}

  xls_args = list(inspect.signature(pd.DataFrame.to_excel).parameters)
  kwargs_xls = {k: kwargs.pop(k) for k in dict(kwargs) if k in xls_args}

  blob_args = list(inspect.signature(blob_client.upload_blob).parameters)
  kwargs_blob = {k: kwargs.pop(k) for k in dict(kwargs) if k in blob_args}
  
  if blob.split(&#39;.&#39;)[1] == &#39;csv&#39;:
    if blob_client.exists() and append:
      blob_client.upload_blob(data=data.to_csv(header=False, **kwargs_csv),
                              **kwargs_blob,
                              blob_type=&#34;AppendBlob&#34;
                              )
    else:
      blob_client.upload_blob(data=data.to_csv(**kwargs_csv),
                              **kwargs_blob, overwrite=overwrite,
                              blob_type=&#34;AppendBlob&#34;
                              )
  elif blob.split(&#39;.&#39;)[1] == &#39;parquet&#39;:
    if blob_client.exists() and append:
      df_current = blob2pd(blob_dict)
      df_current = pd.concat([df_current, data],axis=0)
      blob_client.upload_blob(data=df_current.to_parquet(**kwargs_parq),
                              overwrite=True,
                              **kwargs_blob)
    else:
      blob_client.upload_blob(data=data.to_parquet(**kwargs_parq),
                              overwrite=overwrite,
                              **kwargs_blob, 
                              ) 

  elif blob.split(&#39;.&#39;)[1]==&#39;xls&#39; :
    sys.exit(&#34;the function does not support the old .xls file format, please use xlsx format&#34;)
 
  elif blob.split(&#39;.&#39;)[1]==&#39;xlsx&#39; :
    import openpyxl, io
    if (append) &amp;(~overwrite):
      sys.exit(&#34;the function does not append to an existing excel file, use xls2blob to save multiple dataframes to a excel file&#34;)    
    else:
      output = io.BytesIO()
      with pd.ExcelWriter(output, engine=&#39;xlsxwriter&#39;) as writer:
          data.to_excel(writer, sheet_name=sheetName, **kwargs_xls)
      xlsx_data = output.getvalue() 
      blob_client.upload_blob(data=xlsx_data,
                                overwrite=True,
                                **kwargs_blob,
                                )
  else:
      print(&#34;Append option is not usable&#34;)
      blob_client.upload_blob(data=data,
                              overwrite=overwrite,
                              **kwargs_blob,
                              )        
  return blob_client.get_blob_properties()

def pd2blob_batch(outputs:dict,
                  blob_dict={&#39;container&#39;:&#39;xxx&#39;, 
                              &#39;key_vault_dict&#39;:&#39;prdadlafblockmodel&#39;},
                  append=True ,
                  platform=&#39;databricks&#39;,
                  **kwargs):
  &#34;&#34;&#34;Save pandas dataframes (df) into Azure blob storage using df.write.format command
      Params:
          
      outputs: (Dictionary)  key: blob path   , value:dataframe
      
      blob_dict:(Dictionary) TargetFile path   : {&#39;key_vault_dict&#39;:storage account_name(string) ,
                                                  &#39;container&#39;:container_name(string) ,
                                                  &#39;blob&#39;: blob_name(string) 
                                                  }  

      append(boolean):      when append=True, append dataframe to existing file   

      write_mode(string)  attached like options to df.write.format command    

      **kwargs: args for blob_client.upload_blob see:https://docs.microsoft.com/en-us/python/api/azure-storage-blob/azure.storage.blob.blobclient?view=azure-python#azure-storage-blob-blobclient-upload-blob
  &#34;&#34;&#34;    
  for out in outputs:
    try:
      blob_dict[&#39;blob&#39;]=out
      pd2blob(outputs.get(out),
              blob_dict=blob_dict,
              platform=platform,
              append=append,
              **kwargs
              )
      print(f&#39;{out} saved&#39;)
    except Exception as e:
      print(f&#39;***writing {out} failed: \n\t\t {str(e)}&#39;)
      pass

def blob_check(blob_dict:dict,
               custom_config=None,
               platform=&#39;databricks&#39;):
  &#34;&#34;&#34;check a blob exist or not
  
    Params:
    
    blob_dict:(Dictionary) file path  : {&#39;key_vault_dict&#39;:storage account_name(string) ,
                                            &#39;container&#39;:container_name(string) ,
                                            &#39;blob&#39;: blob_name(string) 
                                        } 

    Returns: (Boolean)
  &#34;&#34;&#34;  
  from azure.storage.blob import BlobClient
  
  storage_account = blob_dict.get(&#39;storage_account&#39;)
  container       = blob_dict.get(&#39;container&#39;)  
  blob            = blob_dict.get(&#39;blob&#39;)

  c = cred_strings(key_vault_dict=storage_account,
                   custom_config=custom_config,
                   platform=platform)
  blob_host, blob_path, blob_connectionStr=c.blob_connector(blob, container)

  blob = BlobClient.from_connection_string(conn_str=blob_connectionStr,
                                           container_name=container,
                                           blob_name=blob)

  return blob.exists()

def xls2blob(dataframe_dict: dict,
            blob_dict:dict,
            overwrite=True,
            custom_config=None,
            platform=&#39;databricks&#39;,
            **kwargs
          ):
  &#34;&#34;&#34;Save pandas dataframe(s) as a excel file into Azure blob storage
    Params:
    
    dataframe_dict:  dictionary of sheet_name annd corresponding dataframes to write
    
    blob_dict:(Dictionary) TargetFile path   : {&#39;key_vault_dict&#39;:storage account_name(string) ,
                                                  &#39;container&#39;:container_name(string) ,
                                                  &#39;blob&#39;: blob_name(string) 
                                                  }  
    
    overwrite(boolean):    when overwrite=True, overwrite to  existing file  

    custom_config(dict/filePath): a dictionary of configuration credentials or path of a yaml file, if it is not provided, dsToolbox.config_dict will be used instead

    **kwargs: args for :
      blob_client.upload_blob see:https://docs.microsoft.com/en-us/python/api/azure-storage-blob/azure.storage.blob.blobclient?view=azure-python#azure-storage-blob-blobclient-upload-blob
      pandas.DataFrame.to_excel
                    
    Returns: get_blob_properties
  &#34;&#34;&#34;      
  ###using BlobServiceClient    
  from azure.storage.blob import BlobServiceClient
  storage_account = blob_dict.get(&#39;storage_account&#39;)
  container       = blob_dict.get(&#39;container&#39;)  
  blob            = blob_dict.get(&#39;blob&#39;)

  c = cred_strings(key_vault_dict=storage_account,
                   custom_config=custom_config,
                   platform=platform)
  blob_host, blob_path, blob_connectionStr=c.blob_connector(blob, container)
  blob_service_client = BlobServiceClient.from_connection_string(blob_connectionStr)
  container_client    = blob_service_client.get_container_client(container)
  
  blob_client=container_client.get_blob_client(blob)    

  import inspect
  import sys, io
  import pandas as pd
    
  blob_args = list(inspect.signature(blob_client.upload_blob).parameters)
  kwargs_blob = {k: kwargs.pop(k) for k in dict(kwargs) if k in blob_args}
  
  xls_args = list(inspect.signature(pd.DataFrame.to_excel).parameters)
  kwargs_xls = {k: kwargs.pop(k) for k in dict(kwargs) if k in xls_args}

  output = io.BytesIO()
  writer =pd.ExcelWriter(output, engine=&#39;xlsxwriter&#39;)
  for sheetName, df in  dataframe_dict.items(): 
    df.to_excel(writer, sheet_name=sheetName, **kwargs_xls)
        
  writer.close()
  xlsx_data = output.getvalue() 
  blob_client.upload_blob(data=xlsx_data,
                          overwrite=overwrite,
                          **kwargs_blob,
                          )
  return blob_client.get_blob_properties()

# -------------------------------------------------------------------------
# -------------------------------------------------------------------------
#Call  PI Cloud API&#39;s point controller to get WebID for tag list in MST (WebID = persistent unique ID for a PI tag, required for majority of PI api calls)
def get_web_ids(accessToken, tags):
  import requests, json
  web_ids = {}
  for tag in tags:
    url = &#39;https://svc.apiproxy.exxonmobil.com/KRLPIV01/v1/piwebapi/points?path=\\KRLPIH01\{}&#39;.format(tag)
    headers = {
      &#39;Authorization&#39;:&#39;Bearer &#39; + accessToken,
      &#39;Content-Type&#39;:&#39;application/json&#39;}
    response = requests.request(&#34;GET&#34;, url, headers=headers)

    web_id=json.loads(response.text).get(&#39;WebId&#39;)
    web_ids[tag] =web_id

    if web_id is None:
      print(f&#34;PI tag not found: {tag}&#34;)
  return web_ids

def pi2pd_interpolate(tags,
                    start_date=par.start_date, end_date=par.end_date,
                    interval = &#39;1h&#39;,
                    pi_vault_dict=&#39;webapi&#39;, 
                    custom_config=None,
                    platform=&#39;databricks&#39;):
  &#34;&#34;&#34;
  Call PI Cloud API&#39;s stream controller to get interpolated data for tags  
  Get pi tag data according to desired frequency
  
  Params:
  
  tags(list)                  : maximum 11 tags at a time
    
  start_date (string)          : start date, format: &#34;%Y-%m-%d&#34;
  
  end_date   (string)          : end date, format: &#34;%Y-%m-%d&#34;
  
  pi_vault_dict(String)   : key_vault_dict dictionary  name for webapi in azure_valuet_cred dictionary     
  
  interval(string)  &#39;1h&#39;      : get interpolated data (default hourly)
  
  custom_config(dict/filePath): a dictionary of configuration credentials or path of a yaml file, if it is not provided, dsToolbox.config_dict will be used instead
  
  &#34;&#34;&#34; 
  import requests, urllib, json
  if isinstance(start_date,str):
    start_date   = dt.datetime.strptime(start_date, &#34;%Y-%m-%d&#34;) ###&#39;2021-12-31&#39;, &#34;%Y-%m-%d&#34;
  
  if isinstance(end_date, str):
    end_date     = dt.datetime.strptime(end_date,   &#34;%Y-%m-%d&#34;) ###&#39;2021-12-31&#39;, &#34;%Y-%m-%d&#34;

  if not isinstance(tags, list):tags=[tags]

  c = cred_strings(key_vault_dict=pi_vault_dict,
                   custom_config=custom_config,
                   platform=platform)
  accessToken = c.pi_server_connector()
  web_ids     = get_web_ids(accessToken, tags)
  tagData     = {}
  for i, tag in enumerate(tags):
    webID = web_ids[tag]
    print(&#39;tag=&#39;,tag,&#34;,webID=&#34;,f&#34;{webID[:5]}...{webID[20:25]}...{webID[-5:]}&#34;)
    if webID is not None:
      query_dict = {&#39;startTime&#39;:start_date, &#39;endTime&#39;:end_date, &#39;interval&#39;:interval}
      url = &#39;https://svc.apiproxy.exxonmobil.com/KRLPIV01/v1/piwebapi/streams/&#39;+webID+&#39;/interpolated?&#39;+urllib.parse.urlencode(query_dict)
      headers = {
        &#39;Authorization&#39;:&#39;Bearer &#39; + accessToken,
        &#39;Content-Type&#39;:&#39;application/json&#39;}
      response = requests.request(&#34;GET&#34;, url, headers=headers)
      json_str = json.loads(response.text)
      if i == 0:
          tagData[&#39;Date&#39;] = [j[&#39;Timestamp&#39;] for j in json_str[&#39;Items&#39;]]
      if isinstance(json_str[&#39;Items&#39;][0][&#39;Value&#39;], dict):
          tagData[tag] =  [j[&#39;Value&#39;][&#39;Name&#39;] if isinstance(j[&#39;Value&#39;], dict) else j[&#39;Value&#39;] for j in json_str[&#39;Items&#39;]]
      else:
          tagData[tag] = [j[&#39;Value&#39;] for j in json_str[&#39;Items&#39;]]
  df = pd.DataFrame(tagData, columns = tagData.keys())
  df[&#39;Date&#39;] = pd.to_datetime(df[&#39;Date&#39;]).dt.tz_convert(&#39;US/Mountain&#39;)
  return df  #, web_ids

def pi2pd_rawData(tags,
                start_date=par.start_date, end_date=par.end_date,
                pi_vault_dict=&#39;webapi&#39;,
                custom_config=None,
                platform=&#39;databricks&#39;):
  &#34;&#34;&#34;Call PI Cloud API&#39;s stream controller to get tags in with their original frequency

  Params:
    tags: (list)                : maximum 11 tags at a time: 
    
    start_date (string)          : start date, format: &#34;%Y-%m-%d&#34;
    
    end_date   (string)          : end date, format: &#34;%Y-%m-%d&#34;
        
    pi_vault_dict(Dictionary)   : key_vault_dict(string) key name for webapi in azure_valuet_cred dictionary     
    
    custom_config(dict/filePath): a dictionary of configuration credentials or path of a yaml file, if it is not provided, dsToolbox.config_dict will be used instead
  
  &#34;&#34;&#34; 
  #   start_date=&#39;2023-07-13&#39;
  #   end_date=&#39;2023-08-10&#39;
  #   tags=PI_WEB_API_TAGS.keys()
  #   pi_vault_dict=&#39;webapi&#39;
  #   platform=&#39;local&#39;

  import requests, urllib, json
  if isinstance(start_date,str):
    start_date   = dt.datetime.strptime(start_date, &#34;%Y-%m-%d&#34;) ###&#39;2021-12-31&#39;, &#34;%Y-%m-%d&#34;
  
  if isinstance(end_date, str):
    end_date     = dt.datetime.strptime(end_date,   &#34;%Y-%m-%d&#34;) ###&#39;2021-12-31&#39;, &#34;%Y-%m-%d&#34;

  if not isinstance(tags, list):tags=[tags]

  c = cred_strings(key_vault_dict=pi_vault_dict,
                   custom_config=custom_config,
                   platform=platform)
  accessToken = c.pi_server_connector()
  web_ids     = get_web_ids(accessToken, tags)
  
  data_entries = []
  for tag_name in tags:
    webID = web_ids[tag_name]
    print(&#39;tag=&#39;,tag_name,&#34;,webID=&#34;,f&#34;{webID[:5]}...{webID[20:25]}...{webID[-5:]}&#34;)
    if webID is not None:
      query_dict = {&#39;startTime&#39;:start_date, &#39;endTime&#39;:end_date}
      url = &#39;https://svc.apiproxy.exxonmobil.com/KRLPIV01/v1/piwebapi/streams/&#39;+webID+&#39;/recorded?&#39;+urllib.parse.urlencode(query_dict)
      headers = {
        &#39;Authorization&#39;:&#39;Bearer &#39; + accessToken,
        &#39;Content-Type&#39;:&#39;application/json&#39;}
      response = requests.request(&#34;GET&#34;, url, headers=headers)
      if response.status_code == 200:
        json_str = json.loads(response.text)
        # print(json_str)
        if &#39;Items&#39; in json_str:
          for entry in json_str[&#39;Items&#39;]:
            timestamp = entry[&#39;Timestamp&#39;]
            value = entry[&#39;Value&#39;]
            if isinstance(value, dict):
              if value.get(&#39;Name&#39;) == &#39;Bad&#39;:
                value = None
              else:
                value = value.get(&#39;Value&#39;)
            data_entries.append({&#39;Timestamp&#39;: timestamp, tag_name: value})
  df = pd.DataFrame(data_entries)
  df[&#39;Timestamp&#39;] = pd.to_datetime(df[&#39;Timestamp&#39;]).dt.tz_convert(&#39;US/Mountain&#39;)
  return df

def pi2pd_seconds(tags,
                start_date=par.start_date, end_date=par.end_date,
                pi_vault_dict=&#39;webapi&#39;,
                custom_config=None,
                platform=&#39;databricks&#39;):
  &#34;&#34;&#34;Call PI Cloud API&#39;s stream controller to get tags in every second
  maximum 11 tags at a time get pi data by second

  Params:
    tags: (list)                : maximum 11 tags at a time: 
    
    start_date (string)          : start date, format: &#34;%Y-%m-%d&#34;
    
    end_date   (string)          : end date, format: &#34;%Y-%m-%d&#34;
        
    pi_vault_dict(Dictionary)   : key_vault_dict(string) key name for webapi in azure_valuet_cred dictionary   
    
    custom_config(dict/filePath): a dictionary of configuration credentials or path of a yaml file, if it is not provided, dsToolbox.config_dict will be used instead
    
  &#34;&#34;&#34; 
  #   start_date=&#39;2023-07-13&#39;
  #   end_date=&#39;2023-08-10&#39;
  #   tags=PI_WEB_API_TAGS.keys()
  #   pi_vault_dict=&#39;webapi&#39;
  #   platform=&#39;local&#39;

  start_date   = dt.datetime.strptime(start_date, &#34;%Y-%m-%d&#34;) ###&#39;2021-12-31&#39;, &#34;%Y-%m-%d&#34;
  end_date     = dt.datetime.strptime(end_date,   &#34;%Y-%m-%d&#34;) ###&#39;2021-12-31&#39;, &#34;%Y-%m-%d&#34;

  start = start_date
  end = start + dt.timedelta(days=1)
  ret = pd.DataFrame()
  while(end &lt;= end_date):
    print(f&#34;getting data between {start} and {end}&#34;)
    try:
        
      pi_web_df = pi2pd_interpolate(tags,
                                    start_date=start, end_date=end,
                                    interval = &#39;1s&#39;,
                                    pi_vault_dict=pi_vault_dict, 
                                    custom_config=custom_config,
                                    platform=platform)
      print(&#34;Done&#34;)
    except:
      print(&#34;Skipped&#34;)
      start = end
      end += dt.timedelta(days=1)
      continue
    ret = pd.concat([ret, pi_web_df], ignore_index=True)
    start = end
    end += dt.timedelta(days=1)
  # ret.drop_duplicates(inplace=True) 
  return ret

# -------------------------------------------------------------------------
# -------------------------------------------------------------------------
# ---------------------------- ------------------------------------    </code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="io_funcs.blob2pd"><code class="name flex">
<span>def <span class="ident">blob2pd</span></span>(<span>blob_dict: dict, verbose=True, custom_config=None, platform='databricks', create_tmp_file=False, **kwargs) ‑> pandas.core.frame.DataFrame</span>
</code></dt>
<dd>
<div class="desc"><p>read a blob file (csv or parquet file) as a panadas dataframe</p>
<p>Params:</p>
<p>blob_dict:(Dictionary) TargetFile path
: {'key_vault_dict':storage account_name(string) ,
'container':container_name(string) ,
'blob': blob_name(string)
}</p>
<p>verbose:(Boolean)
an option for print detailed information</p>
<p>custom_config(dict/filePath): a dictionary of configuration credentials or path of a yaml file, if it is not provided, dsToolbox.config_dict will be used instead</p>
<p>create_tmp_file:(Boolean)
</p>
<p>**kwargs: options used in pd.read_csv or pd.read_parquet
</p>
<p>Returns: panda dataframe</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def blob2pd(blob_dict:dict,
            verbose=True,
            custom_config=None,
            platform=&#39;databricks&#39;,
            create_tmp_file=False,
            **kwargs
            )-&gt; pd.DataFrame:
  &#34;&#34;&#34;read a blob file (csv or parquet file) as a panadas dataframe
  
    Params:
    
    blob_dict:(Dictionary) TargetFile path   : {&#39;key_vault_dict&#39;:storage account_name(string) ,
                                                  &#39;container&#39;:container_name(string) ,
                                                  &#39;blob&#39;: blob_name(string) 
                                                  }

    verbose:(Boolean)  an option for print detailed information
    
    custom_config(dict/filePath): a dictionary of configuration credentials or path of a yaml file, if it is not provided, dsToolbox.config_dict will be used instead
    
    create_tmp_file:(Boolean)  

    **kwargs: options used in pd.read_csv or pd.read_parquet   

    Returns: panda dataframe
  &#34;&#34;&#34;   
  import inspect 
  csv_args = list(inspect.signature(pd.read_csv).parameters)
  kwargs_csv = {k: kwargs.pop(k) for k in dict(kwargs) if k in csv_args}
  # print(&#34;csv arugments:&#34;, kwargs_csv)
  parq_args = list(inspect.signature(pd.read_parquet).parameters)
  kwargs_parq = {k: kwargs.pop(k) for k in dict(kwargs) if k in parq_args}
  # print(f&#34;parquet arugments:&#34;, parq_args)

  from azure.storage.blob import BlobServiceClient
  # from azure.storage.blob import ContainerClient

  storage_account = blob_dict.get(&#39;storage_account&#39;)
  container       = blob_dict.get(&#39;container&#39;)  
  blob            = blob_dict.get(&#39;blob&#39;)

  c = cred_strings(key_vault_dict=storage_account,
                   custom_config=custom_config,
                   platform=platform)
  blob_host, blob_path, blob_connectionStr=c.blob_connector(blob, container)
  blob_service_client = BlobServiceClient.from_connection_string(blob_connectionStr)
  blob_client         = blob_service_client.get_blob_client(container = container, blob = blob)
  extension           = blob.split(&#39;.&#39;)[-1]

  if verbose: print(f&#34;Downloading from storage_account:&#39;{storage_account}&#39;, container:&#39;{container},&#39; and blob:&#39;{blob}&#39; as bytes&#34;)
  
  if create_tmp_file:
    with io.BytesIO() as blob_dest:
      blob_client.download_blob().readinto(blob_dest)
      blob_dest.seek(0)
      if extension==&#39;csv&#39;:
        df = pd.read_csv(blob_dest,**kwargs_csv)
      elif extension==&#39;parquet&#39;:
        df = pd.read_parquet(blob_dest,**kwargs_parq)  
      else:
        print(f&#34;file uploaded into the memory&#34;)
        df=blob_dest
    
  else:  
    tmp_file_locs={
                  &#39;databricks&#39; : f&#39;/tmp/tmp_download_blob.{extension}&#39;,
                  &#39;aml&#39;        : f&#39;{os.getcwd()}/tmp_download_blob.{extension}&#39;,
                  &#39;local&#39;      : f&#39;{os.getcwd()}/tmp_download_blob.{extension}&#39;,
                  &#39;vm_docker&#39;  : f&#39;{os.getcwd()}/tmp_download_blob.{extension}&#39;,
                  }
  
    tmp_file=tmp_file_locs[platform]
    with open(tmp_file, &#39;wb&#39;) as blob_dest:
      data = blob_client.download_blob()
      blob_dest.write(data.readall())
    if extension==&#39;csv&#39;:
      df = pd.read_csv(tmp_file,**kwargs_csv)
      os.remove(tmp_file)
    elif extension==&#39;parquet&#39;:
      df = pd.read_parquet(tmp_file,**kwargs_parq)  
      os.remove(tmp_file)
    else:
      print(f&#34;file uploaded in {tmp_file}&#34;)
      df=tmp_file
  
  return df</code></pre>
</details>
</dd>
<dt id="io_funcs.blob2spark"><code class="name flex">
<span>def <span class="ident">blob2spark</span></span>(<span>blob_dict: dict, custom_config=None, platform='databricks')</span>
</code></dt>
<dd>
<div class="desc"><p>read a blob file (csv or parquet file) as a spark dataframe</p>
<h2 id="params">Params</h2>
<p>blob_dict:(Dictionary) TargetFile path
: {'key_vault_dict':storage account_name(string) ,
'container':container_name(string) ,
'blob': blob_name(string)
}
</p>
<p>verbose:(Boolean)
an option for producing detailed information
</p>
<p>custom_config(dict/filePath): a dictionary of configuration credentials or path of a yaml file, if it is not provided, dsToolbox.config_dict will be used instead</p>
<p>Returns: spark dataframe</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def blob2spark(blob_dict:dict,
              custom_config=None,
              platform=&#39;databricks&#39;
              ):
  &#34;&#34;&#34;read a blob file (csv or parquet file) as a spark dataframe
    Params:
    
      blob_dict:(Dictionary) TargetFile path   : {&#39;key_vault_dict&#39;:storage account_name(string) ,
                                                  &#39;container&#39;:container_name(string) ,
                                                  &#39;blob&#39;: blob_name(string) 
                                                  }  
                                                        
      verbose:(Boolean)  an option for producing detailed information   
      
      custom_config(dict/filePath): a dictionary of configuration credentials or path of a yaml file, if it is not provided, dsToolbox.config_dict will be used instead
                            
    Returns: spark dataframe
  &#34;&#34;&#34;  
  storage_account=blob_dict.get(&#39;storage_account&#39;)
  container=blob_dict.get(&#39;container&#39;)  
  blob=blob_dict.get(&#39;blob&#39;)
  
  c = cred_strings(key_vault_dict=storage_account,
                   custom_config=custom_config,
                   platform=platform)
  blob_host, blob_path, blob_connectionStr=c.blob_connector(blob, container)
  spark, sqlContext=get_spark()
  spark.conf.set(blob_host, c.password)
  extension=blob.split(&#39;.&#39;)[-1]
  if extension==&#39;csv&#39;:
    df = spark.read.format(&#39;csv&#39;)\
                  .option(&#39;header&#39;,&#39;true&#39;)\
                  .option(&#39;inferSchema&#39;,&#39;true&#39;)\
                  .load(blob_path)
  elif extension==&#39;parquet&#39;:
    df= spark.read.format(&#34;parquet&#34;)\
                  .load(blob_path)
  return df</code></pre>
</details>
</dd>
<dt id="io_funcs.blob_check"><code class="name flex">
<span>def <span class="ident">blob_check</span></span>(<span>blob_dict: dict, custom_config=None, platform='databricks')</span>
</code></dt>
<dd>
<div class="desc"><p>check a blob exist or not</p>
<p>Params:</p>
<p>blob_dict:(Dictionary) file path
: {'key_vault_dict':storage account_name(string) ,
'container':container_name(string) ,
'blob': blob_name(string)
} </p>
<p>Returns: (Boolean)</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def blob_check(blob_dict:dict,
               custom_config=None,
               platform=&#39;databricks&#39;):
  &#34;&#34;&#34;check a blob exist or not
  
    Params:
    
    blob_dict:(Dictionary) file path  : {&#39;key_vault_dict&#39;:storage account_name(string) ,
                                            &#39;container&#39;:container_name(string) ,
                                            &#39;blob&#39;: blob_name(string) 
                                        } 

    Returns: (Boolean)
  &#34;&#34;&#34;  
  from azure.storage.blob import BlobClient
  
  storage_account = blob_dict.get(&#39;storage_account&#39;)
  container       = blob_dict.get(&#39;container&#39;)  
  blob            = blob_dict.get(&#39;blob&#39;)

  c = cred_strings(key_vault_dict=storage_account,
                   custom_config=custom_config,
                   platform=platform)
  blob_host, blob_path, blob_connectionStr=c.blob_connector(blob, container)

  blob = BlobClient.from_connection_string(conn_str=blob_connectionStr,
                                           container_name=container,
                                           blob_name=blob)

  return blob.exists()</code></pre>
</details>
</dd>
<dt id="io_funcs.dbfs2blob"><code class="name flex">
<span>def <span class="ident">dbfs2blob</span></span>(<span>ufile, blob_dict, custom_config=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Save a File from databricks into Azure blob storage</p>
<h2 id="params">Params</h2>
<p>ufile
: (string) Path to file saved on Databricks File System (DBFS) starting with '/dbfs/'</p>
<p>blob_dict:(Dictionary) TargetFile path
: {'key_vault_dict':storage account_name(string) ,
'container':container_name(string) ,
'blob': blob_name(string)
}
</p>
<p>Returns: None</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def dbfs2blob(ufile, blob_dict, custom_config=None):      
  &#34;&#34;&#34;Save a File from databricks into Azure blob storage
    Params:
      ufile  : (string) Path to file saved on Databricks File System (DBFS) starting with &#39;/dbfs/&#39;
      
      blob_dict:(Dictionary) TargetFile path   : {&#39;key_vault_dict&#39;:storage account_name(string) ,
                                                  &#39;container&#39;:container_name(string) ,
                                                  &#39;blob&#39;: blob_name(string) 
                                                  }     
                                                    
    Returns: None
  &#34;&#34;&#34;                       
  storage_account = blob_dict.get(&#39;storage_account&#39;)
  container       = blob_dict.get(&#39;container&#39;)  
  blob            = blob_dict.get(&#39;blob&#39;)

  c = cred_strings(key_vault_dict=storage_account,
                   custom_config=custom_config,
                   platform=&#39;databricks&#39;)
  blob_host, blob_path, blob_connectionStr=c.blob_connector(blob, container)

  ##TODO: setting spark with blob_host?
  spark, sqlContext=get_spark()
  spark.conf.set(blob_host, c.password)
  
  dbutils=get_dbutils()
  dbutils.fs.cp(ufile.replace(&#34;/dbfs&#34;,&#34;dbfs:&#34;),blob_path)
  print(f&#34;{ufile} saved in {blob_path}&#34;)</code></pre>
</details>
</dd>
<dt id="io_funcs.deltaTable_check"><code class="name flex">
<span>def <span class="ident">deltaTable_check</span></span>(<span>delta_tableName: str) ‑> bool</span>
</code></dt>
<dd>
<div class="desc"><p>check a delta table exist or not</p>
<p>Params:</p>
<p>delta_tableName:(string) the tablename</p>
<p>Returns: (Boolean)</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def deltaTable_check(delta_tableName: str,
                     ) -&gt; bool:

  &#34;&#34;&#34;check a delta table exist or not
  
    Params:
    
    delta_tableName:(string) the tablename

    Returns: (Boolean)
  &#34;&#34;&#34;  
  spark, sqlContext=get_spark()
  is_delta = spark._jsparkSession.catalog().tableExists(delta_tableName)
  ###https://kb.databricks.com/en_US/delta/programmatically-determine-if-a-table-is-a-delta-table-or-not
  # desc_table = spark.sql(f&#34;describe formatted {delta_tableName}&#34;).collect()
  # location = [i[1] for i in desc_table if i[0] == &#39;Location&#39;][0]
  # try:
  #   dir_check = dbutils.fs.ls(f&#34;{location}/_delta_log&#34;)
  #   is_delta = True
  # except Exception as e:
  #   is_delta = False

  if is_delta:
    print(f&#34;table {delta_tableName} exists!&#34;)
  return is_delta</code></pre>
</details>
</dd>
<dt id="io_funcs.pd2blob"><code class="name flex">
<span>def <span class="ident">pd2blob</span></span>(<span>data: pandas.core.frame.DataFrame, blob_dict: dict, append=False, overwrite=True, platform='databricks', custom_config=None, sheetName='dataframe1', **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Save pandas dataframe (df) into Azure blob storage using blob_client</p>
<p>Params:</p>
<p>data: pandas dataframe</p>
<p>blob_dict:(Dictionary) TargetFile path
: {'key_vault_dict':storage account_name(string) ,
'container':container_name(string) ,
'blob': blob_name(string)
}
</p>
<p>append(boolean):
when append=True, append dataframe to existing file </p>
<p>sheetName(string):
sheet name in the excel file</p>
<p>overwrite(boolean):
when overwrite=True, overwrite to
existing file
</p>
<p>custom_config(dict/filePath): a dictionary of configuration credentials or path of a yaml file, if it is not provided, dsToolbox.config_dict will be used instead</p>
<p>**kwargs: args for :
blob_client.upload_blob see:<a href="https://docs.microsoft.com/en-us/python/api/azure-storage-blob/azure.storage.blob.blobclient?view=azure-python#azure-storage-blob-blobclient-upload-blob">https://docs.microsoft.com/en-us/python/api/azure-storage-blob/azure.storage.blob.blobclient?view=azure-python#azure-storage-blob-blobclient-upload-blob</a>
pandas.DataFrame.to_csv
pandas.DataFrame.to_parquet
pandas.DataFrame.to_excel</p>
<p>Returns: get_blob_properties</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def pd2blob(data: pd.DataFrame,
            blob_dict:dict,
            append=False,
            overwrite=True,
            platform=&#39;databricks&#39;,
            custom_config=None,
            sheetName=&#39;dataframe1&#39;,
            **kwargs
          ):
  &#34;&#34;&#34;Save pandas dataframe (df) into Azure blob storage using blob_client
  
    Params:
    
    data: pandas dataframe
    
    blob_dict:(Dictionary) TargetFile path   : {&#39;key_vault_dict&#39;:storage account_name(string) ,
                                                  &#39;container&#39;:container_name(string) ,
                                                  &#39;blob&#39;: blob_name(string) 
                                                  }  

    append(boolean):       when append=True, append dataframe to existing file 
    
    sheetName(string):     sheet name in the excel file
    
    overwrite(boolean):    when overwrite=True, overwrite to  existing file  

    custom_config(dict/filePath): a dictionary of configuration credentials or path of a yaml file, if it is not provided, dsToolbox.config_dict will be used instead
    

    **kwargs: args for :
      blob_client.upload_blob see:https://docs.microsoft.com/en-us/python/api/azure-storage-blob/azure.storage.blob.blobclient?view=azure-python#azure-storage-blob-blobclient-upload-blob
      pandas.DataFrame.to_csv
      pandas.DataFrame.to_parquet
      pandas.DataFrame.to_excel
                    
    Returns: get_blob_properties
  &#34;&#34;&#34;      
  ##for more information please see 
  ##https://docs.microsoft.com/en-us/python/api/overview/azure/storage-blob-readme?view=azure-python#key-concepts
  ##https://docs.microsoft.com/en-us/azure/storage/blobs/storage-blobs-introduction
  ##https://github.com/Azure/azure-sdk-for-python/tree/azure-storage-blob_12.9.0/sdk/storage/azure-storage-blob/samples

  import sys, io
  import pandas as pd
  ###using BlobServiceClient    
  from azure.storage.blob import BlobServiceClient
  storage_account = blob_dict.get(&#39;storage_account&#39;)
  container       = blob_dict.get(&#39;container&#39;)  
  blob            = blob_dict.get(&#39;blob&#39;)

  c = cred_strings(key_vault_dict=storage_account,
                   custom_config=custom_config,
                   platform=platform)
  blob_host, blob_path, blob_connectionStr=c.blob_connector(blob, container)
  blob_service_client = BlobServiceClient.from_connection_string(blob_connectionStr)
  container_client    = blob_service_client.get_container_client(container)
  
  ##method1:
  # blob_client = container_client.upload_blob(name=blob,
  #                                            data=data,
  #                                            **kwargs
  #                                           )

  ##method2:
  blob_client=container_client.get_blob_client(blob)    

  if blob_client.exists():
    print(f&#34;File Exists!&#34;)
    if  (append==overwrite) :             ###(&#39;overwrite&#39; in kwargs)and (kwargs.get(&#34;overwrite&#34;)==append):
      print(f&#34;append and overwrite have values same = {append}&#34;,&#39;\n Append set to True and overwrite set to False&#39;)    
      append=True
      overwrite=False

  ##method3:
  ##https://medium.com/featurepreneur/parquet-on-azure-27725ab1246b
  #blob_client = blob_service_client.get_blob_client(container = container, blob = blob)

  # print(f&#34;Uploading file: {} to key_vault_dict:&#39;{key_vault_dict}&#39; container:&#39;{container},&#39; and blob:&#39;{blob}&#39; as bytes&#34;)
  import inspect
  
  csv_args = list(inspect.signature(pd.DataFrame.to_csv).parameters)
  kwargs_csv = {k: kwargs.pop(k) for k in dict(kwargs) if k in csv_args}

  parq_args = list(inspect.signature(pd.DataFrame.to_parquet).parameters)
  kwargs_parq = {k: kwargs.pop(k) for k in dict(kwargs) if k in parq_args}

  xls_args = list(inspect.signature(pd.DataFrame.to_excel).parameters)
  kwargs_xls = {k: kwargs.pop(k) for k in dict(kwargs) if k in xls_args}

  blob_args = list(inspect.signature(blob_client.upload_blob).parameters)
  kwargs_blob = {k: kwargs.pop(k) for k in dict(kwargs) if k in blob_args}
  
  if blob.split(&#39;.&#39;)[1] == &#39;csv&#39;:
    if blob_client.exists() and append:
      blob_client.upload_blob(data=data.to_csv(header=False, **kwargs_csv),
                              **kwargs_blob,
                              blob_type=&#34;AppendBlob&#34;
                              )
    else:
      blob_client.upload_blob(data=data.to_csv(**kwargs_csv),
                              **kwargs_blob, overwrite=overwrite,
                              blob_type=&#34;AppendBlob&#34;
                              )
  elif blob.split(&#39;.&#39;)[1] == &#39;parquet&#39;:
    if blob_client.exists() and append:
      df_current = blob2pd(blob_dict)
      df_current = pd.concat([df_current, data],axis=0)
      blob_client.upload_blob(data=df_current.to_parquet(**kwargs_parq),
                              overwrite=True,
                              **kwargs_blob)
    else:
      blob_client.upload_blob(data=data.to_parquet(**kwargs_parq),
                              overwrite=overwrite,
                              **kwargs_blob, 
                              ) 

  elif blob.split(&#39;.&#39;)[1]==&#39;xls&#39; :
    sys.exit(&#34;the function does not support the old .xls file format, please use xlsx format&#34;)
 
  elif blob.split(&#39;.&#39;)[1]==&#39;xlsx&#39; :
    import openpyxl, io
    if (append) &amp;(~overwrite):
      sys.exit(&#34;the function does not append to an existing excel file, use xls2blob to save multiple dataframes to a excel file&#34;)    
    else:
      output = io.BytesIO()
      with pd.ExcelWriter(output, engine=&#39;xlsxwriter&#39;) as writer:
          data.to_excel(writer, sheet_name=sheetName, **kwargs_xls)
      xlsx_data = output.getvalue() 
      blob_client.upload_blob(data=xlsx_data,
                                overwrite=True,
                                **kwargs_blob,
                                )
  else:
      print(&#34;Append option is not usable&#34;)
      blob_client.upload_blob(data=data,
                              overwrite=overwrite,
                              **kwargs_blob,
                              )        
  return blob_client.get_blob_properties()</code></pre>
</details>
</dd>
<dt id="io_funcs.pd2blob_batch"><code class="name flex">
<span>def <span class="ident">pd2blob_batch</span></span>(<span>outputs: dict, blob_dict={'container': 'xxx', 'key_vault_dict': 'prdadlafblockmodel'}, append=True, platform='databricks', **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Save pandas dataframes (df) into Azure blob storage using df.write.format command</p>
<h2 id="params">Params</h2>
<p>outputs: (Dictionary)
key: blob path
, value:dataframe</p>
<p>blob_dict:(Dictionary) TargetFile path
: {'key_vault_dict':storage account_name(string) ,
'container':container_name(string) ,
'blob': blob_name(string)
}
</p>
<p>append(boolean):
when append=True, append dataframe to existing file
</p>
<p>write_mode(string)
attached like options to df.write.format command
</p>
<p>**kwargs: args for blob_client.upload_blob see:<a href="https://docs.microsoft.com/en-us/python/api/azure-storage-blob/azure.storage.blob.blobclient?view=azure-python#azure-storage-blob-blobclient-upload-blob">https://docs.microsoft.com/en-us/python/api/azure-storage-blob/azure.storage.blob.blobclient?view=azure-python#azure-storage-blob-blobclient-upload-blob</a></p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def pd2blob_batch(outputs:dict,
                  blob_dict={&#39;container&#39;:&#39;xxx&#39;, 
                              &#39;key_vault_dict&#39;:&#39;prdadlafblockmodel&#39;},
                  append=True ,
                  platform=&#39;databricks&#39;,
                  **kwargs):
  &#34;&#34;&#34;Save pandas dataframes (df) into Azure blob storage using df.write.format command
      Params:
          
      outputs: (Dictionary)  key: blob path   , value:dataframe
      
      blob_dict:(Dictionary) TargetFile path   : {&#39;key_vault_dict&#39;:storage account_name(string) ,
                                                  &#39;container&#39;:container_name(string) ,
                                                  &#39;blob&#39;: blob_name(string) 
                                                  }  

      append(boolean):      when append=True, append dataframe to existing file   

      write_mode(string)  attached like options to df.write.format command    

      **kwargs: args for blob_client.upload_blob see:https://docs.microsoft.com/en-us/python/api/azure-storage-blob/azure.storage.blob.blobclient?view=azure-python#azure-storage-blob-blobclient-upload-blob
  &#34;&#34;&#34;    
  for out in outputs:
    try:
      blob_dict[&#39;blob&#39;]=out
      pd2blob(outputs.get(out),
              blob_dict=blob_dict,
              platform=platform,
              append=append,
              **kwargs
              )
      print(f&#39;{out} saved&#39;)
    except Exception as e:
      print(f&#39;***writing {out} failed: \n\t\t {str(e)}&#39;)
      pass</code></pre>
</details>
</dd>
<dt id="io_funcs.pi2pd_interpolate"><code class="name flex">
<span>def <span class="ident">pi2pd_interpolate</span></span>(<span>tags, start_date='2024-01-30', end_date='2024-02-27', interval='1h', pi_vault_dict='webapi', custom_config=None, platform='databricks')</span>
</code></dt>
<dd>
<div class="desc"><p>Call PI Cloud API's stream controller to get interpolated data for tags<br>
Get pi tag data according to desired frequency</p>
<p>Params:</p>
<p>tags(list)
: maximum 11 tags at a time</p>
<p>start_date (string)
: start date, format: "%Y-%m-%d"</p>
<p>end_date
(string)
: end date, format: "%Y-%m-%d"</p>
<p>pi_vault_dict(String)
: key_vault_dict dictionary
name for webapi in azure_valuet_cred dictionary
</p>
<p>interval(string)
'1h'
: get interpolated data (default hourly)</p>
<p>custom_config(dict/filePath): a dictionary of configuration credentials or path of a yaml file, if it is not provided, dsToolbox.config_dict will be used instead</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def pi2pd_interpolate(tags,
                    start_date=par.start_date, end_date=par.end_date,
                    interval = &#39;1h&#39;,
                    pi_vault_dict=&#39;webapi&#39;, 
                    custom_config=None,
                    platform=&#39;databricks&#39;):
  &#34;&#34;&#34;
  Call PI Cloud API&#39;s stream controller to get interpolated data for tags  
  Get pi tag data according to desired frequency
  
  Params:
  
  tags(list)                  : maximum 11 tags at a time
    
  start_date (string)          : start date, format: &#34;%Y-%m-%d&#34;
  
  end_date   (string)          : end date, format: &#34;%Y-%m-%d&#34;
  
  pi_vault_dict(String)   : key_vault_dict dictionary  name for webapi in azure_valuet_cred dictionary     
  
  interval(string)  &#39;1h&#39;      : get interpolated data (default hourly)
  
  custom_config(dict/filePath): a dictionary of configuration credentials or path of a yaml file, if it is not provided, dsToolbox.config_dict will be used instead
  
  &#34;&#34;&#34; 
  import requests, urllib, json
  if isinstance(start_date,str):
    start_date   = dt.datetime.strptime(start_date, &#34;%Y-%m-%d&#34;) ###&#39;2021-12-31&#39;, &#34;%Y-%m-%d&#34;
  
  if isinstance(end_date, str):
    end_date     = dt.datetime.strptime(end_date,   &#34;%Y-%m-%d&#34;) ###&#39;2021-12-31&#39;, &#34;%Y-%m-%d&#34;

  if not isinstance(tags, list):tags=[tags]

  c = cred_strings(key_vault_dict=pi_vault_dict,
                   custom_config=custom_config,
                   platform=platform)
  accessToken = c.pi_server_connector()
  web_ids     = get_web_ids(accessToken, tags)
  tagData     = {}
  for i, tag in enumerate(tags):
    webID = web_ids[tag]
    print(&#39;tag=&#39;,tag,&#34;,webID=&#34;,f&#34;{webID[:5]}...{webID[20:25]}...{webID[-5:]}&#34;)
    if webID is not None:
      query_dict = {&#39;startTime&#39;:start_date, &#39;endTime&#39;:end_date, &#39;interval&#39;:interval}
      url = &#39;https://svc.apiproxy.exxonmobil.com/KRLPIV01/v1/piwebapi/streams/&#39;+webID+&#39;/interpolated?&#39;+urllib.parse.urlencode(query_dict)
      headers = {
        &#39;Authorization&#39;:&#39;Bearer &#39; + accessToken,
        &#39;Content-Type&#39;:&#39;application/json&#39;}
      response = requests.request(&#34;GET&#34;, url, headers=headers)
      json_str = json.loads(response.text)
      if i == 0:
          tagData[&#39;Date&#39;] = [j[&#39;Timestamp&#39;] for j in json_str[&#39;Items&#39;]]
      if isinstance(json_str[&#39;Items&#39;][0][&#39;Value&#39;], dict):
          tagData[tag] =  [j[&#39;Value&#39;][&#39;Name&#39;] if isinstance(j[&#39;Value&#39;], dict) else j[&#39;Value&#39;] for j in json_str[&#39;Items&#39;]]
      else:
          tagData[tag] = [j[&#39;Value&#39;] for j in json_str[&#39;Items&#39;]]
  df = pd.DataFrame(tagData, columns = tagData.keys())
  df[&#39;Date&#39;] = pd.to_datetime(df[&#39;Date&#39;]).dt.tz_convert(&#39;US/Mountain&#39;)
  return df  #, web_ids</code></pre>
</details>
</dd>
<dt id="io_funcs.pi2pd_rawData"><code class="name flex">
<span>def <span class="ident">pi2pd_rawData</span></span>(<span>tags, start_date='2024-01-30', end_date='2024-02-27', pi_vault_dict='webapi', custom_config=None, platform='databricks')</span>
</code></dt>
<dd>
<div class="desc"><p>Call PI Cloud API's stream controller to get tags in with their original frequency</p>
<h2 id="params">Params</h2>
<p>tags: (list)
: maximum 11 tags at a time: </p>
<p>start_date (string)
: start date, format: "%Y-%m-%d"</p>
<p>end_date
(string)
: end date, format: "%Y-%m-%d"</p>
<p>pi_vault_dict(Dictionary)
: key_vault_dict(string) key name for webapi in azure_valuet_cred dictionary
</p>
<p>custom_config(dict/filePath): a dictionary of configuration credentials or path of a yaml file, if it is not provided, dsToolbox.config_dict will be used instead</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def pi2pd_rawData(tags,
                start_date=par.start_date, end_date=par.end_date,
                pi_vault_dict=&#39;webapi&#39;,
                custom_config=None,
                platform=&#39;databricks&#39;):
  &#34;&#34;&#34;Call PI Cloud API&#39;s stream controller to get tags in with their original frequency

  Params:
    tags: (list)                : maximum 11 tags at a time: 
    
    start_date (string)          : start date, format: &#34;%Y-%m-%d&#34;
    
    end_date   (string)          : end date, format: &#34;%Y-%m-%d&#34;
        
    pi_vault_dict(Dictionary)   : key_vault_dict(string) key name for webapi in azure_valuet_cred dictionary     
    
    custom_config(dict/filePath): a dictionary of configuration credentials or path of a yaml file, if it is not provided, dsToolbox.config_dict will be used instead
  
  &#34;&#34;&#34; 
  #   start_date=&#39;2023-07-13&#39;
  #   end_date=&#39;2023-08-10&#39;
  #   tags=PI_WEB_API_TAGS.keys()
  #   pi_vault_dict=&#39;webapi&#39;
  #   platform=&#39;local&#39;

  import requests, urllib, json
  if isinstance(start_date,str):
    start_date   = dt.datetime.strptime(start_date, &#34;%Y-%m-%d&#34;) ###&#39;2021-12-31&#39;, &#34;%Y-%m-%d&#34;
  
  if isinstance(end_date, str):
    end_date     = dt.datetime.strptime(end_date,   &#34;%Y-%m-%d&#34;) ###&#39;2021-12-31&#39;, &#34;%Y-%m-%d&#34;

  if not isinstance(tags, list):tags=[tags]

  c = cred_strings(key_vault_dict=pi_vault_dict,
                   custom_config=custom_config,
                   platform=platform)
  accessToken = c.pi_server_connector()
  web_ids     = get_web_ids(accessToken, tags)
  
  data_entries = []
  for tag_name in tags:
    webID = web_ids[tag_name]
    print(&#39;tag=&#39;,tag_name,&#34;,webID=&#34;,f&#34;{webID[:5]}...{webID[20:25]}...{webID[-5:]}&#34;)
    if webID is not None:
      query_dict = {&#39;startTime&#39;:start_date, &#39;endTime&#39;:end_date}
      url = &#39;https://svc.apiproxy.exxonmobil.com/KRLPIV01/v1/piwebapi/streams/&#39;+webID+&#39;/recorded?&#39;+urllib.parse.urlencode(query_dict)
      headers = {
        &#39;Authorization&#39;:&#39;Bearer &#39; + accessToken,
        &#39;Content-Type&#39;:&#39;application/json&#39;}
      response = requests.request(&#34;GET&#34;, url, headers=headers)
      if response.status_code == 200:
        json_str = json.loads(response.text)
        # print(json_str)
        if &#39;Items&#39; in json_str:
          for entry in json_str[&#39;Items&#39;]:
            timestamp = entry[&#39;Timestamp&#39;]
            value = entry[&#39;Value&#39;]
            if isinstance(value, dict):
              if value.get(&#39;Name&#39;) == &#39;Bad&#39;:
                value = None
              else:
                value = value.get(&#39;Value&#39;)
            data_entries.append({&#39;Timestamp&#39;: timestamp, tag_name: value})
  df = pd.DataFrame(data_entries)
  df[&#39;Timestamp&#39;] = pd.to_datetime(df[&#39;Timestamp&#39;]).dt.tz_convert(&#39;US/Mountain&#39;)
  return df</code></pre>
</details>
</dd>
<dt id="io_funcs.pi2pd_seconds"><code class="name flex">
<span>def <span class="ident">pi2pd_seconds</span></span>(<span>tags, start_date='2024-01-30', end_date='2024-02-27', pi_vault_dict='webapi', custom_config=None, platform='databricks')</span>
</code></dt>
<dd>
<div class="desc"><p>Call PI Cloud API's stream controller to get tags in every second
maximum 11 tags at a time get pi data by second</p>
<h2 id="params">Params</h2>
<p>tags: (list)
: maximum 11 tags at a time: </p>
<p>start_date (string)
: start date, format: "%Y-%m-%d"</p>
<p>end_date
(string)
: end date, format: "%Y-%m-%d"</p>
<p>pi_vault_dict(Dictionary)
: key_vault_dict(string) key name for webapi in azure_valuet_cred dictionary
</p>
<p>custom_config(dict/filePath): a dictionary of configuration credentials or path of a yaml file, if it is not provided, dsToolbox.config_dict will be used instead</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def pi2pd_seconds(tags,
                start_date=par.start_date, end_date=par.end_date,
                pi_vault_dict=&#39;webapi&#39;,
                custom_config=None,
                platform=&#39;databricks&#39;):
  &#34;&#34;&#34;Call PI Cloud API&#39;s stream controller to get tags in every second
  maximum 11 tags at a time get pi data by second

  Params:
    tags: (list)                : maximum 11 tags at a time: 
    
    start_date (string)          : start date, format: &#34;%Y-%m-%d&#34;
    
    end_date   (string)          : end date, format: &#34;%Y-%m-%d&#34;
        
    pi_vault_dict(Dictionary)   : key_vault_dict(string) key name for webapi in azure_valuet_cred dictionary   
    
    custom_config(dict/filePath): a dictionary of configuration credentials or path of a yaml file, if it is not provided, dsToolbox.config_dict will be used instead
    
  &#34;&#34;&#34; 
  #   start_date=&#39;2023-07-13&#39;
  #   end_date=&#39;2023-08-10&#39;
  #   tags=PI_WEB_API_TAGS.keys()
  #   pi_vault_dict=&#39;webapi&#39;
  #   platform=&#39;local&#39;

  start_date   = dt.datetime.strptime(start_date, &#34;%Y-%m-%d&#34;) ###&#39;2021-12-31&#39;, &#34;%Y-%m-%d&#34;
  end_date     = dt.datetime.strptime(end_date,   &#34;%Y-%m-%d&#34;) ###&#39;2021-12-31&#39;, &#34;%Y-%m-%d&#34;

  start = start_date
  end = start + dt.timedelta(days=1)
  ret = pd.DataFrame()
  while(end &lt;= end_date):
    print(f&#34;getting data between {start} and {end}&#34;)
    try:
        
      pi_web_df = pi2pd_interpolate(tags,
                                    start_date=start, end_date=end,
                                    interval = &#39;1s&#39;,
                                    pi_vault_dict=pi_vault_dict, 
                                    custom_config=custom_config,
                                    platform=platform)
      print(&#34;Done&#34;)
    except:
      print(&#34;Skipped&#34;)
      start = end
      end += dt.timedelta(days=1)
      continue
    ret = pd.concat([ret, pi_web_df], ignore_index=True)
    start = end
    end += dt.timedelta(days=1)
  # ret.drop_duplicates(inplace=True) 
  return ret</code></pre>
</details>
</dd>
<dt id="io_funcs.query_deltaTable"><code class="name flex">
<span>def <span class="ident">query_deltaTable</span></span>(<span>query: str, key_vault_dict: str = 'azure_synapse', custom_config=None, verbose=True)</span>
</code></dt>
<dd>
<div class="desc"><p>Run a Query in azure synapse</p>
<h2 id="params">Params</h2>
<p>query: (string) SQL query string</p>
<p>key_vault_dict(string) dictionary name in config.yml</p>
<p>custom_config(dict/filePath): a dictionary of configuration credentials or path of a yaml file, if it is not provided, dsToolbox.config_dict will be used instead</p>
<p>verbose:(Boolean)
an option for producing detailed information</p>
<p>Returns: a pandas dataframe</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def query_synapse_local(query: str,
                        key_vault_dict: str =&#39;azure_synapse&#39;,
                        custom_config=None,
                        verbose=True,
                        ):
  &#34;&#34;&#34;Run a Query in azure synapse
  
    Params:
    
      query: (string) SQL query string
      
      key_vault_dict(string) dictionary name in config.yml
      
      custom_config(dict/filePath): a dictionary of configuration credentials or path of a yaml file, if it is not provided, dsToolbox.config_dict will be used instead

      verbose:(Boolean)  an option for producing detailed information
      
    Returns: a pandas dataframe
  &#34;&#34;&#34;
  # query=query.strip()
  # if (query[-5:]==&#39;query&#39;):
  #   query=query[1:-5]

  c=cred_strings(key_vault_dict=key_vault_dict,
                 custom_config=custom_config,
                 platform=&#39;local&#39;)
  _, _, cnstr=c.synapse_connector()

  query = clean_query(query)  
  if verbose: print(&#34;pulling data from azure_synapse:\n&#34;, query) 

  import pyodbc
  con = pyodbc.connect(cnstr)
  df  = pd.read_sql(query,con)
  con.close()
  return df</code></pre>
</details>
</dd>
<dt id="io_funcs.query_synapse"><code class="name flex">
<span>def <span class="ident">query_synapse</span></span>(<span>query: str, platform='databricks', key_vault_dict: str = 'azure_synapse', custom_config=None, verbose=True)</span>
</code></dt>
<dd>
<div class="desc"><p>Run a Query in azure synapse</p>
<h2 id="params">Params</h2>
<p>query: (string) SQL query string</p>
<p>verbose:(Boolean)
an option for producing detailed information</p>
<p>Returns: a spark dataframe in databricks or pandas dataframe in local/ vm_docker</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def query_synapse(query: str,
                  platform=&#39;databricks&#39;,
                  key_vault_dict: str =&#39;azure_synapse&#39;,
                  custom_config=None,
                  verbose=True):
  &#34;&#34;&#34;Run a Query in azure synapse

  Params:
    
    query: (string) SQL query string
    
    verbose:(Boolean)  an option for producing detailed information
    
  Returns: a spark dataframe in databricks or pandas dataframe in local/ vm_docker
  &#34;&#34;&#34;
  
  if platform==&#39;databricks&#39;:
    query_synapse_db(query,
                    key_vault_dict=key_vault_dict,
                    custom_config=custom_config,
                    verbose=verbose,
                    )
  elif (platform==&#39;local&#39;) or (platform==&#39;vm_docker&#39;):
    query_synapse_local(query,
                        key_vault_dict=key_vault_dict,
                        custom_config=custom_config,
                        verbose=verbose,
                        )</code></pre>
</details>
</dd>
<dt id="io_funcs.query_synapse_db"><code class="name flex">
<span>def <span class="ident">query_synapse_db</span></span>(<span>query: str, key_vault_dict: str = 'azure_synapse', custom_config=None, verbose=True)</span>
</code></dt>
<dd>
<div class="desc"><p>Run a Query in azure synapse</p>
<p>Params:
query: (string) SQL query string</p>
<p>key_vault_dict(string) dictionary name in config.yml</p>
<p>verbose:(Boolean)
an option for producing detailed information</p>
<p>custom_config(dict/filePath): a dictionary of configuration credentials or path of a yaml file, if it is not provided, dsToolbox.config_dict will be used instead</p>
<p>Returns: a spark dataframe</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def query_synapse_db(query: str,
                    key_vault_dict: str =&#39;azure_synapse&#39;,
                    custom_config=None,
                    verbose=True,
                    ):
  &#34;&#34;&#34;Run a Query in azure synapse
  
    Params:
     query: (string) SQL query string
      
      key_vault_dict(string) dictionary name in config.yml
      
      verbose:(Boolean)  an option for producing detailed information
      
      custom_config(dict/filePath): a dictionary of configuration credentials or path of a yaml file, if it is not provided, dsToolbox.config_dict will be used instead
      
    Returns: a spark dataframe
  &#34;&#34;&#34;
  c=cred_strings(key_vault_dict=key_vault_dict,
                 custom_config=custom_config,
                 platform=&#39;databricks&#39;)
  url, properties, _=c.synapse_connector()

  query=f&#39;({query}) query&#39; if (query.strip()[-5:]!=&#39;query&#39;) or (query.strip()[0]!=&#39;(&#39;) else query 
  if verbose: print(&#34;pulling data from azure_synapse:\n&#34;, query) 

  spark, sqlContext=get_spark()
  df  = spark.read.jdbc(table=query, url=url, properties=properties)

  ###----for local:
  # query = clean_query(query)  
  # import pyodbc
  # con = pyodbc.connect(cnstr)
  # df  = pd.read_sql(query,con)
  # con.close()

  return df  </code></pre>
</details>
</dd>
<dt id="io_funcs.query_synapse_local"><code class="name flex">
<span>def <span class="ident">query_synapse_local</span></span>(<span>query: str, key_vault_dict: str = 'azure_synapse', custom_config=None, verbose=True)</span>
</code></dt>
<dd>
<div class="desc"><p>Run a Query in azure synapse</p>
<h2 id="params">Params</h2>
<p>query: (string) SQL query string</p>
<p>key_vault_dict(string) dictionary name in config.yml</p>
<p>custom_config(dict/filePath): a dictionary of configuration credentials or path of a yaml file, if it is not provided, dsToolbox.config_dict will be used instead</p>
<p>verbose:(Boolean)
an option for producing detailed information</p>
<p>Returns: a pandas dataframe</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def query_synapse_local(query: str,
                        key_vault_dict: str =&#39;azure_synapse&#39;,
                        custom_config=None,
                        verbose=True,
                        ):
  &#34;&#34;&#34;Run a Query in azure synapse
  
    Params:
    
      query: (string) SQL query string
      
      key_vault_dict(string) dictionary name in config.yml
      
      custom_config(dict/filePath): a dictionary of configuration credentials or path of a yaml file, if it is not provided, dsToolbox.config_dict will be used instead

      verbose:(Boolean)  an option for producing detailed information
      
    Returns: a pandas dataframe
  &#34;&#34;&#34;
  # query=query.strip()
  # if (query[-5:]==&#39;query&#39;):
  #   query=query[1:-5]

  c=cred_strings(key_vault_dict=key_vault_dict,
                 custom_config=custom_config,
                 platform=&#39;local&#39;)
  _, _, cnstr=c.synapse_connector()

  query = clean_query(query)  
  if verbose: print(&#34;pulling data from azure_synapse:\n&#34;, query) 

  import pyodbc
  con = pyodbc.connect(cnstr)
  df  = pd.read_sql(query,con)
  con.close()
  return df</code></pre>
</details>
</dd>
<dt id="io_funcs.query_template_run"><code class="name flex">
<span>def <span class="ident">query_template_run</span></span>(<span>query_temp_name: str, replace_dict: dict = {'start___date': '2024-01-30', 'end___date': '2024-02-27'}, custom_config=None, custom_sql_template_yml=None, platform='databricks')</span>
</code></dt>
<dd>
<div class="desc"><p>Run a Query in deltaTable or azure synapse</p>
<h2 id="params">Params</h2>
<p>query_temp_name: (dictionary) the name of dictionary that contains query template</p>
<p>custom_config(dict/filePath): a dictionary of configuration credentials or path of a yaml file, if it is not provided, dsToolbox.config_dict will be used instead</p>
<p>custom_sql_template_yml(path): a path of predefined
SQL qureies, if it is not provided, dsToolbox.sql_template_dict will be used instead</p>
<p>replace_dict:(dictionary) it is used to replace start and end date</p>
<p>Returns: a spark dataframe</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def query_template_run(query_temp_name: str,
                      replace_dict: dict={&#39;start___date&#39;:par.start_date,
                                          &#39;end___date&#39;  :par.end_date
                                          },
                      custom_config=None,
                      custom_sql_template_yml=None,
                      platform=&#39;databricks&#39;,
                      ):
  &#34;&#34;&#34;Run a Query in deltaTable or azure synapse
    Params:
      query_temp_name: (dictionary) the name of dictionary that contains query template
      
      custom_config(dict/filePath): a dictionary of configuration credentials or path of a yaml file, if it is not provided, dsToolbox.config_dict will be used instead
      
      custom_sql_template_yml(path): a path of predefined  SQL qureies, if it is not provided, dsToolbox.sql_template_dict will be used instead
      
      replace_dict:(dictionary) it is used to replace start and end date
      
    Returns: a spark dataframe
  &#34;&#34;&#34;
  
  if custom_sql_template_yml is None:
    with res.open_binary(&#39;dsToolbox&#39;, &#39;sql_template.yml&#39;) as fp:
      sql_template_dict = yaml.load(fp, Loader=yaml.Loader)
  else:
    from pathlib import Path
    sql_template_dict = yaml.safe_load(Path(custom_sql_template_yml).read_text())

  tmp=sql_template_dict.get(query_temp_name)
  host, query_str = tmp[&#39;db&#39;], tmp[&#39;query&#39;] 

  query=query_template_reader(query_str,
                            replace_dict=replace_dict
                            )
  if host==&#39;azure_synapse&#39;:
    if platform==&#39;databricks&#39;:
      df=query_synapse_db(
                          query,
                          key_vault_dict=host,
                          custom_config=custom_config,
                          verbose=True,
                          )
    elif (platform==&#39;local&#39;) or (platform==&#39;vm_docker&#39;):
      df=query_synapse_local(query,
                            key_vault_dict=host,
                            custom_config=custom_config,
                            verbose=True,
                            )
  else:
    df=query_deltaTable_db(
                          query,
                          key_vault_dict=host,
                          custom_config=custom_config,
                          )  
  return df</code></pre>
</details>
</dd>
<dt id="io_funcs.spark2blob"><code class="name flex">
<span>def <span class="ident">spark2blob</span></span>(<span>df, blob_dict: dict, write_mode: str = "mode('append')", custom_config=None, platform='databricks')</span>
</code></dt>
<dd>
<div class="desc"><p>Save spark dataframe (df) into Azure blob storage using df.write.format command
Params:</p>
<p>df: park dataframe</p>
<p>blob_dict:(Dictionary) TargetFile path
: {'key_vault_dict':storage account_name(string) ,
'container':container_name(string) ,
'blob': blob_name(string)
}
</p>
<p>write_mode(string)
attached like options to df.write.format command
</p>
<p>custom_config(dict/filePath): a dictionary of configuration credentials or path of a yaml file, if it is not provided, dsToolbox.config_dict will be used instead</p>
<p>Returns: None</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def spark2blob(df,
              blob_dict:dict,
              write_mode:str = &#34;mode(&#39;append&#39;)&#34;,
              custom_config=None,
              platform=&#39;databricks&#39;
              ):
  &#34;&#34;&#34;Save spark dataframe (df) into Azure blob storage using df.write.format command
    Params:
    
    df: park dataframe
    
    blob_dict:(Dictionary) TargetFile path   : {&#39;key_vault_dict&#39;:storage account_name(string) ,
                                                  &#39;container&#39;:container_name(string) ,
                                                  &#39;blob&#39;: blob_name(string) 
                                                  }  

    write_mode(string)  attached like options to df.write.format command   
    
    custom_config(dict/filePath): a dictionary of configuration credentials or path of a yaml file, if it is not provided, dsToolbox.config_dict will be used instead

    Returns: None
  &#34;&#34;&#34;          
  storage_account = blob_dict.get(&#39;storage_account&#39;)
  container       = blob_dict.get(&#39;container&#39;)  
  blob            = blob_dict.get(&#39;blob&#39;)

  c = cred_strings(key_vault_dict=storage_account,
                   custom_config=custom_config,
                   platform=platform)
  blob_host, blob_path, blob_connectionStr=c.blob_connector(blob, container)
  spark, sqlContext=get_spark()
  spark.conf.set(blob_host, c.password)
  extension=blob.split(&#39;.&#39;)[-1]
  
  string_run=f&#34;&#34;&#34;df.write.format(&#39;{extension}&#39;)\
                .{write_mode}
              &#34;&#34;&#34;

  string_run=f&#34;&#34;&#34;{string_run}
                  .save(blob_path)&#34;&#34;&#34;  
  string_run=re.sub(r&#34;[\n\t\s]*&#34;, &#34;&#34;, string_run)
  print(&#34;running:\n&#34;, string_run)
  eval(string_run)</code></pre>
</details>
</dd>
<dt id="io_funcs.spark2deltaTable"><code class="name flex">
<span>def <span class="ident">spark2deltaTable</span></span>(<span>df, table_name: str, schema: str = 'xxx_analytics', write_mode: str = 'append', partitionby: list = None, **options)</span>
</code></dt>
<dd>
<div class="desc"><p>Writes to databricks delta tables</p>
<p>Params:
df
: Spark Dataframe to write to delta table</p>
<pre><code>  database     : (string) Name of database

  table        : (string) Name of table. Creates a new table if it doesn't exist

  write_mode   : (string) Write mode ('append' or 'overwrite'). Default is 'append'

  partitionby  : (list) list of Column names to partition by.

  **options    : (dict) all other string options
</code></pre>
<p>Return: None</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def spark2deltaTable(df, table_name: str, schema: str = &#39;xxx_analytics&#39;,
                    write_mode:str = &#39;append&#39;, partitionby:list = None, 
                    **options
                    ):
  &#34;&#34;&#34; Writes to databricks delta tables
  
    Params: 
          df           : Spark Dataframe to write to delta table
            
          database     : (string) Name of database
            
          table        : (string) Name of table. Creates a new table if it doesn&#39;t exist
            
          write_mode   : (string) Write mode (&#39;append&#39; or &#39;overwrite&#39;). Default is &#39;append&#39;
            
          partitionby  : (list) list of Column names to partition by.

          **options    : (dict) all other string options 
            
    Return: None
  &#34;&#34;&#34;
  spark, sqlContext=get_spark()
  spark.sql(f&#34;CREATE DATABASE IF NOT EXISTS {schema}&#34;)

  if partitionby is None:
    partitionBy=&#39;&#39;
  else:
    partitionby=[partitionby] if not isinstance(partitionby, list) else partitionby
    partitionBy=f&#39;, partitionby={partitionby}&#39;

  eval(f&#34;df.write.saveAsTable(&#39;{schema}.{table_name}&#39;, mode=&#39;{write_mode}&#39; {partitionBy} , **{options})&#34;)
  
  return</code></pre>
</details>
</dd>
<dt id="io_funcs.xls2blob"><code class="name flex">
<span>def <span class="ident">xls2blob</span></span>(<span>dataframe_dict: dict, blob_dict: dict, overwrite=True, custom_config=None, platform='databricks', **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Save pandas dataframe(s) as a excel file into Azure blob storage
Params:</p>
<p>dataframe_dict:
dictionary of sheet_name annd corresponding dataframes to write</p>
<p>blob_dict:(Dictionary) TargetFile path
: {'key_vault_dict':storage account_name(string) ,
'container':container_name(string) ,
'blob': blob_name(string)
}
</p>
<p>overwrite(boolean):
when overwrite=True, overwrite to
existing file
</p>
<p>custom_config(dict/filePath): a dictionary of configuration credentials or path of a yaml file, if it is not provided, dsToolbox.config_dict will be used instead</p>
<p>**kwargs: args for :
blob_client.upload_blob see:<a href="https://docs.microsoft.com/en-us/python/api/azure-storage-blob/azure.storage.blob.blobclient?view=azure-python#azure-storage-blob-blobclient-upload-blob">https://docs.microsoft.com/en-us/python/api/azure-storage-blob/azure.storage.blob.blobclient?view=azure-python#azure-storage-blob-blobclient-upload-blob</a>
pandas.DataFrame.to_excel</p>
<p>Returns: get_blob_properties</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def xls2blob(dataframe_dict: dict,
            blob_dict:dict,
            overwrite=True,
            custom_config=None,
            platform=&#39;databricks&#39;,
            **kwargs
          ):
  &#34;&#34;&#34;Save pandas dataframe(s) as a excel file into Azure blob storage
    Params:
    
    dataframe_dict:  dictionary of sheet_name annd corresponding dataframes to write
    
    blob_dict:(Dictionary) TargetFile path   : {&#39;key_vault_dict&#39;:storage account_name(string) ,
                                                  &#39;container&#39;:container_name(string) ,
                                                  &#39;blob&#39;: blob_name(string) 
                                                  }  
    
    overwrite(boolean):    when overwrite=True, overwrite to  existing file  

    custom_config(dict/filePath): a dictionary of configuration credentials or path of a yaml file, if it is not provided, dsToolbox.config_dict will be used instead

    **kwargs: args for :
      blob_client.upload_blob see:https://docs.microsoft.com/en-us/python/api/azure-storage-blob/azure.storage.blob.blobclient?view=azure-python#azure-storage-blob-blobclient-upload-blob
      pandas.DataFrame.to_excel
                    
    Returns: get_blob_properties
  &#34;&#34;&#34;      
  ###using BlobServiceClient    
  from azure.storage.blob import BlobServiceClient
  storage_account = blob_dict.get(&#39;storage_account&#39;)
  container       = blob_dict.get(&#39;container&#39;)  
  blob            = blob_dict.get(&#39;blob&#39;)

  c = cred_strings(key_vault_dict=storage_account,
                   custom_config=custom_config,
                   platform=platform)
  blob_host, blob_path, blob_connectionStr=c.blob_connector(blob, container)
  blob_service_client = BlobServiceClient.from_connection_string(blob_connectionStr)
  container_client    = blob_service_client.get_container_client(container)
  
  blob_client=container_client.get_blob_client(blob)    

  import inspect
  import sys, io
  import pandas as pd
    
  blob_args = list(inspect.signature(blob_client.upload_blob).parameters)
  kwargs_blob = {k: kwargs.pop(k) for k in dict(kwargs) if k in blob_args}
  
  xls_args = list(inspect.signature(pd.DataFrame.to_excel).parameters)
  kwargs_xls = {k: kwargs.pop(k) for k in dict(kwargs) if k in xls_args}

  output = io.BytesIO()
  writer =pd.ExcelWriter(output, engine=&#39;xlsxwriter&#39;)
  for sheetName, df in  dataframe_dict.items(): 
    df.to_excel(writer, sheet_name=sheetName, **kwargs_xls)
        
  writer.close()
  xlsx_data = output.getvalue() 
  blob_client.upload_blob(data=xlsx_data,
                          overwrite=overwrite,
                          **kwargs_blob,
                          )
  return blob_client.get_blob_properties()</code></pre>
</details>
</dd>
</dl>
</section>
<section>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="two-column">
<li><code><a title="io_funcs.blob2pd" href="#io_funcs.blob2pd">blob2pd</a></code></li>
<li><code><a title="io_funcs.blob2spark" href="#io_funcs.blob2spark">blob2spark</a></code></li>
<li><code><a title="io_funcs.blob_check" href="#io_funcs.blob_check">blob_check</a></code></li>
<li><code><a title="io_funcs.dbfs2blob" href="#io_funcs.dbfs2blob">dbfs2blob</a></code></li>
<li><code><a title="io_funcs.deltaTable_check" href="#io_funcs.deltaTable_check">deltaTable_check</a></code></li>
<li><code><a title="io_funcs.pd2blob" href="#io_funcs.pd2blob">pd2blob</a></code></li>
<li><code><a title="io_funcs.pd2blob_batch" href="#io_funcs.pd2blob_batch">pd2blob_batch</a></code></li>
<li><code><a title="io_funcs.pi2pd_interpolate" href="#io_funcs.pi2pd_interpolate">pi2pd_interpolate</a></code></li>
<li><code><a title="io_funcs.pi2pd_rawData" href="#io_funcs.pi2pd_rawData">pi2pd_rawData</a></code></li>
<li><code><a title="io_funcs.pi2pd_seconds" href="#io_funcs.pi2pd_seconds">pi2pd_seconds</a></code></li>
<li><code><a title="io_funcs.query_deltaTable" href="#io_funcs.query_deltaTable">query_deltaTable</a></code></li>
<li><code><a title="io_funcs.query_synapse" href="#io_funcs.query_synapse">query_synapse</a></code></li>
<li><code><a title="io_funcs.query_synapse_db" href="#io_funcs.query_synapse_db">query_synapse_db</a></code></li>
<li><code><a title="io_funcs.query_synapse_local" href="#io_funcs.query_synapse_local">query_synapse_local</a></code></li>
<li><code><a title="io_funcs.query_template_run" href="#io_funcs.query_template_run">query_template_run</a></code></li>
<li><code><a title="io_funcs.spark2blob" href="#io_funcs.spark2blob">spark2blob</a></code></li>
<li><code><a title="io_funcs.spark2deltaTable" href="#io_funcs.spark2deltaTable">spark2deltaTable</a></code></li>
<li><code><a title="io_funcs.xls2blob" href="#io_funcs.xls2blob">xls2blob</a></code></li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>