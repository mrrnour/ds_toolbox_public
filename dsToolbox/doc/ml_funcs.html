<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>ml_funcs API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>ml_funcs</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import pandas as pd
import numpy as np
import math 

import os ,sys
import re
####------------------------------model performances----------------------------------------------------------------------------------------
##-----------------------------------------------------------------------------------------------------------------------------------------------
##-----------------------------------------------------------------------------------------------------------------------------------------------
##-----------------------------------------------------------------------------------------------------------------------------------------------
from sklearn import metrics

metric_dict={
            &#39;accuracy&#39;                          : metrics.accuracy_score,
            &#39;balanced_accuracy&#39;                 : metrics.balanced_accuracy_score,
            &#39;top_k_accuracy&#39;                    : metrics.top_k_accuracy_score,
            &#39;average_precision&#39;                 : metrics.average_precision_score,
            &#39;aucpr&#39;                             : metrics.average_precision_score,
            &#39;brier_score&#39;                       : metrics.brier_score_loss,
            &#39;f1&#39;                                : metrics.f1_score,
            &#39;f1_micro&#39;                          : metrics.f1_score,
            &#39;f1_macro&#39;                          : metrics.f1_score,
            &#39;f1_weighted&#39;                       : metrics.f1_score,
            &#39;f1_samples&#39;                        : metrics.f1_score,
            &#39;log_loss&#39;                          : metrics.log_loss,
            &#39;precision&#39;                         : metrics.precision_score,
            &#39;recall&#39;                            : metrics.recall_score,
            &#39;jaccard&#39;                           : metrics.jaccard_score,
            &#39;auc&#39;                               : metrics.roc_auc_score,
            &#39;auc_macro&#39;                         : metrics.roc_auc_score,
            &#39;auc_weighted&#39;                      : metrics.roc_auc_score,
            &#39;auc_micro&#39;                         : metrics.roc_auc_score,
            &#39;roc_auc&#39;                           : metrics.roc_auc_score,
            &#39;roc_auc_ovr&#39;                       : metrics.roc_auc_score,
            &#39;roc_auc_ovo&#39;                       : metrics.roc_auc_score,
            &#39;roc_auc_ovr_weighted&#39;              : metrics.roc_auc_score,
            &#39;roc_auc_ovo_weighted&#39;              : metrics.roc_auc_score,
            &#39;adjusted_mutual_info_score&#39;        : metrics.adjusted_mutual_info_score,
            &#39;adjusted_rand_score&#39;               : metrics.adjusted_rand_score,
            &#39;completeness_score&#39;                : metrics.completeness_score,
            &#39;fowlkes_mallows_score&#39;             : metrics.fowlkes_mallows_score,
            &#39;homogeneity_score&#39;                 : metrics.homogeneity_score,
            &#39;mutual_info_score&#39;                 : metrics.mutual_info_score,
            &#39;normalized_mutual_info_score&#39;      : metrics.normalized_mutual_info_score,
            &#39;rand_score&#39;                        : metrics.rand_score,
            &#39;v_measure_score&#39;                   : metrics.v_measure_score,
            
            &#39;explained_variance&#39;                : metrics.explained_variance_score,
            &#39;max_error&#39;                         : metrics.max_error,
            &#39;mean_absolute_error&#39;               : metrics.mean_absolute_error,
            &#39;mean_squared_error&#39;                : metrics.mean_squared_error,
            &#39;mean_squared_log_error&#39;            : metrics.mean_squared_log_error,
            &#39;median_absolute_error&#39;             : metrics.median_absolute_error,
            &#39;R2&#39;                                    : metrics.r2_score,
            &#39;mean_poisson_deviance&#39;             : metrics.mean_poisson_deviance,
            &#39;mean_gamma_deviance&#39;               : metrics.mean_gamma_deviance,
            &#39;mean_absolute_percentage_error&#39;    : metrics.mean_absolute_percentage_error,

            &#39;mcc&#39;                                   : metrics.matthews_corrcoef,
            &#39;kappa&#39;                             : metrics.cohen_kappa_score,

            }

#scoring_dict=dict(zip(scoring,[metrics.make_scorer(metric_dict.get(i)) for i in scoring]))

# from sklearn.metrics import matthews_corrcoef
# from sklearn.metrics import cohen_kappa_score
# from sklearn.metrics import f1_score
# scoring_dict=dict(zip(scoring, scoring))
# ## https://scikit-learn.org/stable/modules/model_evaluation.html#scoring
# scoring_dict[&#39;mcc&#39;]=make_scorer(matthews_corrcoef)
# scoring_dict[&#39;kappa&#39;]=make_scorer(cohen_kappa_score)

##TODO: change y to y_ratio
def classifiers_template(y, random_state=10):
  
  import numpy as np

  from sklearn.pipeline import Pipeline
  from sklearn.preprocessing import StandardScaler
  from sklearn.decomposition import PCA
  from sklearn.impute import SimpleImputer
  
  from sklearn.tree import DecisionTreeClassifier
  from sklearn import tree
  from sklearn.svm import SVC, LinearSVC, NuSVC
  from sklearn.gaussian_process import GaussianProcessClassifier
  from sklearn.gaussian_process.kernels import RBF

  from xgboost import XGBClassifier
  from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier, BaggingClassifier
  from lightgbm import LGBMClassifier

  from sklearn.linear_model import LogisticRegression

  from sklearn.naive_bayes import GaussianNB

  from sklearn.neighbors import KNeighborsClassifier

  from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
  from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis

  from sklearn.neural_network import MLPClassifier
  from sklearn.ensemble import GradientBoostingClassifier

  classifiers={
  &#34;Nearest_Neighbors_2&#34;:            KNeighborsClassifier(2),
  &#34;Nearest_Neighbors_3&#34;:            KNeighborsClassifier(3),
  &#34;Nearest_Neighbors_4&#34;:            KNeighborsClassifier(4),
  &#34;Nearest_Neighbors_5&#34;:            KNeighborsClassifier(5),

  &#34;Decision_Tree_depth5&#34;:         DecisionTreeClassifier(max_depth=5, random_state=random_state),
  &#34;Decision_Tree_depth10&#34;:        DecisionTreeClassifier(max_depth=10, random_state=random_state),

  &#34;Naive_Bayes&#34;:                  GaussianNB(),

  &#34;LinearDiscriminantAnalysis&#34;    :LinearDiscriminantAnalysis(),
  &#34;QuadraticDiscriminantAnalysis&#34; :QuadraticDiscriminantAnalysis(),

  &#39;logReg_mode_l1&#39;   :            LogisticRegression(penalty=&#39;l1&#39;, solver=&#39;liblinear&#39;, max_iter=1000, random_state=random_state),
  &#39;logReg_mode_l2&#39;   :            LogisticRegression(max_iter=1000, random_state=random_state),
  &#39;logReg_model_pca&#39;:             LogisticRegression(max_iter=1000, random_state=random_state),

  &#39;RandomForest_model1&#39;:          RandomForestClassifier(random_state=random_state),
  &#39;RandomForest_model_balanced&#39;:  RandomForestClassifier(class_weight=&#39;balanced&#39;, random_state=random_state),
  &#39;RandomForest_model_n200&#39;:      RandomForestClassifier(n_estimators=200, random_state=random_state),
  &#39;RandomForest_model_n300&#39;:      RandomForestClassifier(n_estimators=300, random_state=random_state),
  &#39;Xgboost_n200&#39; :                XGBClassifier(n_estimators=200, random_state=random_state),
  &#39;Xgboost_n200_dp10&#39;:            XGBClassifier(n_estimators=200, max_depth=10, random_state=random_state) ,
  &#39;Xgboost_Weighted&#39;:             XGBClassifier(scale_pos_weight=float(np.sum(y == 0)) / np.sum(y==1), random_state=random_state) ,
  &#39;Xgboost_Weighted_n200&#39;:        XGBClassifier(n_estimators=200, scale_pos_weight=float(np.sum(y == 0)) / np.sum(y==1), random_state=random_state) ,
  &#39;Xgboost_Weighted_n200_dp10&#39;:   XGBClassifier(n_estimators=200, max_depth=10, scale_pos_weight=float(np.sum(y == 0)) / np.sum(y==1), random_state=random_state) ,

  &#39;LightGBM&#39;:                     LGBMClassifier(random_state=random_state),
  &#39;LightGBM_n200&#39;:                LGBMClassifier(n_estimators=200, random_state=random_state),
  &#39;LightGBM_n400&#39;:                LGBMClassifier(n_estimators=300, random_state=random_state),
  &#39;LightGBM_n1000&#39;:                LGBMClassifier(n_estimators=1000, random_state=random_state),
  &#39;LightGBM_n200_dp10&#39;:           LGBMClassifier(max_depth=10,  n_estimators=200, random_state=random_state),
  &#39;LightGBM_n300_dp10&#39;:           LGBMClassifier(max_depth=10,  n_estimators=300, random_state=random_state),

  &#39;MLPClassifier1&#39;   :            MLPClassifier(alpha=1, max_iter=1000, random_state=random_state),
  &#39;MLPClassifier_early_stopping&#39;   :  MLPClassifier(alpha=1, max_iter=1000, early_stopping=True, random_state=random_state),
  &#39;MLPClassifier3&#39;   :            MLPClassifier(alpha=1, max_iter=1000, solver=&#39;sgd&#39;, early_stopping=True, random_state=random_state),

  # &#34;Linear_SVM&#34;:                  SVC(kernel=&#34;linear&#34;, C=0.025, probability=True, random_state=random_state),
  # &#34;RBF_SVM&#34;:                     SVC(kernel=&#34;rbf&#34;, C=0.025, probability=True, random_state=random_state),
  # &#34;NuSVC&#34;:                       NuSVC(probability=True, random_state=random_state),

  &#34;AdaBoost&#34;:                     AdaBoostClassifier(random_state=random_state),
  # &#39;bagging&#39;:                      BaggingClassifier(estimator=DecisionTreeClassifier(), n_estimators=10, random_state=random_state),

  }

  basic_params = {&#34;random_state&#34;:random_state}

  classifiers2={}
  for name, classifier in classifiers.items():
    ##TODO: find a way to add randomstate here
    # params={**basic_params,**classifier.get_params()}
    # print(params)
    if any([x in name for x in [&#39;pca&#39;,&#39;DiscriminantAnalysis&#39;]]):  
        classifiers2[name]= Pipeline(steps=[(&#34;imputer&#34;, SimpleImputer(strategy=&#34;median&#34;)),
                                            (&#34;scaler&#34;, StandardScaler()),
                                            (&#34;reduce_dims&#34;, PCA(n_components=20)),
                                            (name, classifier)])
        
    elif (&#39;xgb&#39; not in name.lower()) &amp; (&#39;gbm&#39; not in name.lower()) :
        classifiers2[name]= Pipeline(steps=[(&#34;imputer&#34;, SimpleImputer(strategy=&#34;median&#34;)),
                                            (&#34;scaler&#34;, StandardScaler()),
                                            (name, classifier)])
    else:
        classifiers2[name]=classifier
  return classifiers2

def regressors_template(random_state=10):
  from sklearn.tree import DecisionTreeRegressor
  from sklearn.svm import SVR, LinearSVR, NuSVR
  from sklearn.gaussian_process import GaussianProcessRegressor
  from sklearn.gaussian_process.kernels import RBF

  from xgboost import XGBRegressor
  from sklearn.ensemble import AdaBoostRegressor, RandomForestRegressor, BaggingRegressor
  from lightgbm import LGBMRegressor

  from sklearn.linear_model import LinearRegression, Ridge, SGDRegressor, ElasticNet, ElasticNetCV, Lars, Lasso, LassoLars, LassoLarsIC, LassoCV

  from sklearn.naive_bayes import GaussianNB

  from sklearn.neighbors import KNeighborsRegressor

  from sklearn.neural_network import MLPRegressor  
  from sklearn.ensemble import GradientBoostingRegressor

  from sklearn.pipeline import Pipeline
  import numpy as np
  from sklearn.preprocessing import StandardScaler
  from sklearn.decomposition import PCA
  from sklearn.impute import SimpleImputer

  Regressors={
  &#34;Nearest_Neighbors_2&#34;:          KNeighborsRegressor(2),
  &#34;Nearest_Neighbors_3&#34;:          KNeighborsRegressor(3),
  &#34;Nearest_Neighbors_4&#34;:          KNeighborsRegressor(4),
  &#34;Nearest_Neighbors_5&#34;:          KNeighborsRegressor(5),

  &#34;Decision_Tree_depth5&#34;:         DecisionTreeRegressor(max_depth=5, random_state=random_state),
  &#34;Decision_Tree_depth10&#34;:        DecisionTreeRegressor(max_depth=10, random_state=random_state),

  &#34;Naive_Bayes&#34;:                  GaussianNB(),

  &#39;LinearRegression&#39; :            LinearRegression(),
  &#34;Ridge&#34;        :                Ridge(random_state=random_state)        ,
  &#34;SGDRegressor&#34; :                SGDRegressor(random_state=random_state) ,
  &#34;ElasticNet&#34;   :                ElasticNet(random_state=random_state)   ,
  &#34;ElasticNetCV&#34; :                ElasticNetCV(random_state=random_state) ,
  &#34;Lars&#34;         :                Lars(random_state=random_state)         ,
  &#34;Lasso&#34;        :                Lasso()        ,
  &#34;LassoCV&#34;      :                LassoCV(random_state=random_state)    ,
  &#34;LassoLars&#34;    :                LassoLars(random_state=random_state)    ,
  &#34;LassoLarsIC&#34;  :                LassoLarsIC()  ,
  
  &#39;RandomForest_model1&#39;:          RandomForestRegressor(random_state=random_state),
  &#39;RandomForest_model_n200&#39;:      RandomForestRegressor(n_estimators=200, random_state=random_state),
  &#39;RandomForest_model_n300&#39;:      RandomForestRegressor(n_estimators=300, random_state=random_state),
  &#39;Xgboost_n200&#39; :                XGBRegressor(n_estimators=200, random_state=random_state),
  &#39;Xgboost_n200_dp10&#39;:            XGBRegressor(n_estimators=200, max_depth=10, random_state=random_state) ,

  &#39;LightGBM&#39;:                     LGBMRegressor(random_state=random_state),
  &#39;LightGBM_n200&#39;:                LGBMRegressor(n_estimators=200, random_state=random_state),
  &#39;LightGBM_n400&#39;:                LGBMRegressor(n_estimators=300, random_state=random_state),
  &#39;LightGBM_n1000&#39;:               LGBMRegressor(n_estimators=1000, random_state=random_state),
  &#39;LightGBM_n200_dp10&#39;:           LGBMRegressor(max_depth=10,  n_estimators=200, random_state=random_state),
  &#39;LightGBM_n300_dp10&#39;:           LGBMRegressor(max_depth=10,  n_estimators=300, random_state=random_state),

  &#39;MLPRegressor1&#39;   :             MLPRegressor(alpha=1, max_iter=1000, random_state=random_state),
  &#39;MLPRegressor_early_stopping&#39;:  MLPRegressor(alpha=1, max_iter=1000, early_stopping=True, random_state=random_state),
  &#39;MLPRegressor3&#39;   :             MLPRegressor(alpha=1, max_iter=1000, solver=&#39;sgd&#39;, early_stopping=True, random_state=random_state),

  # &#34;Linear_SVM&#34;:                  SVR(kernel=&#34;linear&#34;, C=0.025, probability=True, random_state=random_state),
  # &#34;RBF_SVM&#34;:                     SVR(kernel=&#34;rbf&#34;, C=0.025, probability=True, random_state=random_state),
  # &#34;NuSVR&#34;:                       NuSVR(probability=True, random_state=random_state),

  &#34;AdaBoost&#34;:                     AdaBoostRegressor(random_state=random_state),
  # &#39;bagging&#39;:                      BaggingRegressor(estimator=DecisionTreeRegressor(), n_estimators=10, random_state=random_state),

  }

  basic_params = {&#34;random_state&#34;:random_state}

  regressors2={}
  for name, Regressor in Regressors.items():
    ##TODO: find a way to add randomstate here
    # params={**basic_params,**Regressor.get_params()}
    # print(params)
    if any([x in name for x in [&#39;pca&#39;,&#39;DiscriminantAnalysis&#39;]]):  
        regressors2[name]= Pipeline(steps=[(&#34;imputer&#34;, SimpleImputer(strategy=&#34;median&#34;)),
                                            (&#34;scaler&#34;, StandardScaler()),
                                            (&#34;reduce_dims&#34;, PCA(n_components=20)),
                                            (name, Regressor)])
        
    elif (&#39;xgb&#39; not in name.lower()) &amp; (&#39;gbm&#39; not in name.lower()) :
        regressors2[name]= Pipeline(steps=[(&#34;imputer&#34;, SimpleImputer(strategy=&#34;median&#34;)),
                                            (&#34;scaler&#34;, StandardScaler()),
                                            (name, Regressor)])
    else:
        regressors2[name]=Regressor
  return regressors2

def ml_scores(y_model, scores_names):
  
  if &#39;CV_Iteration&#39; not in y_model.columns:
    y_model[&#39;CV_Iteration&#39;]=&#39;All_data&#39;
     
  scores_all=pd.Series(index=scores_names, dtype=&#39;float64&#39;,name=&#39;scores_all&#39;)
  scores=pd.DataFrame(index=y_model[&#39;CV_Iteration&#39;].unique(), columns=scores_names)
  
  for con, score_name in enumerate(scores_names):
    ##TODO: it is not good practice to catch known error with try:
    try:
      umetric=metric_dict.get(score_name)

      if umetric in [&#39;auc_weighted&#39;, &#39;auc_micro&#39;, &#39;auc_macro&#39;]:
        scores_all.iloc[con]=umetric(y_model[&#39;y_true&#39;], y_model[&#39;y_pred&#39;], average=umetric.split(&#34;_&#34;)[1])
        scores.loc[:,score_name]=y_model.groupby(&#39;CV_Iteration&#39;).apply(lambda x:\
                                                                        pd.Series({score_name: umetric(x[&#39;y_true&#39;], x[&#39;y_pred&#39;], 
                                                                                                      average=umetric.split(&#34;_&#34;)[1])
                                                                                  })
                                                                      )
      else:
        scores_all.iloc[con]=umetric(y_model[&#39;y_true&#39;], y_model[&#39;y_pred&#39;])
        scores.loc[:,score_name]=y_model.groupby(&#39;CV_Iteration&#39;).apply(lambda x:\
                                                                        pd.Series({score_name: umetric(x[&#39;y_true&#39;], x[&#39;y_pred&#39;])})
                                                                      )
        
      # if umetric in [&#39;auc_weighted&#39;, &#39;auc_micro&#39;, &#39;auc_macro&#39;]:
      #   ufun=lambda x:umetric(x[&#39;y_true&#39;] , x[&#39;y_pred&#39;], umetric.split(&#34;)&#34;))
      # else:
      #   ufun=lambda x:umetric(x[&#39;y_true&#39;] , x[&#39;y_pred&#39;])

      # scores_all.iloc[con]=umetric(y_model[&#39;y_true&#39;],y_model[&#39;y_pred&#39;])
      # scores.loc[:,score_name]=y_model.groupby(&#39;CV_Iteration&#39;).apply(ufun)
    except Exception as e:
      print (f&#34;{score_name} wasn&#39;t added to scores data frame:\n &#34; , str(e))
      
  scores=pd.concat([scores,
                   pd.DataFrame(scores.mean(axis=0)).T.rename({0:&#39;CV_scores_Mean&#39;},axis=0),
                   pd.DataFrame(scores.std(axis=0)).T.rename({0:&#39;CV_scores_STD&#39;},axis=0),
                   pd.DataFrame(scores_all).T
                   ],
                   axis=0,
                  )
  scores=scores.reset_index().rename({&#39;index&#39;:&#39;CV&#39;}, axis=1)
  return scores

def ml_scores_crossvalidate(**kwargs):
    from sklearn.model_selection import cross_validate
    ##NOTE: you can&#39;t use cross_validate for early stopping
    ####scoring for cross_validate
    # scoring=[
    #         &#39;accuracy&#39;,
    #         &#39;roc_auc&#39;,
    #         &#39;recall&#39; ,
    #         &#39;f1&#39;,
    #         &#39;kappa&#39;,
    #         &#39;mcc&#39;,
    #         &#39;average_precision&#39;,
    #         &#39;balanced_accuracy&#39;,
    #         &#39;precision&#39;,
    #         ]
    # scoring_dict=dict(zip(scoring, scoring))
    # ## https://scikit-learn.org/stable/modules/model_evaluation.html#scoring
    # scoring_dict[&#39;mcc&#39;]=make_scorer(matthews_corrcoef)
    # scoring_dict[&#39;kappa&#39;]=make_scorer(cohen_kappa_score)

    # scoring_dict=dict(zip(scoring,[eval(&#34;make_scorer(&#34;+metric_dict.get(i)+&#34;)&#34;) for i in scoring])) 
    
    cv_results = cross_validate(**kwargs)
    cv_results=pd.DataFrame(cv_results)
    keys_to_remove = [&#39;fit_time&#39;, &#39;score_time&#39;]
    for key in keys_to_remove:
      del cv_results[key]
    cv_results=cv_results.append(cv_results.mean(axis=0).rename(&#34;CV_scores_Mean&#34;))
    cv_results=cv_results.append(cv_results.std(axis=0).rename(&#34;CV_scores_STD&#34;))
    # print(cv_results)
    return cv_results  

def ml_prediction_sub_epochs(model):
  results = model.evals_result()
  df_epochs=pd.DataFrame()
  for metric_key in results[&#39;validation_0&#39;].keys():
      val0=results[&#39;validation_0&#39;][metric_key]
      val1=results[&#39;validation_1&#39;][metric_key]
      tmp=pd.DataFrame([val0,val1],index=[f&#39;Train_{metric_key}&#39;,f&#39;Validation_{metric_key}&#39;]).T
      df_epochs=pd.concat([df_epochs, tmp], axis=1)
      
  df_epochs.index.name=&#39;epochs&#39; 
  df_epochs=df_epochs.reset_index()
  df_epochs[&#39;best_ntree&#39;]=model.best_iteration

  return df_epochs

def ml_prediction(ml_model, 
                    X,
                    y,
                    sk_fold,  ##[X_val, y_val]
                    X_test=None,
                    y_test=None,
                    callbacks=None,
                    verbose=False,
                    ):
  from sklearn.pipeline import Pipeline
  from sklearn.model_selection import StratifiedKFold
  from sklearn.model_selection import TimeSeriesSplit
  from sklearn.base import is_classifier
    
  y_model=pd.DataFrame([])   
  df_epochs=pd.DataFrame([])  
  ml_models=[]

  umodel= ml_model[-1] if isinstance(ml_model, Pipeline) else ml_model
  model_name= umodel.__class__.__name__ 
  
  ##TODO:include all models with early_stopping
  
  early_stopping_rounds= ml_model.early_stopping_rounds if &#39;xgb&#39; in model_name.lower() else None
  if (X_test is not None) &amp; ((isinstance(sk_fold, StratifiedKFold))|(isinstance(sk_fold, TimeSeriesSplit))) &amp; (early_stopping_rounds is  None):
    print(&#34;Warning! Awkward senario. cross validaiton is on and predicition on a seprate test data set!&#34;)

  if sk_fold is None:
    print(&#34;Warning! training and validation data sets are the same&#34;)
    cv=zip([range(X.shape[0])], [range(X.shape[0])])

  elif isinstance(sk_fold, list):
    print(&#34;no cross validation &#34;)
    X_val=sk_fold[0]
    y_val=sk_fold[1]
    train_no=X.shape[0]
    X=pd.concat([X, X_val], axis=0)
    y=pd.concat([y, y_val], axis=0)
    cv=zip([range(train_no)],[range(train_no, X.shape[0])])

  else:
    cv= sk_fold.split(X,y) 

  for cv_itr,(train_index, val_index) in enumerate(cv):
    if verbose:
      print(f&#34;CV Itreation {cv_itr+1}&#34;) #{len(cv)}
    X_train, X_val = X.iloc[train_index,:], X.iloc[val_index,:] 
    y_train, y_val = y.iloc[train_index], y.iloc[val_index]

    ##TODO: it is only for xgboost, cover other ml_models
    if early_stopping_rounds is not None:
      ###NOTE: use [(X_train, y_train), (X_val, y_val)] instead of [(X_val, y_val)] to save epochs. [(X_val, y_val)] is much faster.
      eval_set = [(X_train, y_train), (X_val, y_val)]

      ml_model.fit(X_train,
                  y_train,
                  eval_set              = eval_set,
                  callbacks             = callbacks,
                  verbose               = 10
                  )
      df_epochs_tmp           = ml_prediction_sub_epochs(ml_model)
      df_epochs_tmp[&#39;CV_Iteration&#39;] = cv_itr
      df_epochs               = pd.concat([df_epochs,df_epochs_tmp],axis = 0)

      print(&#34;best_ntree=&#34;, ml_model.best_iteration, &#34;, best_score=&#34;, ml_model.best_score)

    else:
      ml_model.fit(X_train,
                  y_train,
                  )

    ml_models.append(ml_model)

    if X_test is not None: X_val=X_test
    if y_test is not None: y_val=y_test
    
    if is_classifier(umodel):
      y_model0 = pd.DataFrame(ml_model.predict_proba(X_val), index=y_val.index)
      y_model0=pd.concat([y_model0, y_model0.idxmax(axis=1).rename(&#39;y_pred&#39;), y_val.rename(&#39;y_true&#39;)], axis=1)
    else:
      y_model0 = pd.DataFrame(ml_model.predict(X_val), index=y_val.index)
      y_model0=pd.concat([y_model0,  y_val], axis=1)
      y_model0.columns=[&#39;y_pred&#39;,&#39;y_true&#39;]
      
    # print(y_model0)
    y_model0[&#39;CV_Iteration&#39;]= cv_itr
    y_model=pd.concat([y_model, y_model0],axis=0)
    
  # print(&#39;                    &#39;)

  if early_stopping_rounds is not None:
    df_epochs[&#39;best_ntree&#39;]=df_epochs[&#39;best_ntree&#39;]==df_epochs[&#39;epochs&#39;]
  else:
    df_epochs=None

  return y_model, ml_models, df_epochs

def ml_comparison(ml_models,
                  X,
                  y,
                  scores_names,
                  sk_fold,
                  mapNames={},
                  plot=True,
                  verbose=True
                  ):
  import warnings
  from sklearn.pipeline import Pipeline
  import datetime as dt
  with warnings.catch_warnings():
    if verbose:
      warnings.simplefilter(&#34;default&#34;)
    else:
      warnings.simplefilter(&#34;ignore&#34;)  
    metrics_all=pd.DataFrame()
    for con, model in enumerate(ml_models):
      if con in mapNames.keys():
        model_name=mapNames.get(con)    
      elif isinstance(model, Pipeline):
        model_name=&#39;--&gt;&#39;.join([i.__class__.__name__ for i in model])
      else:
        model_name=model.__class__.__name__
      
      start_time=dt.datetime.now()
      print(model_name+&#39;...&#39;)

      y_model,  _ , _ = ml_prediction(model,
                                        X,
                                        y,
                                        sk_fold,
                                        )

      cv_results= ml_scores(y_model, scores_names)

      tmp=cv_results
      tmp.insert(0, &#34;model&#34;, model_name)
      end_time = dt.datetime.now()
      run_time=end_time-start_time
      tmp[&#39;elapsed_time&#39;]= run_time

      #tmp.index=pd.MultiIndex.from_product([[model_name],tmp.index])
      metrics_all=pd.concat([metrics_all, tmp] ,axis=0)

      # print(tmp)
      # print(&#39;run_time:&#39;, run_time)
      if verbose:
        txt=metrics_all.loc[metrics_all[&#39;CV&#39;].isin([&#34;CV_scores_Mean&#34;,
                                                  &#34;CV_scores_STD&#34;,
                                                  # &#34;scores_all&#34;
                                                  ]), :]
                                                                  # .sort_values(by=[&#39;CV&#39;,&#39;recall&#39;],
                                                                    # ascending=[True, False]
                                                                    # ).set_index([&#39;CV&#39;,&#39;model&#39;])
        print(&#39;models summary:\n&#39;,txt)
        print(&#34;-------------------------------------------&#34;)
    #metrics_all=metrics_all.reset_index().rename({&#39;level_0&#39;:&#39;model&#39;,&#39;level_1&#39;:&#39;CV&#39;},axis=1)
    #    idx = pd.IndexSlice
    #    metrics_all_summary=metrics_all.loc[idx[:, [&#34;CV_scores_Mean&#34;, &#34;CV_scores_STD&#34;]], :]
    if plot:
      ml_comparison_plot(# The above code is not doing anything. It is just a comment.
      metrics_all, outputFile=None)
    return metrics_all

def classifer_performance_batch(y_model,
                        map_lbls={0:&#39;Low Loss&#39;, 1:&#39;High Loss&#39;},
                        scores_names=[
                                      &#39;accuracy&#39;,
                                      # &#39;balanced_accuracy&#39;,
                                      &#39;recall&#39; ,
                                      &#39;precision&#39;,
                                      # &#39;roc_auc&#39;,  
                                      # &#39;aucpr&#39;,
                                      ]
                        ):
  confMats=plot_confusion_matrix2(y_model, map_lbls, outputFile=None)

  model_prob=y_model[1] #y_model[map_lbls.get(1)]
  pos_label=1 #map_lbls.get(1)  

  df_rp, thresholds= precision_recall_curve2(y_model[&#39;y_true&#39;],
                                                      model_prob,
                                                      pos_label,
                                                      outputFile=None
                                                      )

  df_auc, thresholds= roc_curve2(y_model[&#39;y_true&#39;],
                                          model_prob,
                                          pos_label,
                                          outputFile=None
                                        )

  out, df_gain_chart, df_lift_chart = gainNlift(y_model[&#39;y_true&#39;],
                                                model_prob,
                                                pos_label,
                                                outputFile=None,
                                                groupNo = 25
                                                )
  
  scores= ml_scores(y_model, scores_names)
  return scores, confMats

def ml_prediction_xValNest(ml_model,
                              X,
                              y,
                              outter_fold,
                              inner_fold,
                              ):
  from xgboost import XGBClassifier
  y_model=pd.DataFrame([])   
  df_epochs=pd.DataFrame([])   

  for cv_outter, (trainVal_index, tst_index) in enumerate(outter_fold.split(X, y)):
    X_trainVal, X_tst = X.iloc[trainVal_index,:], X.iloc[tst_index,:] 
    y_trainVal, y_tst = y.iloc[trainVal_index],   y.iloc[tst_index]

    for cv_itr, (train_index, val_index) in enumerate(inner_fold.split(X_trainVal, y_trainVal)):
      #print(&#34;Itreation &#34;,cv_itr)

      X_train, X_val = X_trainVal.iloc[train_index,:], X_trainVal.iloc[val_index,:] 
      y_train, y_val = y_trainVal.iloc[train_index],   y_trainVal.iloc[val_index]

      ##TODO: it is only for xgboost, cover other ml_models
      eval_set = [(X_train, y_train), (X_val, y_val)]

      ml_model.fit(X_train,
                  y_train,
                  eval_set=eval_set,
                  verbose=200
                  )
      df_epochs_tmp=ml_prediction_sub_epochs(ml_model)
      df_epochs_tmp[&#39;CV_Iteration&#39;]=f&#39;{cv_outter}_{cv_itr}&#39;
      df_epochs=pd.concat([df_epochs,df_epochs_tmp],axis=0)

      print(&#34;best_ntree=&#34;, ml_model.best_iteration)
      print(&#34;best_score=&#34;, ml_model.best_score)

      ml_model2=XGBClassifier(n_estimators=ml_model.best_iteration)
      
      ml_model2.fit(X_trainVal,
                  y_trainVal,
                  )
            
      y_model0 = pd.DataFrame(ml_model2.predict_proba(X_tst),index=X_tst.index)
    #   y_model0.rename(columns=map_lbls,inplace=True)
      y_model0 = pd.concat([y_model0,y_model0.idxmax(axis=1).rename(&#39;y_pred&#39;)],axis=1)
    
      y_model0[&#39;CV_Iteration&#39;]=f&#39;{cv_outter}_{cv_itr}&#39;
      y_model0[&#39;y_true&#39;]=y_tst
      
      y_model = pd.concat([y_model,y_model0],axis=0)

  #print(&#34;--------------------------------------------------------&#34;)

  df_epochs[&#39;best_ntree&#39;]=df_epochs[&#39;best_ntree&#39;]==df_epochs[&#39;epochs&#39;]
    
  return y_model, df_epochs

####------------------------------plot model performance----------------------------------------------------------------------------------------
##-----------------------------------------------------------------------------------------------------------------------------------------------
##-----------------------------------------------------------------------------------------------------------------------------------------------
##-----------------------------------------------------------------------------------------------------------------------------------------------
import matplotlib.pyplot as plt
from matplotlib.ticker import MultipleLocator
#import pydotplus
import pylab as pl
import seaborn as sns
sns.set_style(&#34;darkgrid&#34;)
sns.set(rc = {&#39;figure.figsize&#39;:(30,20)})

def ml_comparison_plot(metrics_all, outputFile=None):
    ##---plot comparison box plot
    df_tmp=metrics_all.loc[~metrics_all[&#39;CV&#39;].isin([&#34;CV_scores_Mean&#34;,
                                                    &#34;CV_scores_STD&#34;,
                                                    &#34;scores_all&#34;]
                                                  ),
                           :]

    if &#39;model&#39; in df_tmp.columns.tolist():
      df_tmp=df_tmp.drop(&#39;CV&#39;,axis=1)
      hue=id_vars=&#39;model&#39;
    else:
      id_vars=&#39;CV&#39;
      hue=&#39;scores&#39;
    ucols=[col for col in df_tmp if col not in [&#39;elapsed_time&#39;,&#39;Feature_nos&#39;]]
    df_long = pd.melt(df_tmp[ucols], id_vars=[id_vars], var_name=[&#39;scores&#39;])  
    # sns.set_style(&#34;darkgrid&#34;)
    plt.figure(figsize = (25,15))
    
    uplot   = sns.boxplot(x=&#34;scores&#34;,
                          y=&#34;value&#34;,
                          hue=hue,
                          data=df_long,
                          # orient=&#39;h&#39;,  ##it takes forever
                          showfliers=False,
                          ) 
    
    uplot.set_xticklabels(uplot.get_xticklabels(),rotation=90)
    uplot.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)
    uplot.grid()

    if outputFile is not None:
      # graphfile=os.path.join(outputFile,&#39;compare_models.png&#39;)
      print(&#34;plot save in %s&#34; %outputFile)
      plt.savefig(outputFile)
      plt.show()
      plt.close() 
 
def learning_curve_early_stopping(df_epochs, outputFile=None):
  ###https://machinelearningmastery.com/avoid-overfitting-by-early-stopping-with-xgboost-in-python/

  ##TODO: it is only for xgb now (best_ntree)
  cols=df_epochs.columns[~df_epochs.columns.str.contains(&#39;Validation_|Train_&#39;)].tolist()
  df_epochs_melted=df_epochs.melt(id_vars=cols)
  uPlot=sns.relplot(
                    data=df_epochs_melted,
                    y=&#34;value&#34;,
                    x=&#34;epochs&#34;,
                    col=&#34;CV_Iteration&#34;,
                    hue=&#34;variable&#34;,
                    style=&#34;variable&#34;,
                    kind=&#34;line&#34;,
        #             markers=True,
                    palette=[&#39;green&#39;, &#39;black&#39;],
                    col_wrap=3
                )

  axes = uPlot.axes.flatten()

  sns.set(rc = {&#39;figure.figsize&#39;:(60,30)})
  for con, ax in enumerate(axes):
      data_tmp=df_epochs_melted[df_epochs_melted[&#39;CV_Iteration&#39;]==con]
      xc=data_tmp.loc[data_tmp[&#39;best_ntree&#39;],&#39;epochs&#39;]
      ax.axvline(xc.iloc[0], ls=&#39;-&#39;, linewidth=3, color=&#39;red&#39;, alpha=0.75)
    
  # fig, ax = plt.subplots(math.ceil((df_epochs_melted[&#39;CV_Iteration&#39;].nunique())/3), 3, figsize=(30, 20))
  # for subplot, var in enumerate(df_epochs_melted[&#39;CV_Iteration&#39;].unique()):
  #   axs=ax.flatten()[subplot]
  #   data_tmp=df_epochs_melted[df_epochs_melted[&#39;CV_Iteration&#39;]==var]
  #   sns.lineplot(
  #                ax=axs,
  #                data=data_tmp,
  #                 y=&#34;value&#34;,
  #                 x=&#34;epochs&#34;,
  #                 hue=&#34;variable&#34;,
  #                 style=&#34;variable&#34;,
  #               )
  #   xc=data_tmp.loc[data_tmp[&#39;best_ntree&#39;],&#39;epochs&#39;]
  #   axs.title.set_text(f&#39;cv_itr: {var}&#39;)
  #   axs.axvline(x=xc.iloc[0],
  #               color=&#39;red&#39;,
  #               linestyle=&#39;--&#39;)
  
  if outputFile is not None:
    figure = uPlot.get_figure()
    # ,&#34;learning_curve.png&#34;)
    figure.savefig(outputFile, bbox_inches=&#39;tight&#39;)
    plt.close(&#39;all&#39;)

def gainNlift(y, model_prob, pos_label, outputFile, groupNo=25):
    ## Lift/cumulative gains charts aren&#39;t a good way to evaluate a model (as it cannot be used for comparison between ml_models), and are instead a means of evaluating the results where your resources are finite. Either because there&#39;s a cost to action each result (in a marketing scenario) or you want to ignore a certain number of guaranteed voters, and only action those that are on the fence. Where your model is very good, and has high classification accuracy for all results, you won&#39;t get much lift from ordering your results by confidence.(https://stackoverflow.com/questions/42699243/how-to-build-a-lift-chart-a-k-a-gains-chart-in-python)

    ## gain Interpretation:
    ## % of targets (events) covered at a given decile level. For example,  80% of targets covered in top 20% of data based in model. In the case of propensity to buy model, we can say we can identify and target 80% of customers who are likely to buy the product by just sending email to 20% of total customers.
    ## lift Interpretation:
    ## The Cum Lift of 4.03 for top two deciles, means that when selecting 20% of the records based on the model, one can expect 4.03 times the total number of targets (events) found by randomly selecting 20%-of-file without a model.

    ##note: when intersted column has much more freq than the other:
    #  df[df.columns[0].value_counts()
            # bluecurvetv        127289
            # Decline_offer    2853
    ##the lift and gain chart of intersted column doesnot show any supererioty of using ML,(model and random output shows same output)

    df=pd.concat([y,model_prob],axis=1)
    df.sort_values(by=df.columns[1], ascending=False, inplace=True)

    def gain_stp1(subset):
        pos_event=sum(subset[y.name]==pos_label)
        return  len(subset), pos_event

    # subset=np.array_split(df,groupNo)[0]
    tmp=list(map(gain_stp1,np.array_split(df,groupNo)))
    out = pd.DataFrame(tmp,columns=[&#39;case&#39;,&#39;event&#39;])
    out[&#39;event%&#39;]=out[&#39;event&#39;]/out[&#39;event&#39;].sum()*100
    out[&#39;cum_case&#39;] = out[&#39;case&#39;].cumsum()
    out[&#39;cum_case%&#39;] = out[&#39;cum_case&#39;]/out[&#39;case&#39;].sum()*100
    out[&#39;gain&#39;] = out[&#39;event%&#39;].cumsum()
    out[&#39;cum_lift&#39;] = out[&#39;gain&#39;]/out[&#39;cum_case%&#39;]

    row_no=int(out.shape[0])

    df_gain_chart=pd.DataFrame(out[&#39;gain&#39;].tolist()+out[&#39;cum_case%&#39;].tolist(),columns=[&#39;values&#39;])
    df_gain_chart[&#39;x&#39;]=pd.Series(out[&#39;cum_case%&#39;].tolist()*2)
    df_gain_chart[&#39;selection method&#39;]=pd.Series([&#39;model&#39;]*row_no+[&#39;random&#39;]*row_no)
    df_gain_chart=df_gain_chart.append(pd.DataFrame.from_dict({&#39;values&#39;:[0,0],&#34;x&#34;:[0,0],&#39;selection method&#39;:[&#39;model&#39;,&#39;random&#39;]}), ignore_index=True)

    df_lift_chart=pd.DataFrame(out[&#39;cum_lift&#39;].tolist()+[1]*row_no,columns=[&#39;values&#39;])
    df_lift_chart[&#39;x&#39;]=pd.Series(out[&#39;cum_case%&#39;].tolist()*row_no)
    df_lift_chart[&#39;selection method&#39;]=pd.Series([&#39;model&#39;]*row_no+[&#39;random&#39;]*row_no)
        
    fig, ax = plt.subplots(2,1,figsize=(20, 10))   
    
    uPlot1=sns.lineplot(data=df_gain_chart,
                        ax=ax[0],
                        x=&#39;x&#39;,
                        y=&#39;values&#39;,
                        hue=&#39;selection method&#39;,
                        style=&#39;selection method&#39;,
                        markers=True
                       )
    uPlot1.set(xlabel=&#39;&#39;, ylabel=&#39;% of events&#39;)
    ax[0].set_title(&#39;Gain Chart&#39;)

    uPlot2=sns.lineplot(data=df_lift_chart,
                        ax=ax[1],
                        x=&#39;x&#39;,
                        y=&#39;values&#39;,
                        hue=&#39;selection method&#39;,
                        style=&#39;selection method&#39;,
                        markers=True
                       )
    uPlot2.set(xlabel=&#39;% 0f data sets&#39;, ylabel=&#39;Lift&#39;)
    ax[1].set_title(&#39;Lift Chart&#39;)
    
    plt.ylim(0,int(df_lift_chart[&#39;values&#39;].max()+1))
    plt.xlim(0,100) 
    
    if outputFile is not None:
      uPlot1.get_figure().savefig(outputFile[0], bbox_inches=&#39;tight&#39;)
      uPlot2.get_figure().savefig(outputFile[1], bbox_inches=&#39;tight&#39;)
      plt.close(&#39;all&#39;)

    return out, df_gain_chart, df_lift_chart
  
def precision_recall_curve2(y, model_prob, pos_label, outputFile=None, **kwargs):   
    from sklearn.metrics import precision_recall_curve, auc
    
    model_precision, model_recall, thresholds = precision_recall_curve(y_true=y, probas_pred=model_prob, pos_label=pos_label, **kwargs)
    model_auc_rp = auc(model_recall, model_precision)

    tmpTxt=&#39;ROC of precision recall curve=%.3f&#39; % (model_auc_rp)
    ### plot the precision-recall curves
    fig, ax = plt.subplots(figsize=(20, 10))

    df_rp=pd.DataFrame([model_precision[:-1],
                      model_recall[:-1],
                      thresholds], index=[&#39;Precision&#39;, &#39;Recall&#39;, &#39;thresholds&#39;]).T
    df_rp[&#39;style&#39;]=1
    # df_rp2=pd.melt(df_rp, id_vars=&#39;recall&#39;, value_vars=[&#39;Precision&#39;, &#39;thresholds&#39;],  var_name=&#39;precision_thresholds&#39;)
    # print(df_rp2)

    uPlot2=sns.lineplot(data=df_rp,
                        ax=ax,
                        y=&#39;Precision&#39;,
                        x=&#39;Recall&#39;,
                        # hue=&#39;precision_thresholds&#39;,
                        markers=True,
                        style=&#34;style&#34;,
                        # palette=[&#34;red&#34;],
                        alpha=0.1,
                        )
    plt.legend([],[], frameon=False)
    ax.set_title(&#39;Precision Recall Curve&#39;)
    no_skill = len(y[y==1]) / len(y)
    # ax.set_ylim(0, 1.1)

    df_rp_tmp=df_rp.drop_duplicates(subset=[&#39;Recall&#39;]).reset_index()
    interval_no=min(15,df_rp_tmp.shape[0])
    idx=list(np.linspace(df_rp_tmp.index.min(),df_rp_tmp.index.max(),interval_no,endpoint=True,dtype=&#39;int&#39;))
    plt.xticks(df_rp_tmp.iloc[idx][&#39;Recall&#39;])
    ticks_loc = ax.get_xticks().tolist()
    threshs=(df_rp_tmp.iloc[idx][&#39;thresholds&#39;].round(3).astype(str))  ### or:df_rp.loc[df_rp[&#39;Recall&#39;].isin(ticks_loc),&#39;thresholds&#39;]

    ax.set_xticks(ax.get_xticks().tolist())
    ax.set_xticklabels([str(round(x,2))+&#34;(t=&#34;+y+&#34;)&#34; for x,y in zip(ticks_loc,threshs)])
    ax.set(xlabel=&#39;Recall/(threshold)&#39;)
    plt.plot([0, 1], [no_skill, no_skill], linestyle=&#39;--&#39;, label=&#39;No Skill&#39;, color=&#39;black&#39;)
    # plt.plot([0, 0], [no_skill, no_skill], linestyle=&#39;--&#39;, label=&#39;No Skill&#39;, color=&#39;red&#39;,alpha=1)
    ax.text(.9, no_skill, f&#39;No skill line&#39;, color=&#39;black&#39;, fontsize=10)

    plt.xticks(rotation=90)
    ax.annotate(&#39;ROC of Precision Recall curve=%.3f&#39; % (model_auc_rp),
                xy=(.4, 0), xycoords=&#39;axes fraction&#39;,
                xytext=(-20, 25),
                textcoords=&#39;offset pixels&#39;,
                horizontalalignment=&#39;right&#39;,
                verticalalignment=&#39;bottom&#39;,
                fontsize=10)

    # labels = ax.get_xticklabels()
    # print(labels)
    # labels=[x.get_text()+&#34;(&#34;+y+&#34;)&#34; for x,y in zip(labels,list(df_rp[&#39;x_label&#39;]))]
    # ax.set_xticklabels(labels)

    # ax2 = ax.twinx()
    # ax2.set(ylim=(df_rp[&#39;thresholds&#39;].min(),
    #               df_rp[&#39;thresholds&#39;].max()))

    # ax3 = ax.twiny()
    # ax3.set(ylim=(df_rp[&#39;thresholds&#39;].min(),
    #               df_rp[&#39;thresholds&#39;].max()))

    # uPlot2=sns.lineplot(data=df_rp,
    #                     ax=ax2,
    #                     y=&#39;thresholds&#39;,
    #                     x=&#39;precision&#39;,
    #                     markers=True
    #                    )

    if outputFile is not None:
      plt.savefig(outputFile, bbox_inches=&#39;tight&#39; ,dpi=300)
      plt.show()
      # plt.close(&#39;all&#39;)

    return df_rp.drop([&#39;style&#39;],axis=1), idx 

def roc_curve2(y, model_prob, pos_label, outputFile, **kwargs):
    ##TODO: add **kwargs to roc_curve, after sepration augs
    from sklearn.metrics import roc_auc_score

    model_auc = roc_auc_score(y_true=y, y_score=model_prob, **kwargs)
    ## NOTE:    Different result with roc_auc_score() and auc()
    # : https://stackoverflow.com/questions/31159157/different-result-with-roc-auc-score-and-auc
    # from sklearn.metrics import roc_curve,auc
    # model_fpr, model_tpr, _ = roc_curve(y.map(map_lbls_inv), model_prob)
    # model_auc2 = auc(model_fpr, model_tpr)

    tmpTxt=&#39;ROC AUC=%.3f (random selection=.5)&#39; % (model_auc)
#     print(tmpTxt+&#39;\n&#39;,&#39;green&#39;)
    from sklearn.metrics import roc_curve
    model_fpr, model_tpr, thresholds = roc_curve(y_true=y, y_score=model_prob, pos_label=pos_label)

    thresholds[0]=1
    df_roc=pd.DataFrame([model_fpr,
                      model_tpr,
                      thresholds], index=[&#39;False_Positive_Rate&#39;, &#39;True_Positive_Rate&#39;, &#39;thresholds&#39;]).T

    fig, ax = plt.subplots(figsize=(20, 10))
    plt.plot([0,1],[0,1], linestyle=&#39;--&#39;, label=&#39;No Skill&#39;)    ###or ns_fpr, ns_tpr =[0,1],[0,1]= roc_curve(y, [0 for _ in range(len(y))], pos_label)
    plt.plot(model_fpr, model_tpr, marker=&#39;.&#39;, label=&#39;Model&#39;)
    plt.xlabel(&#39;False Positive Rate&#39;)
    plt.ylabel(&#39;True Positive Rate&#39;)
    plt.title(&#39;ROC curve&#39;)
    ax.legend(loc=&#39;upper right&#39;, frameon=True)
    ax.annotate(&#39;ROC AUC=%.3f (random selection=.5)&#39; % (model_auc),
                xy=(1, 0), xycoords=&#39;axes fraction&#39;,
                xytext=(-20, 20),
                textcoords=&#39;offset pixels&#39;,
                horizontalalignment=&#39;right&#39;,
                verticalalignment=&#39;bottom&#39;)

    df_roc_tmp=df_roc.drop_duplicates(subset=[&#39;False_Positive_Rate&#39;]).reset_index()
    interval_no=min(15,df_roc_tmp.shape[0])
    idx=list(np.linspace(df_roc_tmp.index.min(),df_roc_tmp.index.max(),interval_no,endpoint=True,dtype=&#39;int&#39;))
    plt.xticks(df_roc_tmp.iloc[idx][&#39;False_Positive_Rate&#39;])
    ticks_loc = ax.get_xticks().tolist()
    threshs=(df_roc_tmp.iloc[idx][&#39;thresholds&#39;].round(3).astype(str)) 

    ax.set_xticks(ax.get_xticks().tolist())
    ax.set_xticklabels([str(round(x,2))+&#34;(t=&#34;+y+&#34;)&#34; for x,y in zip(ticks_loc,threshs)])
    plt.xticks(rotation=90)

    if outputFile is not None:
      plt.savefig(outputFile, bbox_inches=&#39;tight&#39;)
      plt.close(&#39;all&#39;)

    return df_roc, model_auc
  
def reliability_diagram(y, model_prob, pos_label, outputFile, **kwargs):
    from sklearn.calibration import calibration_curve
    prob_true, prob_pred= calibration_curve(y_true=y, y_prob=model_prob, n_bins=50, normalize=False, **kwargs)  # pos_label=pos_label, 
    prob_true_norm, prob_pred_norm= calibration_curve(y_true=y, y_prob=model_prob,  n_bins=50,normalize=True, **kwargs)  # pos_label=pos_label, 

    fig, ax = plt.subplots(figsize=(20, 10))
    plt.plot([0,1],[0,1]) 
    plt.plot(prob_pred_norm, prob_true_norm, label=&#39;Normlized&#39;) 
    plt.plot(prob_pred, prob_true, label=&#39;Original&#39;) 
    plt.grid() 
    plt.xlabel(&#34;Average probability&#34;)
    plt.ylabel(&#34;Fraction of positive&#34;)
    plt.title(&#34;Reliability diagram&#34;) 
    ax.legend(loc=&#39;upper right&#39;, frameon=True)
    
    if outputFile is not None:
      plt.savefig(outputFile, bbox_inches=&#39;tight&#39;)
      plt.close(&#39;all&#39;)
      
    return prob_true, prob_pred, prob_true_norm, prob_pred_norm

def plot_confusion_matrix2(y_model, map_lbls, outputFile=None):
  ##y_model=pd.concat([y_true, y_pred],axis=1)
  
  from sklearn.metrics import confusion_matrix
  y_model1=y_model.copy()
  
  if (&#39;CV_Iteration&#39; in y_model1.columns)&amp;(y_model1[&#39;CV_Iteration&#39;].nunique()!=1) :
    #if (y_model1[&#39;CV_Iteration&#39;].nunique()!=1):
    y_model1[&#39;CV_Iteration&#39;]=&#39;cv_&#39;+y_model1[&#39;CV_Iteration&#39;].astype(str)
    y_model_all=y_model.copy()
    y_model_all[&#39;CV_Iteration&#39;]=&#39;All_data&#39;
    y_model1=pd.concat([y_model1,y_model_all],axis=0)
    ncol=3
    fig_size=(30,20)
  else:
    y_model1[&#39;CV_Iteration&#39;]=&#39;All_data&#39;
    ncol=1
    fig_size=(10,5)

  print(y_model1)
  confMats=pd.Series([])
  #confMats=pd.Series([],index=y_model[&#39;CV_Iteration&#39;].unique())    
  
  fig, axs = plt.subplots(math.ceil(y_model1[&#39;CV_Iteration&#39;].nunique()/ncol), ncol, figsize=fig_size)   
  axs=np.array([axs]) if ncol==1 else axs

  for cont, (cv, y_model_sub) in  enumerate(y_model1.groupby([&#39;CV_Iteration&#39;])):  
    print(cont,cv)
    y_true=y_model_sub[&#39;y_true&#39;]
    y_pred=y_model_sub[&#39;y_pred&#39;]

    confMat = pd.DataFrame(confusion_matrix(y_true, y_pred))
    confMat=confMat.rename(columns=map_lbls).rename(map_lbls,axis=1).rename(map_lbls,axis=0)
    confMat.index.name=&#39;True label&#39;
    confMat.columns.name=&#39;Predicted label&#39;

    confMats[cv]=confMat
    uPlot=sns.heatmap(ax=axs.flatten()[cont],
                    data=confMat,
                    annot=True,
                    cmap=&#34;YlGnBu&#34;,
                    fmt=&#34;g&#34;
                   )

    axs.flatten()[cont].set_title(f&#39;{cv}&#39;)    
    
  if outputFile is not None:
    figure = uPlot.get_figure()
    figure.savefig(outputFile, bbox_inches=&#39;tight&#39;)
    plt.close(&#39;all&#39;)
    
  return confMats

def feature_importance_batch(umodel, X, y):
    from sklearn.pipeline import Pipeline

    umodel1 = umodel[-1] if isinstance(umodel, Pipeline) else umodel
    model_name = umodel1.__class__.__name__

    ##TODO: generalize for any tree model:
    if &#34;xgb&#34; in model_name.lower():
        from xgboost import plot_importance

        umodel.fit(X, y)
        plt.figure(
            figsize=(100, 100),
            dpi=150,
        )
        sns.set(rc={&#34;figure.figsize&#34;: (20, 10)}, font_scale=1)
        plot_importance(umodel, max_num_features=30)
        plt.show()
        plt.close()

        feature_importance = pd.Series(
            umodel.feature_importances_, index=X.columns
        ).sort_values(ascending=False)
        print(
            &#34;non zero features:&#34;,
            feature_importance[feature_importance &gt;= 0.01].index.tolist(),
        )
        print(
            &#34;zero features:&#34;,
            feature_importance[feature_importance &lt; 0.01].index.tolist(),
        )

        plt.figure(figsize=(5, 2), dpi=150)

        importance_hist = feature_importance.hist()

        sns.set(rc={&#34;figure.figsize&#34;: (20, 10)}, font_scale=1.5)
        importance_hist

        sel_features = feature_importance[feature_importance &gt; 0.01].index.tolist()
    else:
        print(&#34;The model is not tree based- returning None&#34;)
        feature_importance = None
        sel_features = X.columns

    return feature_importance, sel_features

def pdp_plot_batch(X, umodel, sel_features):
    # print(&#39;The scikit-learn version is {}.&#39;.format(sklearn.__version__))
    from sklearn.inspection import PartialDependenceDisplay

    ###see https://scikit-learn.org/stable/auto_examples/inspection/plot_partial_dependence.html#sphx-glr-auto-examples-inspection-plot-partial-dependence-py
    print(
        &#34;Computing partial dependence plots and individual conditional expectation...&#34;
    )

    _, ax = plt.subplots(
        # ncols=3, nrows=math.ceil(len(sel_features)/3),
        figsize=(30, 30),
        sharey=False,
        constrained_layout=True,
    )

    features_info = {
        &#34;features&#34;: sel_features,
        &#34;kind&#34;: &#34;both&#34;,
        &#34;centered&#34;: True,
        # &#34;categorical_features&#34;: [&#39;plant_K1&#39;],
    }
    display = PartialDependenceDisplay.from_estimator(
        umodel,
        X,
        **features_info,
        ax=ax,
        # **common_params,
    )

def shap_plots_batch(X, y, umodel, random_state=100):
    import shap
    from sklearn.model_selection import RandomizedSearchCV, train_test_split

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=random_state
    )
    # X_train, X_test= X, X

    ###https://www.kaggle.com/code/hwwang98/shapley-value-feature-research#kln-19:
    # rather than use the whole training set to estimate expected values, we summarize with
    # a set of weighted kmeans, each weighted by the number of points they represent.
    X_train_summary = shap.kmeans(X_train, 10)
    ###TODO: why it is working only in xgboost:
    # explainer = shap.Explainer(umodel, X)
    # shap_values = explainer(X)
    # # shap.plots.beeswarm(shap_values)
    # shap.plots.bar(shap_values)
    # shap.summary_plot(shap_values, plot_type=&#39;violin&#39;)

    explainer = shap.KernelExplainer(
        model=umodel.predict,
        data=X_train_summary,
        # link=&#34;identity&#34;
    )
    shap_values = explainer.shap_values(
        X_test,
        #  nsamples=1000
    )

    # shap.plots.bar(shap_values)##not working
    shap.summary_plot(shap_values, X_test, plot_type=&#34;violin&#34;)
    # shap.force_plot(explainer.expected_value, shap_values, X_test) ##not working
    # shap.dependence_plot(&#34;NOH&#34;, shap_values, X_test)##not working

    return shap_values

####------------------------------Xgboost HyperParameter tuning----------------------------------------------------------------------------------------
##-----------------------------------------------------------------------------------------------------------------------------------------------
##-----------------------------------------------------------------------------------------------------------------------------------------------
##-----------------------------------------------------------------------------------------------------------------------------------------------
# When working with imbalanced data sets, accuracy is not always the best metric to evaluate the performance of a classifier, because it can be misleading. Some alternative metrics that are better suited for imbalanced data sets are:

# Precision: the proportion of true positive predictions among all positive predictions. It measures the ability of the classifier to avoid false positives.
# Recall (Sensitivity or TPR): the proportion of true positive predictions among all actual positive observations. It measures the ability of the classifier to detect all positive observations.
# F1-score: the harmonic mean of precision and recall. It is a balance between precision and recall.
# AUC-ROC: the area under the Receiver Operating Characteristic curve. It measures the ability of the classifier to distinguish between positive and negative observations.
# AUC-PR: the area under the precision-recall curve. It also measures the ability of the classifier to distinguish between positive and negative observations, but it puts more emphasis on the true positive rate.
# G-mean: the geometric mean of recall and specificity. It is a balance between recall and specificity and is sensitive to imbalanced data.
# These metrics can be calculated using the following sklearn functions:

# Precision: sklearn.metrics.precision_score(y_true, y_pred)
# Recall: sklearn.metrics.recall_score(y_true, y_pred)
# F1-score: sklearn.metrics.f1_score(y_true, y_pred)
# AUC-ROC: sklearn.metrics.roc_auc_score(y_true, y_score)
# AUC-PR: sklearn.metrics.average_precision_score(y_true, y_score) or use    precision, recall, _ = precision_recall_curve(y_test, y_pred) and AUC_PR = auc(recall, precision)
# G-mean: sklearn.metrics.geometric_mean_score(y_true, y_pred)


### NOTE: xgboost parameters: https://xgboost.readthedocs.io/en/latest/parameter.html
# https://scikit-learn.org/stable/modules/generated/sklearn.metrics.average_precision_score.html#sklearn.metrics.average_precision_score5 

### NOTE: Handle Imbalanced Dataset:
### https://xgboost.readthedocs.io/en/latest/tutorials/param_tuning.html:
### For common cases such as ads clickthrough log, the dataset is extremely imbalanced. This can affect the training of XGBoost ucase, and there are two ways to improve it:
### If you care only about the overall performance metric (AUC) of your prediction:
    ### Balance the positive and negative weights via scale_pos_weight
    ### Use AUC for evaluation
### If you care about predicting the right probability:
    ### In such a case, you cannot re-balance the dataset
    ### Set parameter max_delta_step to a finite number (say 1) to help convergence
## AUPRC and Average Precision:
###https://glassboxmedicine.com/2019/03/02/measuring-performance-auprc/
## The baseline of AUPRC is equal to the fraction of positives. If a dataset consists of 8% cancer examples and 92% healthy examples, the baseline AUPRC is 0.08, so obtaining an AUPRC of 0.40 in this scenario is good! AUPRC is most useful when you care a lot about your model handling the positive examples correctly.

###using scale_pos_weight and sample_weight  leads to almost the same auc:
# when  &#34;tree_method&#34;:&#34;gpu_hist&#34; and &#34;predictor&#34;:&#34;gpu_predictor&#34; :  (0.842775:0.842165)
# without using gpu results                                      :  (0.842609:0.842686)

# The hyperparameters that have the greatest effect on XGBoost objective metrics are: alpha, min_child_weight, subsample, eta, and num_round.

# Fill reasonable values for key inputs:
# learning_rate: 0.01
# n_estimators: 100 if the size of your data is high, 1000 is if it is medium-low
# max_depth: 3
# subsample: 0.8
# colsample_bytree: 1
# gamma: 1

###  I usually use 50 rounds for early stopping with 1000 trees in the model. I’ve seen in many places recommendation to use about 10% of total number of trees for early stopping
### early_stopping_rounds offers a way to automatically find the ideal value for n_estimators. Early stopping causes the model to stop iterating when the validation score stops improving, even if we aren&#39;t at the hard stop for n_estimators. It&#39;s smart to set a high value for n_estimators and then use early_stopping_rounds to find the optimal time to stop iterating.

##xgboost GPU parameter:
            ##NOte: Gpu is very fast, but it doesnot give good model performance like cpu, so tune your model with gpu and use cpu to generate final prediction on CPU 
#             &#34;tree_method&#34;:&#34;gpu_hist&#34;,
            ## &#34;gpu_id&#34;:1,
#             &#34;predictor&#34;:&#34;gpu_predictor&#34;,

import sklearn.metrics as metrics
from sklearn.pipeline import Pipeline
def ml_tuner(trial,
             sk_model,
             model_params,
             X,
             y,
             sk_fold,
             var_in_model_params,
             Umetric=&#39;auc&#39;,
             use_early_Stopping=False,
             early_stopping_rounds=300,
             use_callbacks=False
            ):
  import optuna


  local_vars=locals()

  ##TODO: it is not working:
  # if &#39;Pipeline&#39; in  str(type(sk_model)):
  #   model_sub = sk_model.steps[-1][1]
  #   sk_model.steps[-1][1]=model_sub(**model_params) 
  #   model = sk_model
  # else:
  #   model = sk_model(**model_params) 

  ##TODO: Revise logic:
  if model_params is not None:
    model = sk_model(**model_params) 
  ##Umetric is used when use_early_Stopping=True, otherwise it was infered from model_params(&#39;eval_metric&#39;)
    model_params={key:(eval(par, var_in_model_params, local_vars) if (isinstance(par, str) and (par[:14]==&#39;trial.suggest_&#39;)) else par) for (key, par) in model_params.items()}
    print(model_params)
  else:
    model=sk_model

  if use_early_Stopping:
    #eval_metric used by early stopping comes from xgboost package and there is not same as metrics in sklearn:
      #https://xgboost.readthedocs.io/en/stable/parameter.html
      #https://scikit-learn.org/stable/modules/classes.html 
    #for instance use recall metric doesnot exists in eval_metric. Moreover same metric have difference name:  aucpr in eval_metric is same as average_precision_score in metrics
    ##so there is need to used dictionary to convert  eval_metric to metric
    ##TODO: add all metric to it:
    eval_metric_dict={&#39;auc&#39;    : &#39;auc&#39;,
                      &#39;aucpr&#39;  : &#39;aucpr&#39;,
                      }
  
    Umetric=eval_metric_dict.get(model_params.get(&#39;eval_metric&#39;))
      
  if use_callbacks:
    # Each of ‘validation_0‘ and ‘validation_1‘ correspond to the order that datasets were provided to the eval_set argument in the call to fit(). 
    #eval_set = [(X_train, y_train), (X_val, y_val)]
    #it is much faster than [(X_train, y_train), (X_val, y_val)]
  
    #when len(eval_set)=2:
    observation_key=&#34;validation_1-&#34;+model_params[&#39;eval_metric&#39;] 
    #when len(eval_set)=1:
    # observation_key=&#34;validation_0-&#34;+model_params[&#39;eval_metric&#39;]
    
  # Add a callback for pruning.
    pruning_callback = [optuna.integration.XGBoostPruningCallback(trial, observation_key)]
  
  else:
    pruning_callback=None
    
  y_model,_=ml_prediction(model,
                              X,
                              y,
                              sk_fold,
                              use_early_Stopping=use_early_Stopping,
                              early_stopping_rounds=early_stopping_rounds,
                              pruning_callback=pruning_callback,
                              )

  scores=ml_scores(y_model, [Umetric])
  print(scores)
  scores=scores.loc[scores[&#39;CV&#39;]==&#39;CV_scores_Mean&#39;,Umetric]
  
  # scores=[]    
  # for cv_itr,(train_index, val_index) in enumerate(sk_fold.split(X,y)):
  #   model = sk_model(**model_params) 

  #   X_train, X_val = X.iloc[train_index,:], X.iloc[val_index,:] 
  #   y_train, y_val = y.iloc[train_index], y.iloc[val_index]
    
  #   ##TODO: it is only for xgboost, cover other mls
  #   if use_early_Stopping:
  #     eval_set = [(X_val, y_val)]

  #     if use_callbacks:
  #       # Each of ‘validation_0‘ and ‘validation_1‘ correspond to the order that datasets were provided to the eval_set argument in the call to fit(). 
  #       #eval_set = [(X_train, y_train), (X_val, y_val)]
  #       #### it is much faster than [(X_train, y_train), (X_val, y_val)]
        
  #       if len(eval_set)==2:
  #         observation_key=&#34;validation_1-&#34;+model_params[&#39;eval_metric&#39;]
  #       else:
  #         observation_key=&#34;validation_0-&#34;+model_params[&#39;eval_metric&#39;]

  #       # Add a callback for pruning.
  #       pruning_callback = [optuna.integration.XGBoostPruningCallback(trial, observation_key)]

  #       model.fit(X_train,
  #                 y_train,
  #                 early_stopping_rounds=early_stopping_rounds,
  #                 eval_set=eval_set,
  #                 callbacks=pruning_callback,
  #                 verbose=200
  #                 )
  #     else:
  #       model.fit(X_train,
  #                 y_train,
  #                 early_stopping_rounds=early_stopping_rounds,
  #                 eval_set=eval_set,
  #                 verbose=200
  #                 )   
      
  #     ##TODO: add all metric to it:
  #     #eval_metric used by early stopping comes from xgboost package and there is not same as metrics in sklearn:
  #       #https://xgboost.readthedocs.io/en/stable/parameter.html
  #       #https://scikit-learn.org/stable/modules/classes.html 
  #     #for instance use recall metric doesnot exists in eval_metric. Moreover same metric have difference name:  aucpr in eval_metric is same as average_precision_score in metrics
  #     ##so there is need to used dictionary to convert  eval_metric to metric

  #     eval_metric_dict={&#39;auc&#39;    : metrics.roc_auc_score,
  #                       &#39;aucpr&#39;  : metrics.average_precision_score,
  #                       &#39;logloss&#39;: metrics.accuracy_score
  #                      }

  #     Umetric_func=eval_metric_dict.get(model_params.get(&#39;eval_metric&#39;))
    
  #   else:
  #     model.fit(X_train,
  #               y_train,
  #               )
      
  #     Umetric_func=metric_dict.get(Umetric)
      
    # y_val_model = model.predict(X_val)
    # score=Umetric_func(y_val, y_val_model)  
    # scores.append(score)
    #scores=np.mean(scores)
  return scores

from sklearn.metrics import accuracy_score, roc_auc_score
from hyperopt import fmin, tpe, hp, STATUS_OK, Trials
from typing import Any, Dict, Union 

def hyperparameter_tuning(
                        space: Dict[str, Union[float, int]],
                        X: pd.DataFrame, y: pd.Series,
                        sk_fold,   #[X_test,y_test]
                        early_stopping_rounds: int=50,
                        Umetric:callable=accuracy_score)-&gt; Dict[str, Any]:

  # When working with imbalanced data sets, accuracy is not always the best metric to evaluate the performance of a classifier, because it can be misleading. Some alternative metrics that are better suited for imbalanced data sets are:

  # Precision: the proportion of true positive predictions among all positive predictions. It measures the ability of the classifier to avoid false positives.
  # Recall (Sensitivity or TPR): the proportion of true positive predictions among all actual positive observations. It measures the ability of the classifier to detect all positive observations.
  # F1-score: the harmonic mean of precision and recall. It is a balance between precision and recall.
  # AUC-ROC: the area under the Receiver Operating Characteristic curve. It measures the ability of the classifier to distinguish between positive and negative observations.
  # AUC-PR: the area under the precision-recall curve. It also measures the ability of the classifier to distinguish between positive and negative observations, but it puts more emphasis on the true positive rate.
  # G-mean: the geometric mean of recall and specificity. It is a balance between recall and specificity and is sensitive to imbalanced data.
  # These metrics can be calculated using the following sklearn functions:

  # Precision: sklearn.metrics.precision_score(y_true, y_pred)
  # Recall: sklearn.metrics.recall_score(y_true, y_pred)
  # F1-score: sklearn.metrics.f1_score(y_true, y_pred)
  # AUC-ROC: sklearn.metrics.roc_auc_score(y_true, y_score)
  # AUC-PR: sklearn.metrics.average_precision_score(y_true, y_score) or use    precision, recall, _ = precision_recall_curve(y_test, y_pred) and AUC_PR = auc(recall, precision)
  # G-mean: sklearn.metrics.geometric_mean_score(y_true, y_pred)


  ### NOTE: xgboost parameters: https://xgboost.readthedocs.io/en/latest/parameter.html
  # https://scikit-learn.org/stable/modules/generated/sklearn.metrics.average_precision_score.html#sklearn.metrics.average_precision_score5 

  ### NOTE: Handle Imbalanced Dataset:
  ### https://xgboost.readthedocs.io/en/latest/tutorials/param_tuning.html:
  ### For common cases such as ads clickthrough log, the dataset is extremely imbalanced. This can affect the training of XGBoost ucase, and there are two ways to improve it:
  ### If you care only about the overall performance metric (AUC) of your prediction:
      ### Balance the positive and negative weights via scale_pos_weight
      ### Use AUC for evaluation
  ### If you care about predicting the right probability:
      ### In such a case, you cannot re-balance the dataset
      ### Set parameter max_delta_step to a finite number (say 1) to help convergence
  ## AUPRC and Average Precision:
  ###https://glassboxmedicine.com/2019/03/02/measuring-performance-auprc/
  ## The baseline of AUPRC is equal to the fraction of positives. If a dataset consists of 8% cancer examples and 92% healthy examples, the baseline AUPRC is 0.08, so obtaining an AUPRC of 0.40 in this scenario is good! AUPRC is most useful when you care a lot about your model handling the positive examples correctly.

  ###using scale_pos_weight and sample_weight  leads to almost the same auc:
  # when  &#34;tree_method&#34;:&#34;gpu_hist&#34; and &#34;predictor&#34;:&#34;gpu_predictor&#34; :  (0.842775:0.842165)
  # without using gpu results                                      :  (0.842609:0.842686)

  # The hyperparameters that have the greatest effect on XGBoost objective metrics are: alpha, min_child_weight, subsample, eta, and num_round.

  # Fill reasonable values for key inputs:
  # learning_rate: 0.01
  # n_estimators: 100 if the size of your data is high, 1000 is if it is medium-low
  # max_depth: 3
  # subsample: 0.8
  # colsample_bytree: 1
  # gamma: 1

  ###  I usually use 50 rounds for early stopping with 1000 trees in the model. I’ve seen in many places recommendation to use about 10% of total number of trees for early stopping
  ### early_stopping_rounds offers a way to automatically find the ideal value for n_estimators. Early stopping causes the model to stop iterating when the validation score stops improving, even if we aren&#39;t at the hard stop for n_estimators. It&#39;s smart to set a high value for n_estimators and then use early_stopping_rounds to find the optimal time to stop iterating.

  ##xgboost GPU parameter:
              ##NOte: Gpu is very fast, but it doesnot give good model performance like cpu, so tune your model with gpu and use cpu to generate final prediction on CPU 
  #             &#34;tree_method&#34;:&#34;gpu_hist&#34;,
              ## &#34;gpu_id&#34;:1,
  #             &#34;predictor&#34;:&#34;gpu_predictor&#34;,
  from xgboost import XGBClassifier

  int_vals=[&#39;max_depth&#39;, &#39;reg_alpha&#39;]
  space={k: (int(val) if k in int_vals else val)    
          for k,val in space.items()}
  space[&#39;early_stopping_rounds&#39;]=early_stopping_rounds

  model=XGBClassifier(**space)
  y_model, _, df_epochs = ml_prediction(model,
                                              X,
                                              y,
                                              sk_fold,
                                              )
  
  #eval_metric used by early stopping comes from xgboost package and there is not same as metrics in sklearn:
    #https://xgboost.readthedocs.io/en/stable/parameter.html
    #https://scikit-learn.org/stable/modules/classes.html 
  #for instance use recall metric doesnot exists in eval_metric. Moreover same metric have difference name:  aucpr in eval_metric is same as average_precision_score in metrics
  ##so there is need to used dictionary to convert  eval_metric to metric  or add more items in metric_dict       
                                      
  scores=ml_scores(y_model, [Umetric])
  scores_sub=scores.loc[scores[&#39;CV&#39;]==&#39;CV_scores_Mean&#39;,Umetric]

  return {&#39;loss&#39;:-scores_sub, &#39;status&#39;:STATUS_OK, &#39;model&#39;:model}

def xgb_tuner(X_train, y_train,
              X_test,  y_test ,
              random_state,
              metric=roc_auc_score,
              stepWise=True):
  import xgboost as xgb 
  from sklearn.metrics import accuracy_score, roc_auc_score, average_precision_score

  from hyperopt import fmin, tpe, hp, Trials, STATUS_OK
  params = {&#39;random_state&#39;: random_state}
  rounds = [{&#39;max_depth&#39;      : hp.quniform(&#39;max_depth&#39;, 1, 8, 1), # tree
            &#39;min_child_weight&#39;: hp.loguniform(&#39;min_child_weight&#39;, -2, 3)},

            {&#39;subsample&#39;      : hp.uniform(&#39;subsample&#39;, .5, 1), # stochastic
            &#39;colsample_bytree&#39;: hp.uniform(&#39;colsample_bytree&#39;, .5, 1)},

            {&#39;reg_alpha&#39;      : hp.uniform(&#39;reg_alpha&#39;, 0, 10),
            &#39;reg_lambda&#39;     : hp.uniform(&#39;reg_lambda&#39;, 1, 10),},
            
            {&#39;gamma&#39;          : hp.loguniform(&#39;gamma&#39;, -10, 10)}, # regularization
            {&#39;learning_rate&#39;  : hp.loguniform(&#39;learning_rate&#39;, -7, 0)} # boosting
            ]
  if not stepWise:
    rounds = [{&#39;max_depth&#39;      : hp.quniform(&#39;max_depth&#39;, 1, 8, 1), # tree
            &#39;min_child_weight&#39;: hp.loguniform(&#39;min_child_weight&#39;, -2, 3),

            &#39;subsample&#39;      : hp.uniform(&#39;subsample&#39;, .5, 1), # stochastic
            &#39;colsample_bytree&#39;: hp.uniform(&#39;colsample_bytree&#39;, .5, 1),

            &#39;reg_alpha&#39;      : hp.uniform(&#39;reg_alpha&#39;, 0, 10),
            &#39;reg_lambda&#39;     : hp.uniform(&#39;reg_lambda&#39;, 1, 10),
            
            &#39;gamma&#39;          : hp.loguniform(&#39;gamma&#39;, -10, 10), # regularization
            &#39;learning_rate&#39;  : hp.loguniform(&#39;learning_rate&#39;, -7, 0)} # boosting
            ]
  for round in rounds:
    params = {**params, **round}
    trials = Trials()
    best = fmin(fn=lambda space: hyperparameter_tuning(space,
                                                        X_train,y_train,
                                                        X_test, y_test,
                                                        metric=metric),
                space=params,
                algo=tpe.suggest,
                max_evals=200 if stepWise else 1500,
                trials=trials,
                )
  params = {**params, **best}

  params[&#39;max_depth&#39;]=int(params[&#39;max_depth&#39;])

  return params, trials
####------------------------------PCA Functions--------------------------------------------------------------------------------------------------
##-----------------------------------------------------------------------------------------------------------------------------------------------
##-----------------------------------------------------------------------------------------------------------------------------------------------
##-----------------------------------------------------------------------------------------------------------------------------------------------
from sklearn.decomposition import PCA

def pca_explainedVar(pcaML):
    &#34;&#34;&#34; calcluate and plot Variance Explained VS number of features for PCA
    ##TODO: add screeplot
    Parameters:
    ----------
    pcaML (float): Percentage of variance explained by each of the selected components.

    outputFile (string):
    the location of the plot

    returns:
    -------
    var  (float)
    cumulative varaince explained

    -------
    Author: Reza Nourzadeh 
    &#34;&#34;&#34;
    
    eigen_values=pcaML.explained_variance_

    np.round(
            pcaML.explained_variance_ratio_,
            decimals=3)

    explained_var = np.cumsum(np.round(pcaML.explained_variance_ratio_,decimals=3) * 100)

    plt.ylabel(&#39;% explained_variance Explained&#39;)
    plt.xlabel(&#39;# of Features&#39;)
    plt.title(&#39;PCA Analysis&#39;)

    plt.ylim(0, 100)
    plt.style.context(&#39;seaborn-whitegrid&#39;)
    plt.grid()
    plt.plot(explained_var)

    return explained_var,eigen_values

def pca_ortho_rotation(lam,
                   method  = &#39;varimax&#39;,
                   gamma   = None,
                   eps     = 1e-6,
                   itermax = 100
                   ):
    &#34;&#34;&#34;
    ##TODO: document it 
    ## A VARIMAX rotation is a change of coordinates used in principal component analysis1 (PCA) that maximizes the sum of the variances of the squared loadings
    ## https://github.com/rossfadely/consomme/blob/master/consomme/rotate_factor.py
    Return orthogal rotation matrix
    TODO: - other types beyond 
    &#34;&#34;&#34;
    if gamma == None:
        if (method == &#39;varimax&#39;):
            gamma = 1.0
        if (method == &#39;quartimax&#39;):
            gamma = 0.0

    nrow, ncol = lam.shape
    R = np.eye(ncol)
    var = 0

    for i in range(itermax):
        lam_rot = np.dot(lam, R)
        tmp     = np.diag(np.sum(lam_rot ** 2, axis = 0)) / nrow * gamma
        u, s, v = np.linalg.svd(np.dot(lam.T, lam_rot ** 3 - np.dot(lam_rot, tmp)))
        R       = np.dot(u, v)
        var_new = np.sum(s)
        if var_new &lt; var * (1 + eps):
            break
        var = var_new

    return R

def pca_important_features(transformed_features, components_, columns):
    import math
        ##TODO: check it and make a function
    ###http://benalexkeen.com/principle-component-analysis-in-python/    
    &#34;&#34;&#34;
    This function will return the most &#34;important&#34; 
    features so we can determine which have the most
    effect on multi-dimensional scaling
    &#34;&#34;&#34;
    num_columns = len(columns)

    # Scale the principal components by the max value in
    # the transformed set belonging to that component
    xvector = components_[0] * max(transformed_features[:,0])
    yvector = components_[1] * max(transformed_features[:,1])

    # Sort each column by it&#39;s length. These are your *original*
    # columns, not the principal components.
    important_features = { columns[i] : math.sqrt(xvector[i]**2 + yvector[i]**2) for i in range(num_columns) }
    # important_features = sorted(zip(important_features.values(), important_features.keys()), reverse=True)
    important_features = pd.Series(important_features)
    important_features = important_features.sort_values(ascending=[False])
    return important_features

####------------------------------other Functions--------------------------------------------------------------------------------------------------
##-----------------------------------------------------------------------------------------------------------------------------------------------
##-----------------------------------------------------------------------------------------------------------------------------------------------
##-----------------------------------------------------------------------------------------------------------------------------------------------
##TODO: retire if not be used:
def class_weight2(uclass_weight,y):
    &#34;&#34;&#34; crearte a numerical series of samples&#39; weights based on class_weight dictionary 
    Parameters:
    ----------
    class_weight (dictionary) or “balanced” or None, default=None
    Weights associated with classes in the form {class_label: weight}. If not given, all classes are supposed to have weight one. For multi-output problems, a list of dicts can be provided in the same order as the columns of y.

    y (pandas dataframe/series) with [sample*1] format

    returns:
    -------
    df_class_weight  (pandas series) with [sample*1] format:
    weight of samples

    -------
    Author: Reza Nourzadeh 

    &#34;&#34;&#34; 
    ##y=y_train.cat.codes
    if uclass_weight == &#39;balanced&#39;:
        from sklearn.utils import class_weight
        tmp = np.round(class_weight.compute_class_weight(
            &#39;balanced&#39;, np.unique(y.sort_values()), y), 2)
        class_weight_map = dict(zip(y.sort_values().unique().tolist(), tmp))
        df_class_weight = (y.map(class_weight_map))
    elif uclass_weight is None :
        df_class_weight = pd.Series(np.tile(1, y.size))
    else:
        df_class_weight = uclass_weight

    return df_class_weight

####------------------------------Survival Analysis--------------------------------------------------------------------------------------------------
##-----------------------------------------------------------------------------------------------------------------------------------------------
##-----------------------------------------------------------------------------------------------------------------------------------------------
##-----------------------------------------------------------------------------------------------------------------------------------------------

    from sklearn.model_selection import KFold
    from sklearn.model_selection import TimeSeriesSplit
    from sklearn.compose import ColumnTransformer

    # from sklearn.feature_selection import SelectPercentile, chi2
    # from sklearn.impute import SimpleImputer
    # from sklearn.linear_model import LogisticRegression
    from sklearn.model_selection import RandomizedSearchCV, train_test_split
    from sklearn.pipeline import Pipeline
    from sklearn.base import is_classifier

    un_splits = 10
    # sk_fold  = StratifiedKFold(n_splits=un_splits, shuffle=True, random_state=RANDOM_STATE)
    sk_fold = KFold(n_splits=un_splits, shuffle=True, random_state=RANDOM_STATE)

    # sk_fold = TimeSeriesSplit(n_splits=un_splits)

    scores_names = [
        &#34;R2&#34;,
        &#34;explained_variance&#34;,
        &#34;mean_squared_error&#34;,
        &#34;mean_absolute_error&#34;,
        &#34;median_absolute_error&#34;,
        &#34;max_error&#34;,
        # &#39;mean_squared_log_error&#39;  ,
        # &#39;mean_poisson_deviance&#39;  ,
        # &#39;mean_gamma_deviance&#39; ,
        # &#39;mean_absolute_percentage_error&#39;,
    ]

    df_ml_comparison_both = pd.DataFrame([])

    regressors2 = ml_funcs.regressors_template(y, random_state=RANDOM_STATE)

    keys_filter, _ = cfuncs.inWithReg(
        [&#34;lightgbm&#34;, &#34;neighbors&#34;, &#34;lars&#34;, &#34;naive&#34;],
        [i.lower() for i in regressors2.keys()],
    )
    regressors2 = {
        key: value
        for key, value in regressors2.items()
        if key.lower() not in keys_filter
    }

    mapNames = dict(zip(range(len(regressors2.keys())), regressors2.keys()))

    df_ml_comparison_regressors = ml_funcs.ml_comparison(
        list(regressors2.values()),
        X,
        y,
        scores_names,
        sk_fold,
        mapNames=mapNames,
        plot=plot,
        verbose=False,
    )

    df_ml_comparison_regressors[&#34;CV&#34;] = df_ml_comparison_regressors[&#34;CV&#34;].astype(str)
    df_ml_comparison_regressors[&#34;elapsed_time&#34;] = df_ml_comparison_regressors[
        &#34;elapsed_time&#34;
    ].astype(&#34;str&#34;)
    df_ml_comparison_regressors[&#34;Feature_nos&#34;] = X.shape[1]

    summary = df_ml_comparison_regressors[
        (
            (
                df_ml_comparison_regressors[&#34;CV&#34;].isin(
                    [
                        &#34;CV_scores_Mean&#34;,
                        #  &#39;CV_scores_STD&#39;,
                        #  &#39;scores_all&#39;
                    ]
                )
            )
        )
    ].sort_values(by=[&#34;CV&#34;, &#34;R2&#34;, &#34;explained_variance&#34;], ascending=False)

    # summary=summary.drop([&#39;CV_scores_Mean&#39;],axis=1)

    io_funcs.pd2blob(
        df_ml_comparison_regressors,
        blob_dict={
            &#34;container&#34;: &#34;undercarriage-wear-analysis&#34;,
            &#34;blob&#34;: f&#34;df_ml_comparison_regressors{output_suffix}.csv&#34;,
            &#34;storage_account&#34;: &#34;kearlmachinemidasdata&#34;,
        },
        platform=platform,
    )

    return df_ml_comparison_regressors, summary</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="ml_funcs.class_weight2"><code class="name flex">
<span>def <span class="ident">class_weight2</span></span>(<span>uclass_weight, y)</span>
</code></dt>
<dd>
<div class="desc"><p>crearte a numerical series of samples' weights based on class_weight dictionary
Parameters:</p>
<hr>
<p>class_weight (dictionary) or “balanced” or None, default=None
Weights associated with classes in the form {class_label: weight}. If not given, all classes are supposed to have weight one. For multi-output problems, a list of dicts can be provided in the same order as the columns of y.</p>
<p>y (pandas dataframe/series) with [sample*1] format</p>
<h2 id="returns">returns:</h2>
<p>df_class_weight
(pandas series) with [sample*1] format:
weight of samples</p>
<hr>
<p>Author: Reza Nourzadeh</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def class_weight2(uclass_weight,y):
    &#34;&#34;&#34; crearte a numerical series of samples&#39; weights based on class_weight dictionary 
    Parameters:
    ----------
    class_weight (dictionary) or “balanced” or None, default=None
    Weights associated with classes in the form {class_label: weight}. If not given, all classes are supposed to have weight one. For multi-output problems, a list of dicts can be provided in the same order as the columns of y.

    y (pandas dataframe/series) with [sample*1] format

    returns:
    -------
    df_class_weight  (pandas series) with [sample*1] format:
    weight of samples

    -------
    Author: Reza Nourzadeh 

    &#34;&#34;&#34; 
    ##y=y_train.cat.codes
    if uclass_weight == &#39;balanced&#39;:
        from sklearn.utils import class_weight
        tmp = np.round(class_weight.compute_class_weight(
            &#39;balanced&#39;, np.unique(y.sort_values()), y), 2)
        class_weight_map = dict(zip(y.sort_values().unique().tolist(), tmp))
        df_class_weight = (y.map(class_weight_map))
    elif uclass_weight is None :
        df_class_weight = pd.Series(np.tile(1, y.size))
    else:
        df_class_weight = uclass_weight

    return df_class_weight

####------------------------------Survival Analysis--------------------------------------------------------------------------------------------------
##-----------------------------------------------------------------------------------------------------------------------------------------------
##-----------------------------------------------------------------------------------------------------------------------------------------------
##-----------------------------------------------------------------------------------------------------------------------------------------------

    from sklearn.model_selection import KFold
    from sklearn.model_selection import TimeSeriesSplit
    from sklearn.compose import ColumnTransformer

    # from sklearn.feature_selection import SelectPercentile, chi2
    # from sklearn.impute import SimpleImputer
    # from sklearn.linear_model import LogisticRegression
    from sklearn.model_selection import RandomizedSearchCV, train_test_split
    from sklearn.pipeline import Pipeline
    from sklearn.base import is_classifier

    un_splits = 10
    # sk_fold  = StratifiedKFold(n_splits=un_splits, shuffle=True, random_state=RANDOM_STATE)
    sk_fold = KFold(n_splits=un_splits, shuffle=True, random_state=RANDOM_STATE)

    # sk_fold = TimeSeriesSplit(n_splits=un_splits)

    scores_names = [
        &#34;R2&#34;,
        &#34;explained_variance&#34;,
        &#34;mean_squared_error&#34;,
        &#34;mean_absolute_error&#34;,
        &#34;median_absolute_error&#34;,
        &#34;max_error&#34;,
        # &#39;mean_squared_log_error&#39;  ,
        # &#39;mean_poisson_deviance&#39;  ,
        # &#39;mean_gamma_deviance&#39; ,
        # &#39;mean_absolute_percentage_error&#39;,
    ]

    df_ml_comparison_both = pd.DataFrame([])

    regressors2 = ml_funcs.regressors_template(y, random_state=RANDOM_STATE)

    keys_filter, _ = cfuncs.inWithReg(
        [&#34;lightgbm&#34;, &#34;neighbors&#34;, &#34;lars&#34;, &#34;naive&#34;],
        [i.lower() for i in regressors2.keys()],
    )
    regressors2 = {
        key: value
        for key, value in regressors2.items()
        if key.lower() not in keys_filter
    }

    mapNames = dict(zip(range(len(regressors2.keys())), regressors2.keys()))

    df_ml_comparison_regressors = ml_funcs.ml_comparison(
        list(regressors2.values()),
        X,
        y,
        scores_names,
        sk_fold,
        mapNames=mapNames,
        plot=plot,
        verbose=False,
    )

    df_ml_comparison_regressors[&#34;CV&#34;] = df_ml_comparison_regressors[&#34;CV&#34;].astype(str)
    df_ml_comparison_regressors[&#34;elapsed_time&#34;] = df_ml_comparison_regressors[
        &#34;elapsed_time&#34;
    ].astype(&#34;str&#34;)
    df_ml_comparison_regressors[&#34;Feature_nos&#34;] = X.shape[1]

    summary = df_ml_comparison_regressors[
        (
            (
                df_ml_comparison_regressors[&#34;CV&#34;].isin(
                    [
                        &#34;CV_scores_Mean&#34;,
                        #  &#39;CV_scores_STD&#39;,
                        #  &#39;scores_all&#39;
                    ]
                )
            )
        )
    ].sort_values(by=[&#34;CV&#34;, &#34;R2&#34;, &#34;explained_variance&#34;], ascending=False)

    # summary=summary.drop([&#39;CV_scores_Mean&#39;],axis=1)

    io_funcs.pd2blob(
        df_ml_comparison_regressors,
        blob_dict={
            &#34;container&#34;: &#34;undercarriage-wear-analysis&#34;,
            &#34;blob&#34;: f&#34;df_ml_comparison_regressors{output_suffix}.csv&#34;,
            &#34;storage_account&#34;: &#34;kearlmachinemidasdata&#34;,
        },
        platform=platform,
    )

    return df_ml_comparison_regressors, summary</code></pre>
</details>
</dd>
<dt id="ml_funcs.classifer_performance_batch"><code class="name flex">
<span>def <span class="ident">classifer_performance_batch</span></span>(<span>y_model, map_lbls={0: 'Low Loss', 1: 'High Loss'}, scores_names=['accuracy', 'recall', 'precision'])</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def classifer_performance_batch(y_model,
                        map_lbls={0:&#39;Low Loss&#39;, 1:&#39;High Loss&#39;},
                        scores_names=[
                                      &#39;accuracy&#39;,
                                      # &#39;balanced_accuracy&#39;,
                                      &#39;recall&#39; ,
                                      &#39;precision&#39;,
                                      # &#39;roc_auc&#39;,  
                                      # &#39;aucpr&#39;,
                                      ]
                        ):
  confMats=plot_confusion_matrix2(y_model, map_lbls, outputFile=None)

  model_prob=y_model[1] #y_model[map_lbls.get(1)]
  pos_label=1 #map_lbls.get(1)  

  df_rp, thresholds= precision_recall_curve2(y_model[&#39;y_true&#39;],
                                                      model_prob,
                                                      pos_label,
                                                      outputFile=None
                                                      )

  df_auc, thresholds= roc_curve2(y_model[&#39;y_true&#39;],
                                          model_prob,
                                          pos_label,
                                          outputFile=None
                                        )

  out, df_gain_chart, df_lift_chart = gainNlift(y_model[&#39;y_true&#39;],
                                                model_prob,
                                                pos_label,
                                                outputFile=None,
                                                groupNo = 25
                                                )
  
  scores= ml_scores(y_model, scores_names)
  return scores, confMats</code></pre>
</details>
</dd>
<dt id="ml_funcs.classifiers_template"><code class="name flex">
<span>def <span class="ident">classifiers_template</span></span>(<span>y, random_state=10)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def classifiers_template(y, random_state=10):
  
  import numpy as np

  from sklearn.pipeline import Pipeline
  from sklearn.preprocessing import StandardScaler
  from sklearn.decomposition import PCA
  from sklearn.impute import SimpleImputer
  
  from sklearn.tree import DecisionTreeClassifier
  from sklearn import tree
  from sklearn.svm import SVC, LinearSVC, NuSVC
  from sklearn.gaussian_process import GaussianProcessClassifier
  from sklearn.gaussian_process.kernels import RBF

  from xgboost import XGBClassifier
  from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier, BaggingClassifier
  from lightgbm import LGBMClassifier

  from sklearn.linear_model import LogisticRegression

  from sklearn.naive_bayes import GaussianNB

  from sklearn.neighbors import KNeighborsClassifier

  from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
  from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis

  from sklearn.neural_network import MLPClassifier
  from sklearn.ensemble import GradientBoostingClassifier

  classifiers={
  &#34;Nearest_Neighbors_2&#34;:            KNeighborsClassifier(2),
  &#34;Nearest_Neighbors_3&#34;:            KNeighborsClassifier(3),
  &#34;Nearest_Neighbors_4&#34;:            KNeighborsClassifier(4),
  &#34;Nearest_Neighbors_5&#34;:            KNeighborsClassifier(5),

  &#34;Decision_Tree_depth5&#34;:         DecisionTreeClassifier(max_depth=5, random_state=random_state),
  &#34;Decision_Tree_depth10&#34;:        DecisionTreeClassifier(max_depth=10, random_state=random_state),

  &#34;Naive_Bayes&#34;:                  GaussianNB(),

  &#34;LinearDiscriminantAnalysis&#34;    :LinearDiscriminantAnalysis(),
  &#34;QuadraticDiscriminantAnalysis&#34; :QuadraticDiscriminantAnalysis(),

  &#39;logReg_mode_l1&#39;   :            LogisticRegression(penalty=&#39;l1&#39;, solver=&#39;liblinear&#39;, max_iter=1000, random_state=random_state),
  &#39;logReg_mode_l2&#39;   :            LogisticRegression(max_iter=1000, random_state=random_state),
  &#39;logReg_model_pca&#39;:             LogisticRegression(max_iter=1000, random_state=random_state),

  &#39;RandomForest_model1&#39;:          RandomForestClassifier(random_state=random_state),
  &#39;RandomForest_model_balanced&#39;:  RandomForestClassifier(class_weight=&#39;balanced&#39;, random_state=random_state),
  &#39;RandomForest_model_n200&#39;:      RandomForestClassifier(n_estimators=200, random_state=random_state),
  &#39;RandomForest_model_n300&#39;:      RandomForestClassifier(n_estimators=300, random_state=random_state),
  &#39;Xgboost_n200&#39; :                XGBClassifier(n_estimators=200, random_state=random_state),
  &#39;Xgboost_n200_dp10&#39;:            XGBClassifier(n_estimators=200, max_depth=10, random_state=random_state) ,
  &#39;Xgboost_Weighted&#39;:             XGBClassifier(scale_pos_weight=float(np.sum(y == 0)) / np.sum(y==1), random_state=random_state) ,
  &#39;Xgboost_Weighted_n200&#39;:        XGBClassifier(n_estimators=200, scale_pos_weight=float(np.sum(y == 0)) / np.sum(y==1), random_state=random_state) ,
  &#39;Xgboost_Weighted_n200_dp10&#39;:   XGBClassifier(n_estimators=200, max_depth=10, scale_pos_weight=float(np.sum(y == 0)) / np.sum(y==1), random_state=random_state) ,

  &#39;LightGBM&#39;:                     LGBMClassifier(random_state=random_state),
  &#39;LightGBM_n200&#39;:                LGBMClassifier(n_estimators=200, random_state=random_state),
  &#39;LightGBM_n400&#39;:                LGBMClassifier(n_estimators=300, random_state=random_state),
  &#39;LightGBM_n1000&#39;:                LGBMClassifier(n_estimators=1000, random_state=random_state),
  &#39;LightGBM_n200_dp10&#39;:           LGBMClassifier(max_depth=10,  n_estimators=200, random_state=random_state),
  &#39;LightGBM_n300_dp10&#39;:           LGBMClassifier(max_depth=10,  n_estimators=300, random_state=random_state),

  &#39;MLPClassifier1&#39;   :            MLPClassifier(alpha=1, max_iter=1000, random_state=random_state),
  &#39;MLPClassifier_early_stopping&#39;   :  MLPClassifier(alpha=1, max_iter=1000, early_stopping=True, random_state=random_state),
  &#39;MLPClassifier3&#39;   :            MLPClassifier(alpha=1, max_iter=1000, solver=&#39;sgd&#39;, early_stopping=True, random_state=random_state),

  # &#34;Linear_SVM&#34;:                  SVC(kernel=&#34;linear&#34;, C=0.025, probability=True, random_state=random_state),
  # &#34;RBF_SVM&#34;:                     SVC(kernel=&#34;rbf&#34;, C=0.025, probability=True, random_state=random_state),
  # &#34;NuSVC&#34;:                       NuSVC(probability=True, random_state=random_state),

  &#34;AdaBoost&#34;:                     AdaBoostClassifier(random_state=random_state),
  # &#39;bagging&#39;:                      BaggingClassifier(estimator=DecisionTreeClassifier(), n_estimators=10, random_state=random_state),

  }

  basic_params = {&#34;random_state&#34;:random_state}

  classifiers2={}
  for name, classifier in classifiers.items():
    ##TODO: find a way to add randomstate here
    # params={**basic_params,**classifier.get_params()}
    # print(params)
    if any([x in name for x in [&#39;pca&#39;,&#39;DiscriminantAnalysis&#39;]]):  
        classifiers2[name]= Pipeline(steps=[(&#34;imputer&#34;, SimpleImputer(strategy=&#34;median&#34;)),
                                            (&#34;scaler&#34;, StandardScaler()),
                                            (&#34;reduce_dims&#34;, PCA(n_components=20)),
                                            (name, classifier)])
        
    elif (&#39;xgb&#39; not in name.lower()) &amp; (&#39;gbm&#39; not in name.lower()) :
        classifiers2[name]= Pipeline(steps=[(&#34;imputer&#34;, SimpleImputer(strategy=&#34;median&#34;)),
                                            (&#34;scaler&#34;, StandardScaler()),
                                            (name, classifier)])
    else:
        classifiers2[name]=classifier
  return classifiers2</code></pre>
</details>
</dd>
<dt id="ml_funcs.feature_importance_batch"><code class="name flex">
<span>def <span class="ident">feature_importance_batch</span></span>(<span>umodel, X, y)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def feature_importance_batch(umodel, X, y):
    from sklearn.pipeline import Pipeline

    umodel1 = umodel[-1] if isinstance(umodel, Pipeline) else umodel
    model_name = umodel1.__class__.__name__

    ##TODO: generalize for any tree model:
    if &#34;xgb&#34; in model_name.lower():
        from xgboost import plot_importance

        umodel.fit(X, y)
        plt.figure(
            figsize=(100, 100),
            dpi=150,
        )
        sns.set(rc={&#34;figure.figsize&#34;: (20, 10)}, font_scale=1)
        plot_importance(umodel, max_num_features=30)
        plt.show()
        plt.close()

        feature_importance = pd.Series(
            umodel.feature_importances_, index=X.columns
        ).sort_values(ascending=False)
        print(
            &#34;non zero features:&#34;,
            feature_importance[feature_importance &gt;= 0.01].index.tolist(),
        )
        print(
            &#34;zero features:&#34;,
            feature_importance[feature_importance &lt; 0.01].index.tolist(),
        )

        plt.figure(figsize=(5, 2), dpi=150)

        importance_hist = feature_importance.hist()

        sns.set(rc={&#34;figure.figsize&#34;: (20, 10)}, font_scale=1.5)
        importance_hist

        sel_features = feature_importance[feature_importance &gt; 0.01].index.tolist()
    else:
        print(&#34;The model is not tree based- returning None&#34;)
        feature_importance = None
        sel_features = X.columns

    return feature_importance, sel_features</code></pre>
</details>
</dd>
<dt id="ml_funcs.gainNlift"><code class="name flex">
<span>def <span class="ident">gainNlift</span></span>(<span>y, model_prob, pos_label, outputFile, groupNo=25)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def gainNlift(y, model_prob, pos_label, outputFile, groupNo=25):
    ## Lift/cumulative gains charts aren&#39;t a good way to evaluate a model (as it cannot be used for comparison between ml_models), and are instead a means of evaluating the results where your resources are finite. Either because there&#39;s a cost to action each result (in a marketing scenario) or you want to ignore a certain number of guaranteed voters, and only action those that are on the fence. Where your model is very good, and has high classification accuracy for all results, you won&#39;t get much lift from ordering your results by confidence.(https://stackoverflow.com/questions/42699243/how-to-build-a-lift-chart-a-k-a-gains-chart-in-python)

    ## gain Interpretation:
    ## % of targets (events) covered at a given decile level. For example,  80% of targets covered in top 20% of data based in model. In the case of propensity to buy model, we can say we can identify and target 80% of customers who are likely to buy the product by just sending email to 20% of total customers.
    ## lift Interpretation:
    ## The Cum Lift of 4.03 for top two deciles, means that when selecting 20% of the records based on the model, one can expect 4.03 times the total number of targets (events) found by randomly selecting 20%-of-file without a model.

    ##note: when intersted column has much more freq than the other:
    #  df[df.columns[0].value_counts()
            # bluecurvetv        127289
            # Decline_offer    2853
    ##the lift and gain chart of intersted column doesnot show any supererioty of using ML,(model and random output shows same output)

    df=pd.concat([y,model_prob],axis=1)
    df.sort_values(by=df.columns[1], ascending=False, inplace=True)

    def gain_stp1(subset):
        pos_event=sum(subset[y.name]==pos_label)
        return  len(subset), pos_event

    # subset=np.array_split(df,groupNo)[0]
    tmp=list(map(gain_stp1,np.array_split(df,groupNo)))
    out = pd.DataFrame(tmp,columns=[&#39;case&#39;,&#39;event&#39;])
    out[&#39;event%&#39;]=out[&#39;event&#39;]/out[&#39;event&#39;].sum()*100
    out[&#39;cum_case&#39;] = out[&#39;case&#39;].cumsum()
    out[&#39;cum_case%&#39;] = out[&#39;cum_case&#39;]/out[&#39;case&#39;].sum()*100
    out[&#39;gain&#39;] = out[&#39;event%&#39;].cumsum()
    out[&#39;cum_lift&#39;] = out[&#39;gain&#39;]/out[&#39;cum_case%&#39;]

    row_no=int(out.shape[0])

    df_gain_chart=pd.DataFrame(out[&#39;gain&#39;].tolist()+out[&#39;cum_case%&#39;].tolist(),columns=[&#39;values&#39;])
    df_gain_chart[&#39;x&#39;]=pd.Series(out[&#39;cum_case%&#39;].tolist()*2)
    df_gain_chart[&#39;selection method&#39;]=pd.Series([&#39;model&#39;]*row_no+[&#39;random&#39;]*row_no)
    df_gain_chart=df_gain_chart.append(pd.DataFrame.from_dict({&#39;values&#39;:[0,0],&#34;x&#34;:[0,0],&#39;selection method&#39;:[&#39;model&#39;,&#39;random&#39;]}), ignore_index=True)

    df_lift_chart=pd.DataFrame(out[&#39;cum_lift&#39;].tolist()+[1]*row_no,columns=[&#39;values&#39;])
    df_lift_chart[&#39;x&#39;]=pd.Series(out[&#39;cum_case%&#39;].tolist()*row_no)
    df_lift_chart[&#39;selection method&#39;]=pd.Series([&#39;model&#39;]*row_no+[&#39;random&#39;]*row_no)
        
    fig, ax = plt.subplots(2,1,figsize=(20, 10))   
    
    uPlot1=sns.lineplot(data=df_gain_chart,
                        ax=ax[0],
                        x=&#39;x&#39;,
                        y=&#39;values&#39;,
                        hue=&#39;selection method&#39;,
                        style=&#39;selection method&#39;,
                        markers=True
                       )
    uPlot1.set(xlabel=&#39;&#39;, ylabel=&#39;% of events&#39;)
    ax[0].set_title(&#39;Gain Chart&#39;)

    uPlot2=sns.lineplot(data=df_lift_chart,
                        ax=ax[1],
                        x=&#39;x&#39;,
                        y=&#39;values&#39;,
                        hue=&#39;selection method&#39;,
                        style=&#39;selection method&#39;,
                        markers=True
                       )
    uPlot2.set(xlabel=&#39;% 0f data sets&#39;, ylabel=&#39;Lift&#39;)
    ax[1].set_title(&#39;Lift Chart&#39;)
    
    plt.ylim(0,int(df_lift_chart[&#39;values&#39;].max()+1))
    plt.xlim(0,100) 
    
    if outputFile is not None:
      uPlot1.get_figure().savefig(outputFile[0], bbox_inches=&#39;tight&#39;)
      uPlot2.get_figure().savefig(outputFile[1], bbox_inches=&#39;tight&#39;)
      plt.close(&#39;all&#39;)

    return out, df_gain_chart, df_lift_chart</code></pre>
</details>
</dd>
<dt id="ml_funcs.hyperparameter_tuning"><code class="name flex">
<span>def <span class="ident">hyperparameter_tuning</span></span>(<span>space: Dict[str, Union[float, int]], X: pandas.core.frame.DataFrame, y: pandas.core.series.Series, sk_fold, early_stopping_rounds: int = 50, Umetric: <built-in function callable> = &lt;function accuracy_score&gt;) ‑> Dict[str, Any]</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def hyperparameter_tuning(
                        space: Dict[str, Union[float, int]],
                        X: pd.DataFrame, y: pd.Series,
                        sk_fold,   #[X_test,y_test]
                        early_stopping_rounds: int=50,
                        Umetric:callable=accuracy_score)-&gt; Dict[str, Any]:

  # When working with imbalanced data sets, accuracy is not always the best metric to evaluate the performance of a classifier, because it can be misleading. Some alternative metrics that are better suited for imbalanced data sets are:

  # Precision: the proportion of true positive predictions among all positive predictions. It measures the ability of the classifier to avoid false positives.
  # Recall (Sensitivity or TPR): the proportion of true positive predictions among all actual positive observations. It measures the ability of the classifier to detect all positive observations.
  # F1-score: the harmonic mean of precision and recall. It is a balance between precision and recall.
  # AUC-ROC: the area under the Receiver Operating Characteristic curve. It measures the ability of the classifier to distinguish between positive and negative observations.
  # AUC-PR: the area under the precision-recall curve. It also measures the ability of the classifier to distinguish between positive and negative observations, but it puts more emphasis on the true positive rate.
  # G-mean: the geometric mean of recall and specificity. It is a balance between recall and specificity and is sensitive to imbalanced data.
  # These metrics can be calculated using the following sklearn functions:

  # Precision: sklearn.metrics.precision_score(y_true, y_pred)
  # Recall: sklearn.metrics.recall_score(y_true, y_pred)
  # F1-score: sklearn.metrics.f1_score(y_true, y_pred)
  # AUC-ROC: sklearn.metrics.roc_auc_score(y_true, y_score)
  # AUC-PR: sklearn.metrics.average_precision_score(y_true, y_score) or use    precision, recall, _ = precision_recall_curve(y_test, y_pred) and AUC_PR = auc(recall, precision)
  # G-mean: sklearn.metrics.geometric_mean_score(y_true, y_pred)


  ### NOTE: xgboost parameters: https://xgboost.readthedocs.io/en/latest/parameter.html
  # https://scikit-learn.org/stable/modules/generated/sklearn.metrics.average_precision_score.html#sklearn.metrics.average_precision_score5 

  ### NOTE: Handle Imbalanced Dataset:
  ### https://xgboost.readthedocs.io/en/latest/tutorials/param_tuning.html:
  ### For common cases such as ads clickthrough log, the dataset is extremely imbalanced. This can affect the training of XGBoost ucase, and there are two ways to improve it:
  ### If you care only about the overall performance metric (AUC) of your prediction:
      ### Balance the positive and negative weights via scale_pos_weight
      ### Use AUC for evaluation
  ### If you care about predicting the right probability:
      ### In such a case, you cannot re-balance the dataset
      ### Set parameter max_delta_step to a finite number (say 1) to help convergence
  ## AUPRC and Average Precision:
  ###https://glassboxmedicine.com/2019/03/02/measuring-performance-auprc/
  ## The baseline of AUPRC is equal to the fraction of positives. If a dataset consists of 8% cancer examples and 92% healthy examples, the baseline AUPRC is 0.08, so obtaining an AUPRC of 0.40 in this scenario is good! AUPRC is most useful when you care a lot about your model handling the positive examples correctly.

  ###using scale_pos_weight and sample_weight  leads to almost the same auc:
  # when  &#34;tree_method&#34;:&#34;gpu_hist&#34; and &#34;predictor&#34;:&#34;gpu_predictor&#34; :  (0.842775:0.842165)
  # without using gpu results                                      :  (0.842609:0.842686)

  # The hyperparameters that have the greatest effect on XGBoost objective metrics are: alpha, min_child_weight, subsample, eta, and num_round.

  # Fill reasonable values for key inputs:
  # learning_rate: 0.01
  # n_estimators: 100 if the size of your data is high, 1000 is if it is medium-low
  # max_depth: 3
  # subsample: 0.8
  # colsample_bytree: 1
  # gamma: 1

  ###  I usually use 50 rounds for early stopping with 1000 trees in the model. I’ve seen in many places recommendation to use about 10% of total number of trees for early stopping
  ### early_stopping_rounds offers a way to automatically find the ideal value for n_estimators. Early stopping causes the model to stop iterating when the validation score stops improving, even if we aren&#39;t at the hard stop for n_estimators. It&#39;s smart to set a high value for n_estimators and then use early_stopping_rounds to find the optimal time to stop iterating.

  ##xgboost GPU parameter:
              ##NOte: Gpu is very fast, but it doesnot give good model performance like cpu, so tune your model with gpu and use cpu to generate final prediction on CPU 
  #             &#34;tree_method&#34;:&#34;gpu_hist&#34;,
              ## &#34;gpu_id&#34;:1,
  #             &#34;predictor&#34;:&#34;gpu_predictor&#34;,
  from xgboost import XGBClassifier

  int_vals=[&#39;max_depth&#39;, &#39;reg_alpha&#39;]
  space={k: (int(val) if k in int_vals else val)    
          for k,val in space.items()}
  space[&#39;early_stopping_rounds&#39;]=early_stopping_rounds

  model=XGBClassifier(**space)
  y_model, _, df_epochs = ml_prediction(model,
                                              X,
                                              y,
                                              sk_fold,
                                              )
  
  #eval_metric used by early stopping comes from xgboost package and there is not same as metrics in sklearn:
    #https://xgboost.readthedocs.io/en/stable/parameter.html
    #https://scikit-learn.org/stable/modules/classes.html 
  #for instance use recall metric doesnot exists in eval_metric. Moreover same metric have difference name:  aucpr in eval_metric is same as average_precision_score in metrics
  ##so there is need to used dictionary to convert  eval_metric to metric  or add more items in metric_dict       
                                      
  scores=ml_scores(y_model, [Umetric])
  scores_sub=scores.loc[scores[&#39;CV&#39;]==&#39;CV_scores_Mean&#39;,Umetric]

  return {&#39;loss&#39;:-scores_sub, &#39;status&#39;:STATUS_OK, &#39;model&#39;:model}</code></pre>
</details>
</dd>
<dt id="ml_funcs.learning_curve_early_stopping"><code class="name flex">
<span>def <span class="ident">learning_curve_early_stopping</span></span>(<span>df_epochs, outputFile=None)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def learning_curve_early_stopping(df_epochs, outputFile=None):
  ###https://machinelearningmastery.com/avoid-overfitting-by-early-stopping-with-xgboost-in-python/

  ##TODO: it is only for xgb now (best_ntree)
  cols=df_epochs.columns[~df_epochs.columns.str.contains(&#39;Validation_|Train_&#39;)].tolist()
  df_epochs_melted=df_epochs.melt(id_vars=cols)
  uPlot=sns.relplot(
                    data=df_epochs_melted,
                    y=&#34;value&#34;,
                    x=&#34;epochs&#34;,
                    col=&#34;CV_Iteration&#34;,
                    hue=&#34;variable&#34;,
                    style=&#34;variable&#34;,
                    kind=&#34;line&#34;,
        #             markers=True,
                    palette=[&#39;green&#39;, &#39;black&#39;],
                    col_wrap=3
                )

  axes = uPlot.axes.flatten()

  sns.set(rc = {&#39;figure.figsize&#39;:(60,30)})
  for con, ax in enumerate(axes):
      data_tmp=df_epochs_melted[df_epochs_melted[&#39;CV_Iteration&#39;]==con]
      xc=data_tmp.loc[data_tmp[&#39;best_ntree&#39;],&#39;epochs&#39;]
      ax.axvline(xc.iloc[0], ls=&#39;-&#39;, linewidth=3, color=&#39;red&#39;, alpha=0.75)
    
  # fig, ax = plt.subplots(math.ceil((df_epochs_melted[&#39;CV_Iteration&#39;].nunique())/3), 3, figsize=(30, 20))
  # for subplot, var in enumerate(df_epochs_melted[&#39;CV_Iteration&#39;].unique()):
  #   axs=ax.flatten()[subplot]
  #   data_tmp=df_epochs_melted[df_epochs_melted[&#39;CV_Iteration&#39;]==var]
  #   sns.lineplot(
  #                ax=axs,
  #                data=data_tmp,
  #                 y=&#34;value&#34;,
  #                 x=&#34;epochs&#34;,
  #                 hue=&#34;variable&#34;,
  #                 style=&#34;variable&#34;,
  #               )
  #   xc=data_tmp.loc[data_tmp[&#39;best_ntree&#39;],&#39;epochs&#39;]
  #   axs.title.set_text(f&#39;cv_itr: {var}&#39;)
  #   axs.axvline(x=xc.iloc[0],
  #               color=&#39;red&#39;,
  #               linestyle=&#39;--&#39;)
  
  if outputFile is not None:
    figure = uPlot.get_figure()
    # ,&#34;learning_curve.png&#34;)
    figure.savefig(outputFile, bbox_inches=&#39;tight&#39;)
    plt.close(&#39;all&#39;)</code></pre>
</details>
</dd>
<dt id="ml_funcs.ml_comparison"><code class="name flex">
<span>def <span class="ident">ml_comparison</span></span>(<span>ml_models, X, y, scores_names, sk_fold, mapNames={}, plot=True, verbose=True)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def ml_comparison(ml_models,
                  X,
                  y,
                  scores_names,
                  sk_fold,
                  mapNames={},
                  plot=True,
                  verbose=True
                  ):
  import warnings
  from sklearn.pipeline import Pipeline
  import datetime as dt
  with warnings.catch_warnings():
    if verbose:
      warnings.simplefilter(&#34;default&#34;)
    else:
      warnings.simplefilter(&#34;ignore&#34;)  
    metrics_all=pd.DataFrame()
    for con, model in enumerate(ml_models):
      if con in mapNames.keys():
        model_name=mapNames.get(con)    
      elif isinstance(model, Pipeline):
        model_name=&#39;--&gt;&#39;.join([i.__class__.__name__ for i in model])
      else:
        model_name=model.__class__.__name__
      
      start_time=dt.datetime.now()
      print(model_name+&#39;...&#39;)

      y_model,  _ , _ = ml_prediction(model,
                                        X,
                                        y,
                                        sk_fold,
                                        )

      cv_results= ml_scores(y_model, scores_names)

      tmp=cv_results
      tmp.insert(0, &#34;model&#34;, model_name)
      end_time = dt.datetime.now()
      run_time=end_time-start_time
      tmp[&#39;elapsed_time&#39;]= run_time

      #tmp.index=pd.MultiIndex.from_product([[model_name],tmp.index])
      metrics_all=pd.concat([metrics_all, tmp] ,axis=0)

      # print(tmp)
      # print(&#39;run_time:&#39;, run_time)
      if verbose:
        txt=metrics_all.loc[metrics_all[&#39;CV&#39;].isin([&#34;CV_scores_Mean&#34;,
                                                  &#34;CV_scores_STD&#34;,
                                                  # &#34;scores_all&#34;
                                                  ]), :]
                                                                  # .sort_values(by=[&#39;CV&#39;,&#39;recall&#39;],
                                                                    # ascending=[True, False]
                                                                    # ).set_index([&#39;CV&#39;,&#39;model&#39;])
        print(&#39;models summary:\n&#39;,txt)
        print(&#34;-------------------------------------------&#34;)
    #metrics_all=metrics_all.reset_index().rename({&#39;level_0&#39;:&#39;model&#39;,&#39;level_1&#39;:&#39;CV&#39;},axis=1)
    #    idx = pd.IndexSlice
    #    metrics_all_summary=metrics_all.loc[idx[:, [&#34;CV_scores_Mean&#34;, &#34;CV_scores_STD&#34;]], :]
    if plot:
      ml_comparison_plot(# The above code is not doing anything. It is just a comment.
      metrics_all, outputFile=None)
    return metrics_all</code></pre>
</details>
</dd>
<dt id="ml_funcs.ml_comparison_plot"><code class="name flex">
<span>def <span class="ident">ml_comparison_plot</span></span>(<span>metrics_all, outputFile=None)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def ml_comparison_plot(metrics_all, outputFile=None):
    ##---plot comparison box plot
    df_tmp=metrics_all.loc[~metrics_all[&#39;CV&#39;].isin([&#34;CV_scores_Mean&#34;,
                                                    &#34;CV_scores_STD&#34;,
                                                    &#34;scores_all&#34;]
                                                  ),
                           :]

    if &#39;model&#39; in df_tmp.columns.tolist():
      df_tmp=df_tmp.drop(&#39;CV&#39;,axis=1)
      hue=id_vars=&#39;model&#39;
    else:
      id_vars=&#39;CV&#39;
      hue=&#39;scores&#39;
    ucols=[col for col in df_tmp if col not in [&#39;elapsed_time&#39;,&#39;Feature_nos&#39;]]
    df_long = pd.melt(df_tmp[ucols], id_vars=[id_vars], var_name=[&#39;scores&#39;])  
    # sns.set_style(&#34;darkgrid&#34;)
    plt.figure(figsize = (25,15))
    
    uplot   = sns.boxplot(x=&#34;scores&#34;,
                          y=&#34;value&#34;,
                          hue=hue,
                          data=df_long,
                          # orient=&#39;h&#39;,  ##it takes forever
                          showfliers=False,
                          ) 
    
    uplot.set_xticklabels(uplot.get_xticklabels(),rotation=90)
    uplot.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)
    uplot.grid()

    if outputFile is not None:
      # graphfile=os.path.join(outputFile,&#39;compare_models.png&#39;)
      print(&#34;plot save in %s&#34; %outputFile)
      plt.savefig(outputFile)
      plt.show()
      plt.close() </code></pre>
</details>
</dd>
<dt id="ml_funcs.ml_prediction"><code class="name flex">
<span>def <span class="ident">ml_prediction</span></span>(<span>ml_model, X, y, sk_fold, X_test=None, y_test=None, callbacks=None, verbose=False)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def ml_prediction(ml_model, 
                    X,
                    y,
                    sk_fold,  ##[X_val, y_val]
                    X_test=None,
                    y_test=None,
                    callbacks=None,
                    verbose=False,
                    ):
  from sklearn.pipeline import Pipeline
  from sklearn.model_selection import StratifiedKFold
  from sklearn.model_selection import TimeSeriesSplit
  from sklearn.base import is_classifier
    
  y_model=pd.DataFrame([])   
  df_epochs=pd.DataFrame([])  
  ml_models=[]

  umodel= ml_model[-1] if isinstance(ml_model, Pipeline) else ml_model
  model_name= umodel.__class__.__name__ 
  
  ##TODO:include all models with early_stopping
  
  early_stopping_rounds= ml_model.early_stopping_rounds if &#39;xgb&#39; in model_name.lower() else None
  if (X_test is not None) &amp; ((isinstance(sk_fold, StratifiedKFold))|(isinstance(sk_fold, TimeSeriesSplit))) &amp; (early_stopping_rounds is  None):
    print(&#34;Warning! Awkward senario. cross validaiton is on and predicition on a seprate test data set!&#34;)

  if sk_fold is None:
    print(&#34;Warning! training and validation data sets are the same&#34;)
    cv=zip([range(X.shape[0])], [range(X.shape[0])])

  elif isinstance(sk_fold, list):
    print(&#34;no cross validation &#34;)
    X_val=sk_fold[0]
    y_val=sk_fold[1]
    train_no=X.shape[0]
    X=pd.concat([X, X_val], axis=0)
    y=pd.concat([y, y_val], axis=0)
    cv=zip([range(train_no)],[range(train_no, X.shape[0])])

  else:
    cv= sk_fold.split(X,y) 

  for cv_itr,(train_index, val_index) in enumerate(cv):
    if verbose:
      print(f&#34;CV Itreation {cv_itr+1}&#34;) #{len(cv)}
    X_train, X_val = X.iloc[train_index,:], X.iloc[val_index,:] 
    y_train, y_val = y.iloc[train_index], y.iloc[val_index]

    ##TODO: it is only for xgboost, cover other ml_models
    if early_stopping_rounds is not None:
      ###NOTE: use [(X_train, y_train), (X_val, y_val)] instead of [(X_val, y_val)] to save epochs. [(X_val, y_val)] is much faster.
      eval_set = [(X_train, y_train), (X_val, y_val)]

      ml_model.fit(X_train,
                  y_train,
                  eval_set              = eval_set,
                  callbacks             = callbacks,
                  verbose               = 10
                  )
      df_epochs_tmp           = ml_prediction_sub_epochs(ml_model)
      df_epochs_tmp[&#39;CV_Iteration&#39;] = cv_itr
      df_epochs               = pd.concat([df_epochs,df_epochs_tmp],axis = 0)

      print(&#34;best_ntree=&#34;, ml_model.best_iteration, &#34;, best_score=&#34;, ml_model.best_score)

    else:
      ml_model.fit(X_train,
                  y_train,
                  )

    ml_models.append(ml_model)

    if X_test is not None: X_val=X_test
    if y_test is not None: y_val=y_test
    
    if is_classifier(umodel):
      y_model0 = pd.DataFrame(ml_model.predict_proba(X_val), index=y_val.index)
      y_model0=pd.concat([y_model0, y_model0.idxmax(axis=1).rename(&#39;y_pred&#39;), y_val.rename(&#39;y_true&#39;)], axis=1)
    else:
      y_model0 = pd.DataFrame(ml_model.predict(X_val), index=y_val.index)
      y_model0=pd.concat([y_model0,  y_val], axis=1)
      y_model0.columns=[&#39;y_pred&#39;,&#39;y_true&#39;]
      
    # print(y_model0)
    y_model0[&#39;CV_Iteration&#39;]= cv_itr
    y_model=pd.concat([y_model, y_model0],axis=0)
    
  # print(&#39;                    &#39;)

  if early_stopping_rounds is not None:
    df_epochs[&#39;best_ntree&#39;]=df_epochs[&#39;best_ntree&#39;]==df_epochs[&#39;epochs&#39;]
  else:
    df_epochs=None

  return y_model, ml_models, df_epochs</code></pre>
</details>
</dd>
<dt id="ml_funcs.ml_prediction_sub_epochs"><code class="name flex">
<span>def <span class="ident">ml_prediction_sub_epochs</span></span>(<span>model)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def ml_prediction_sub_epochs(model):
  results = model.evals_result()
  df_epochs=pd.DataFrame()
  for metric_key in results[&#39;validation_0&#39;].keys():
      val0=results[&#39;validation_0&#39;][metric_key]
      val1=results[&#39;validation_1&#39;][metric_key]
      tmp=pd.DataFrame([val0,val1],index=[f&#39;Train_{metric_key}&#39;,f&#39;Validation_{metric_key}&#39;]).T
      df_epochs=pd.concat([df_epochs, tmp], axis=1)
      
  df_epochs.index.name=&#39;epochs&#39; 
  df_epochs=df_epochs.reset_index()
  df_epochs[&#39;best_ntree&#39;]=model.best_iteration

  return df_epochs</code></pre>
</details>
</dd>
<dt id="ml_funcs.ml_prediction_xValNest"><code class="name flex">
<span>def <span class="ident">ml_prediction_xValNest</span></span>(<span>ml_model, X, y, outter_fold, inner_fold)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def ml_prediction_xValNest(ml_model,
                              X,
                              y,
                              outter_fold,
                              inner_fold,
                              ):
  from xgboost import XGBClassifier
  y_model=pd.DataFrame([])   
  df_epochs=pd.DataFrame([])   

  for cv_outter, (trainVal_index, tst_index) in enumerate(outter_fold.split(X, y)):
    X_trainVal, X_tst = X.iloc[trainVal_index,:], X.iloc[tst_index,:] 
    y_trainVal, y_tst = y.iloc[trainVal_index],   y.iloc[tst_index]

    for cv_itr, (train_index, val_index) in enumerate(inner_fold.split(X_trainVal, y_trainVal)):
      #print(&#34;Itreation &#34;,cv_itr)

      X_train, X_val = X_trainVal.iloc[train_index,:], X_trainVal.iloc[val_index,:] 
      y_train, y_val = y_trainVal.iloc[train_index],   y_trainVal.iloc[val_index]

      ##TODO: it is only for xgboost, cover other ml_models
      eval_set = [(X_train, y_train), (X_val, y_val)]

      ml_model.fit(X_train,
                  y_train,
                  eval_set=eval_set,
                  verbose=200
                  )
      df_epochs_tmp=ml_prediction_sub_epochs(ml_model)
      df_epochs_tmp[&#39;CV_Iteration&#39;]=f&#39;{cv_outter}_{cv_itr}&#39;
      df_epochs=pd.concat([df_epochs,df_epochs_tmp],axis=0)

      print(&#34;best_ntree=&#34;, ml_model.best_iteration)
      print(&#34;best_score=&#34;, ml_model.best_score)

      ml_model2=XGBClassifier(n_estimators=ml_model.best_iteration)
      
      ml_model2.fit(X_trainVal,
                  y_trainVal,
                  )
            
      y_model0 = pd.DataFrame(ml_model2.predict_proba(X_tst),index=X_tst.index)
    #   y_model0.rename(columns=map_lbls,inplace=True)
      y_model0 = pd.concat([y_model0,y_model0.idxmax(axis=1).rename(&#39;y_pred&#39;)],axis=1)
    
      y_model0[&#39;CV_Iteration&#39;]=f&#39;{cv_outter}_{cv_itr}&#39;
      y_model0[&#39;y_true&#39;]=y_tst
      
      y_model = pd.concat([y_model,y_model0],axis=0)

  #print(&#34;--------------------------------------------------------&#34;)

  df_epochs[&#39;best_ntree&#39;]=df_epochs[&#39;best_ntree&#39;]==df_epochs[&#39;epochs&#39;]
    
  return y_model, df_epochs</code></pre>
</details>
</dd>
<dt id="ml_funcs.ml_scores"><code class="name flex">
<span>def <span class="ident">ml_scores</span></span>(<span>y_model, scores_names)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def ml_scores(y_model, scores_names):
  
  if &#39;CV_Iteration&#39; not in y_model.columns:
    y_model[&#39;CV_Iteration&#39;]=&#39;All_data&#39;
     
  scores_all=pd.Series(index=scores_names, dtype=&#39;float64&#39;,name=&#39;scores_all&#39;)
  scores=pd.DataFrame(index=y_model[&#39;CV_Iteration&#39;].unique(), columns=scores_names)
  
  for con, score_name in enumerate(scores_names):
    ##TODO: it is not good practice to catch known error with try:
    try:
      umetric=metric_dict.get(score_name)

      if umetric in [&#39;auc_weighted&#39;, &#39;auc_micro&#39;, &#39;auc_macro&#39;]:
        scores_all.iloc[con]=umetric(y_model[&#39;y_true&#39;], y_model[&#39;y_pred&#39;], average=umetric.split(&#34;_&#34;)[1])
        scores.loc[:,score_name]=y_model.groupby(&#39;CV_Iteration&#39;).apply(lambda x:\
                                                                        pd.Series({score_name: umetric(x[&#39;y_true&#39;], x[&#39;y_pred&#39;], 
                                                                                                      average=umetric.split(&#34;_&#34;)[1])
                                                                                  })
                                                                      )
      else:
        scores_all.iloc[con]=umetric(y_model[&#39;y_true&#39;], y_model[&#39;y_pred&#39;])
        scores.loc[:,score_name]=y_model.groupby(&#39;CV_Iteration&#39;).apply(lambda x:\
                                                                        pd.Series({score_name: umetric(x[&#39;y_true&#39;], x[&#39;y_pred&#39;])})
                                                                      )
        
      # if umetric in [&#39;auc_weighted&#39;, &#39;auc_micro&#39;, &#39;auc_macro&#39;]:
      #   ufun=lambda x:umetric(x[&#39;y_true&#39;] , x[&#39;y_pred&#39;], umetric.split(&#34;)&#34;))
      # else:
      #   ufun=lambda x:umetric(x[&#39;y_true&#39;] , x[&#39;y_pred&#39;])

      # scores_all.iloc[con]=umetric(y_model[&#39;y_true&#39;],y_model[&#39;y_pred&#39;])
      # scores.loc[:,score_name]=y_model.groupby(&#39;CV_Iteration&#39;).apply(ufun)
    except Exception as e:
      print (f&#34;{score_name} wasn&#39;t added to scores data frame:\n &#34; , str(e))
      
  scores=pd.concat([scores,
                   pd.DataFrame(scores.mean(axis=0)).T.rename({0:&#39;CV_scores_Mean&#39;},axis=0),
                   pd.DataFrame(scores.std(axis=0)).T.rename({0:&#39;CV_scores_STD&#39;},axis=0),
                   pd.DataFrame(scores_all).T
                   ],
                   axis=0,
                  )
  scores=scores.reset_index().rename({&#39;index&#39;:&#39;CV&#39;}, axis=1)
  return scores</code></pre>
</details>
</dd>
<dt id="ml_funcs.ml_scores_crossvalidate"><code class="name flex">
<span>def <span class="ident">ml_scores_crossvalidate</span></span>(<span>**kwargs)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def ml_scores_crossvalidate(**kwargs):
    from sklearn.model_selection import cross_validate
    ##NOTE: you can&#39;t use cross_validate for early stopping
    ####scoring for cross_validate
    # scoring=[
    #         &#39;accuracy&#39;,
    #         &#39;roc_auc&#39;,
    #         &#39;recall&#39; ,
    #         &#39;f1&#39;,
    #         &#39;kappa&#39;,
    #         &#39;mcc&#39;,
    #         &#39;average_precision&#39;,
    #         &#39;balanced_accuracy&#39;,
    #         &#39;precision&#39;,
    #         ]
    # scoring_dict=dict(zip(scoring, scoring))
    # ## https://scikit-learn.org/stable/modules/model_evaluation.html#scoring
    # scoring_dict[&#39;mcc&#39;]=make_scorer(matthews_corrcoef)
    # scoring_dict[&#39;kappa&#39;]=make_scorer(cohen_kappa_score)

    # scoring_dict=dict(zip(scoring,[eval(&#34;make_scorer(&#34;+metric_dict.get(i)+&#34;)&#34;) for i in scoring])) 
    
    cv_results = cross_validate(**kwargs)
    cv_results=pd.DataFrame(cv_results)
    keys_to_remove = [&#39;fit_time&#39;, &#39;score_time&#39;]
    for key in keys_to_remove:
      del cv_results[key]
    cv_results=cv_results.append(cv_results.mean(axis=0).rename(&#34;CV_scores_Mean&#34;))
    cv_results=cv_results.append(cv_results.std(axis=0).rename(&#34;CV_scores_STD&#34;))
    # print(cv_results)
    return cv_results  </code></pre>
</details>
</dd>
<dt id="ml_funcs.ml_tuner"><code class="name flex">
<span>def <span class="ident">ml_tuner</span></span>(<span>trial, sk_model, model_params, X, y, sk_fold, var_in_model_params, Umetric='auc', use_early_Stopping=False, early_stopping_rounds=300, use_callbacks=False)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def ml_tuner(trial,
             sk_model,
             model_params,
             X,
             y,
             sk_fold,
             var_in_model_params,
             Umetric=&#39;auc&#39;,
             use_early_Stopping=False,
             early_stopping_rounds=300,
             use_callbacks=False
            ):
  import optuna


  local_vars=locals()

  ##TODO: it is not working:
  # if &#39;Pipeline&#39; in  str(type(sk_model)):
  #   model_sub = sk_model.steps[-1][1]
  #   sk_model.steps[-1][1]=model_sub(**model_params) 
  #   model = sk_model
  # else:
  #   model = sk_model(**model_params) 

  ##TODO: Revise logic:
  if model_params is not None:
    model = sk_model(**model_params) 
  ##Umetric is used when use_early_Stopping=True, otherwise it was infered from model_params(&#39;eval_metric&#39;)
    model_params={key:(eval(par, var_in_model_params, local_vars) if (isinstance(par, str) and (par[:14]==&#39;trial.suggest_&#39;)) else par) for (key, par) in model_params.items()}
    print(model_params)
  else:
    model=sk_model

  if use_early_Stopping:
    #eval_metric used by early stopping comes from xgboost package and there is not same as metrics in sklearn:
      #https://xgboost.readthedocs.io/en/stable/parameter.html
      #https://scikit-learn.org/stable/modules/classes.html 
    #for instance use recall metric doesnot exists in eval_metric. Moreover same metric have difference name:  aucpr in eval_metric is same as average_precision_score in metrics
    ##so there is need to used dictionary to convert  eval_metric to metric
    ##TODO: add all metric to it:
    eval_metric_dict={&#39;auc&#39;    : &#39;auc&#39;,
                      &#39;aucpr&#39;  : &#39;aucpr&#39;,
                      }
  
    Umetric=eval_metric_dict.get(model_params.get(&#39;eval_metric&#39;))
      
  if use_callbacks:
    # Each of ‘validation_0‘ and ‘validation_1‘ correspond to the order that datasets were provided to the eval_set argument in the call to fit(). 
    #eval_set = [(X_train, y_train), (X_val, y_val)]
    #it is much faster than [(X_train, y_train), (X_val, y_val)]
  
    #when len(eval_set)=2:
    observation_key=&#34;validation_1-&#34;+model_params[&#39;eval_metric&#39;] 
    #when len(eval_set)=1:
    # observation_key=&#34;validation_0-&#34;+model_params[&#39;eval_metric&#39;]
    
  # Add a callback for pruning.
    pruning_callback = [optuna.integration.XGBoostPruningCallback(trial, observation_key)]
  
  else:
    pruning_callback=None
    
  y_model,_=ml_prediction(model,
                              X,
                              y,
                              sk_fold,
                              use_early_Stopping=use_early_Stopping,
                              early_stopping_rounds=early_stopping_rounds,
                              pruning_callback=pruning_callback,
                              )

  scores=ml_scores(y_model, [Umetric])
  print(scores)
  scores=scores.loc[scores[&#39;CV&#39;]==&#39;CV_scores_Mean&#39;,Umetric]
  
  # scores=[]    
  # for cv_itr,(train_index, val_index) in enumerate(sk_fold.split(X,y)):
  #   model = sk_model(**model_params) 

  #   X_train, X_val = X.iloc[train_index,:], X.iloc[val_index,:] 
  #   y_train, y_val = y.iloc[train_index], y.iloc[val_index]
    
  #   ##TODO: it is only for xgboost, cover other mls
  #   if use_early_Stopping:
  #     eval_set = [(X_val, y_val)]

  #     if use_callbacks:
  #       # Each of ‘validation_0‘ and ‘validation_1‘ correspond to the order that datasets were provided to the eval_set argument in the call to fit(). 
  #       #eval_set = [(X_train, y_train), (X_val, y_val)]
  #       #### it is much faster than [(X_train, y_train), (X_val, y_val)]
        
  #       if len(eval_set)==2:
  #         observation_key=&#34;validation_1-&#34;+model_params[&#39;eval_metric&#39;]
  #       else:
  #         observation_key=&#34;validation_0-&#34;+model_params[&#39;eval_metric&#39;]

  #       # Add a callback for pruning.
  #       pruning_callback = [optuna.integration.XGBoostPruningCallback(trial, observation_key)]

  #       model.fit(X_train,
  #                 y_train,
  #                 early_stopping_rounds=early_stopping_rounds,
  #                 eval_set=eval_set,
  #                 callbacks=pruning_callback,
  #                 verbose=200
  #                 )
  #     else:
  #       model.fit(X_train,
  #                 y_train,
  #                 early_stopping_rounds=early_stopping_rounds,
  #                 eval_set=eval_set,
  #                 verbose=200
  #                 )   
      
  #     ##TODO: add all metric to it:
  #     #eval_metric used by early stopping comes from xgboost package and there is not same as metrics in sklearn:
  #       #https://xgboost.readthedocs.io/en/stable/parameter.html
  #       #https://scikit-learn.org/stable/modules/classes.html 
  #     #for instance use recall metric doesnot exists in eval_metric. Moreover same metric have difference name:  aucpr in eval_metric is same as average_precision_score in metrics
  #     ##so there is need to used dictionary to convert  eval_metric to metric

  #     eval_metric_dict={&#39;auc&#39;    : metrics.roc_auc_score,
  #                       &#39;aucpr&#39;  : metrics.average_precision_score,
  #                       &#39;logloss&#39;: metrics.accuracy_score
  #                      }

  #     Umetric_func=eval_metric_dict.get(model_params.get(&#39;eval_metric&#39;))
    
  #   else:
  #     model.fit(X_train,
  #               y_train,
  #               )
      
  #     Umetric_func=metric_dict.get(Umetric)
      
    # y_val_model = model.predict(X_val)
    # score=Umetric_func(y_val, y_val_model)  
    # scores.append(score)
    #scores=np.mean(scores)
  return scores</code></pre>
</details>
</dd>
<dt id="ml_funcs.pca_explainedVar"><code class="name flex">
<span>def <span class="ident">pca_explainedVar</span></span>(<span>pcaML)</span>
</code></dt>
<dd>
<div class="desc"><p>calcluate and plot Variance Explained VS number of features for PCA</p>
<h2 id="todo-add-screeplot">TODO: add screeplot</h2>
<h2 id="parameters">Parameters:</h2>
<p>pcaML (float): Percentage of variance explained by each of the selected components.</p>
<p>outputFile (string):
the location of the plot</p>
<h2 id="returns">returns:</h2>
<p>var
(float)
cumulative varaince explained</p>
<hr>
<p>Author: Reza Nourzadeh</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def pca_explainedVar(pcaML):
    &#34;&#34;&#34; calcluate and plot Variance Explained VS number of features for PCA
    ##TODO: add screeplot
    Parameters:
    ----------
    pcaML (float): Percentage of variance explained by each of the selected components.

    outputFile (string):
    the location of the plot

    returns:
    -------
    var  (float)
    cumulative varaince explained

    -------
    Author: Reza Nourzadeh 
    &#34;&#34;&#34;
    
    eigen_values=pcaML.explained_variance_

    np.round(
            pcaML.explained_variance_ratio_,
            decimals=3)

    explained_var = np.cumsum(np.round(pcaML.explained_variance_ratio_,decimals=3) * 100)

    plt.ylabel(&#39;% explained_variance Explained&#39;)
    plt.xlabel(&#39;# of Features&#39;)
    plt.title(&#39;PCA Analysis&#39;)

    plt.ylim(0, 100)
    plt.style.context(&#39;seaborn-whitegrid&#39;)
    plt.grid()
    plt.plot(explained_var)

    return explained_var,eigen_values</code></pre>
</details>
</dd>
<dt id="ml_funcs.pca_important_features"><code class="name flex">
<span>def <span class="ident">pca_important_features</span></span>(<span>transformed_features, components_, columns)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def pca_important_features(transformed_features, components_, columns):
    import math
        ##TODO: check it and make a function
    ###http://benalexkeen.com/principle-component-analysis-in-python/    
    &#34;&#34;&#34;
    This function will return the most &#34;important&#34; 
    features so we can determine which have the most
    effect on multi-dimensional scaling
    &#34;&#34;&#34;
    num_columns = len(columns)

    # Scale the principal components by the max value in
    # the transformed set belonging to that component
    xvector = components_[0] * max(transformed_features[:,0])
    yvector = components_[1] * max(transformed_features[:,1])

    # Sort each column by it&#39;s length. These are your *original*
    # columns, not the principal components.
    important_features = { columns[i] : math.sqrt(xvector[i]**2 + yvector[i]**2) for i in range(num_columns) }
    # important_features = sorted(zip(important_features.values(), important_features.keys()), reverse=True)
    important_features = pd.Series(important_features)
    important_features = important_features.sort_values(ascending=[False])
    return important_features</code></pre>
</details>
</dd>
<dt id="ml_funcs.pca_ortho_rotation"><code class="name flex">
<span>def <span class="ident">pca_ortho_rotation</span></span>(<span>lam, method='varimax', gamma=None, eps=1e-06, itermax=100)</span>
</code></dt>
<dd>
<div class="desc"><h2 id="todo-document-it">TODO: document it</h2>
<h2 id="a-varimax-rotation-is-a-change-of-coordinates-used-in-principal-component-analysis1-pca-that-maximizes-the-sum-of-the-variances-of-the-squared-loadings">A VARIMAX rotation is a change of coordinates used in principal component analysis1 (PCA) that maximizes the sum of the variances of the squared loadings</h2>
<h2 id="httpsgithubcomrossfadelyconsommeblobmasterconsommerotate_factorpy"><a href="https://github.com/rossfadely/consomme/blob/master/consomme/rotate_factor.py">https://github.com/rossfadely/consomme/blob/master/consomme/rotate_factor.py</a></h2>
<p>Return orthogal rotation matrix
TODO: - other types beyond</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def pca_ortho_rotation(lam,
                   method  = &#39;varimax&#39;,
                   gamma   = None,
                   eps     = 1e-6,
                   itermax = 100
                   ):
    &#34;&#34;&#34;
    ##TODO: document it 
    ## A VARIMAX rotation is a change of coordinates used in principal component analysis1 (PCA) that maximizes the sum of the variances of the squared loadings
    ## https://github.com/rossfadely/consomme/blob/master/consomme/rotate_factor.py
    Return orthogal rotation matrix
    TODO: - other types beyond 
    &#34;&#34;&#34;
    if gamma == None:
        if (method == &#39;varimax&#39;):
            gamma = 1.0
        if (method == &#39;quartimax&#39;):
            gamma = 0.0

    nrow, ncol = lam.shape
    R = np.eye(ncol)
    var = 0

    for i in range(itermax):
        lam_rot = np.dot(lam, R)
        tmp     = np.diag(np.sum(lam_rot ** 2, axis = 0)) / nrow * gamma
        u, s, v = np.linalg.svd(np.dot(lam.T, lam_rot ** 3 - np.dot(lam_rot, tmp)))
        R       = np.dot(u, v)
        var_new = np.sum(s)
        if var_new &lt; var * (1 + eps):
            break
        var = var_new

    return R</code></pre>
</details>
</dd>
<dt id="ml_funcs.pdp_plot_batch"><code class="name flex">
<span>def <span class="ident">pdp_plot_batch</span></span>(<span>X, umodel, sel_features)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def pdp_plot_batch(X, umodel, sel_features):
    # print(&#39;The scikit-learn version is {}.&#39;.format(sklearn.__version__))
    from sklearn.inspection import PartialDependenceDisplay

    ###see https://scikit-learn.org/stable/auto_examples/inspection/plot_partial_dependence.html#sphx-glr-auto-examples-inspection-plot-partial-dependence-py
    print(
        &#34;Computing partial dependence plots and individual conditional expectation...&#34;
    )

    _, ax = plt.subplots(
        # ncols=3, nrows=math.ceil(len(sel_features)/3),
        figsize=(30, 30),
        sharey=False,
        constrained_layout=True,
    )

    features_info = {
        &#34;features&#34;: sel_features,
        &#34;kind&#34;: &#34;both&#34;,
        &#34;centered&#34;: True,
        # &#34;categorical_features&#34;: [&#39;plant_K1&#39;],
    }
    display = PartialDependenceDisplay.from_estimator(
        umodel,
        X,
        **features_info,
        ax=ax,
        # **common_params,
    )</code></pre>
</details>
</dd>
<dt id="ml_funcs.plot_confusion_matrix2"><code class="name flex">
<span>def <span class="ident">plot_confusion_matrix2</span></span>(<span>y_model, map_lbls, outputFile=None)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def plot_confusion_matrix2(y_model, map_lbls, outputFile=None):
  ##y_model=pd.concat([y_true, y_pred],axis=1)
  
  from sklearn.metrics import confusion_matrix
  y_model1=y_model.copy()
  
  if (&#39;CV_Iteration&#39; in y_model1.columns)&amp;(y_model1[&#39;CV_Iteration&#39;].nunique()!=1) :
    #if (y_model1[&#39;CV_Iteration&#39;].nunique()!=1):
    y_model1[&#39;CV_Iteration&#39;]=&#39;cv_&#39;+y_model1[&#39;CV_Iteration&#39;].astype(str)
    y_model_all=y_model.copy()
    y_model_all[&#39;CV_Iteration&#39;]=&#39;All_data&#39;
    y_model1=pd.concat([y_model1,y_model_all],axis=0)
    ncol=3
    fig_size=(30,20)
  else:
    y_model1[&#39;CV_Iteration&#39;]=&#39;All_data&#39;
    ncol=1
    fig_size=(10,5)

  print(y_model1)
  confMats=pd.Series([])
  #confMats=pd.Series([],index=y_model[&#39;CV_Iteration&#39;].unique())    
  
  fig, axs = plt.subplots(math.ceil(y_model1[&#39;CV_Iteration&#39;].nunique()/ncol), ncol, figsize=fig_size)   
  axs=np.array([axs]) if ncol==1 else axs

  for cont, (cv, y_model_sub) in  enumerate(y_model1.groupby([&#39;CV_Iteration&#39;])):  
    print(cont,cv)
    y_true=y_model_sub[&#39;y_true&#39;]
    y_pred=y_model_sub[&#39;y_pred&#39;]

    confMat = pd.DataFrame(confusion_matrix(y_true, y_pred))
    confMat=confMat.rename(columns=map_lbls).rename(map_lbls,axis=1).rename(map_lbls,axis=0)
    confMat.index.name=&#39;True label&#39;
    confMat.columns.name=&#39;Predicted label&#39;

    confMats[cv]=confMat
    uPlot=sns.heatmap(ax=axs.flatten()[cont],
                    data=confMat,
                    annot=True,
                    cmap=&#34;YlGnBu&#34;,
                    fmt=&#34;g&#34;
                   )

    axs.flatten()[cont].set_title(f&#39;{cv}&#39;)    
    
  if outputFile is not None:
    figure = uPlot.get_figure()
    figure.savefig(outputFile, bbox_inches=&#39;tight&#39;)
    plt.close(&#39;all&#39;)
    
  return confMats</code></pre>
</details>
</dd>
<dt id="ml_funcs.precision_recall_curve2"><code class="name flex">
<span>def <span class="ident">precision_recall_curve2</span></span>(<span>y, model_prob, pos_label, outputFile=None, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def precision_recall_curve2(y, model_prob, pos_label, outputFile=None, **kwargs):   
    from sklearn.metrics import precision_recall_curve, auc
    
    model_precision, model_recall, thresholds = precision_recall_curve(y_true=y, probas_pred=model_prob, pos_label=pos_label, **kwargs)
    model_auc_rp = auc(model_recall, model_precision)

    tmpTxt=&#39;ROC of precision recall curve=%.3f&#39; % (model_auc_rp)
    ### plot the precision-recall curves
    fig, ax = plt.subplots(figsize=(20, 10))

    df_rp=pd.DataFrame([model_precision[:-1],
                      model_recall[:-1],
                      thresholds], index=[&#39;Precision&#39;, &#39;Recall&#39;, &#39;thresholds&#39;]).T
    df_rp[&#39;style&#39;]=1
    # df_rp2=pd.melt(df_rp, id_vars=&#39;recall&#39;, value_vars=[&#39;Precision&#39;, &#39;thresholds&#39;],  var_name=&#39;precision_thresholds&#39;)
    # print(df_rp2)

    uPlot2=sns.lineplot(data=df_rp,
                        ax=ax,
                        y=&#39;Precision&#39;,
                        x=&#39;Recall&#39;,
                        # hue=&#39;precision_thresholds&#39;,
                        markers=True,
                        style=&#34;style&#34;,
                        # palette=[&#34;red&#34;],
                        alpha=0.1,
                        )
    plt.legend([],[], frameon=False)
    ax.set_title(&#39;Precision Recall Curve&#39;)
    no_skill = len(y[y==1]) / len(y)
    # ax.set_ylim(0, 1.1)

    df_rp_tmp=df_rp.drop_duplicates(subset=[&#39;Recall&#39;]).reset_index()
    interval_no=min(15,df_rp_tmp.shape[0])
    idx=list(np.linspace(df_rp_tmp.index.min(),df_rp_tmp.index.max(),interval_no,endpoint=True,dtype=&#39;int&#39;))
    plt.xticks(df_rp_tmp.iloc[idx][&#39;Recall&#39;])
    ticks_loc = ax.get_xticks().tolist()
    threshs=(df_rp_tmp.iloc[idx][&#39;thresholds&#39;].round(3).astype(str))  ### or:df_rp.loc[df_rp[&#39;Recall&#39;].isin(ticks_loc),&#39;thresholds&#39;]

    ax.set_xticks(ax.get_xticks().tolist())
    ax.set_xticklabels([str(round(x,2))+&#34;(t=&#34;+y+&#34;)&#34; for x,y in zip(ticks_loc,threshs)])
    ax.set(xlabel=&#39;Recall/(threshold)&#39;)
    plt.plot([0, 1], [no_skill, no_skill], linestyle=&#39;--&#39;, label=&#39;No Skill&#39;, color=&#39;black&#39;)
    # plt.plot([0, 0], [no_skill, no_skill], linestyle=&#39;--&#39;, label=&#39;No Skill&#39;, color=&#39;red&#39;,alpha=1)
    ax.text(.9, no_skill, f&#39;No skill line&#39;, color=&#39;black&#39;, fontsize=10)

    plt.xticks(rotation=90)
    ax.annotate(&#39;ROC of Precision Recall curve=%.3f&#39; % (model_auc_rp),
                xy=(.4, 0), xycoords=&#39;axes fraction&#39;,
                xytext=(-20, 25),
                textcoords=&#39;offset pixels&#39;,
                horizontalalignment=&#39;right&#39;,
                verticalalignment=&#39;bottom&#39;,
                fontsize=10)

    # labels = ax.get_xticklabels()
    # print(labels)
    # labels=[x.get_text()+&#34;(&#34;+y+&#34;)&#34; for x,y in zip(labels,list(df_rp[&#39;x_label&#39;]))]
    # ax.set_xticklabels(labels)

    # ax2 = ax.twinx()
    # ax2.set(ylim=(df_rp[&#39;thresholds&#39;].min(),
    #               df_rp[&#39;thresholds&#39;].max()))

    # ax3 = ax.twiny()
    # ax3.set(ylim=(df_rp[&#39;thresholds&#39;].min(),
    #               df_rp[&#39;thresholds&#39;].max()))

    # uPlot2=sns.lineplot(data=df_rp,
    #                     ax=ax2,
    #                     y=&#39;thresholds&#39;,
    #                     x=&#39;precision&#39;,
    #                     markers=True
    #                    )

    if outputFile is not None:
      plt.savefig(outputFile, bbox_inches=&#39;tight&#39; ,dpi=300)
      plt.show()
      # plt.close(&#39;all&#39;)

    return df_rp.drop([&#39;style&#39;],axis=1), idx </code></pre>
</details>
</dd>
<dt id="ml_funcs.regressors_template"><code class="name flex">
<span>def <span class="ident">regressors_template</span></span>(<span>random_state=10)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def regressors_template(random_state=10):
  from sklearn.tree import DecisionTreeRegressor
  from sklearn.svm import SVR, LinearSVR, NuSVR
  from sklearn.gaussian_process import GaussianProcessRegressor
  from sklearn.gaussian_process.kernels import RBF

  from xgboost import XGBRegressor
  from sklearn.ensemble import AdaBoostRegressor, RandomForestRegressor, BaggingRegressor
  from lightgbm import LGBMRegressor

  from sklearn.linear_model import LinearRegression, Ridge, SGDRegressor, ElasticNet, ElasticNetCV, Lars, Lasso, LassoLars, LassoLarsIC, LassoCV

  from sklearn.naive_bayes import GaussianNB

  from sklearn.neighbors import KNeighborsRegressor

  from sklearn.neural_network import MLPRegressor  
  from sklearn.ensemble import GradientBoostingRegressor

  from sklearn.pipeline import Pipeline
  import numpy as np
  from sklearn.preprocessing import StandardScaler
  from sklearn.decomposition import PCA
  from sklearn.impute import SimpleImputer

  Regressors={
  &#34;Nearest_Neighbors_2&#34;:          KNeighborsRegressor(2),
  &#34;Nearest_Neighbors_3&#34;:          KNeighborsRegressor(3),
  &#34;Nearest_Neighbors_4&#34;:          KNeighborsRegressor(4),
  &#34;Nearest_Neighbors_5&#34;:          KNeighborsRegressor(5),

  &#34;Decision_Tree_depth5&#34;:         DecisionTreeRegressor(max_depth=5, random_state=random_state),
  &#34;Decision_Tree_depth10&#34;:        DecisionTreeRegressor(max_depth=10, random_state=random_state),

  &#34;Naive_Bayes&#34;:                  GaussianNB(),

  &#39;LinearRegression&#39; :            LinearRegression(),
  &#34;Ridge&#34;        :                Ridge(random_state=random_state)        ,
  &#34;SGDRegressor&#34; :                SGDRegressor(random_state=random_state) ,
  &#34;ElasticNet&#34;   :                ElasticNet(random_state=random_state)   ,
  &#34;ElasticNetCV&#34; :                ElasticNetCV(random_state=random_state) ,
  &#34;Lars&#34;         :                Lars(random_state=random_state)         ,
  &#34;Lasso&#34;        :                Lasso()        ,
  &#34;LassoCV&#34;      :                LassoCV(random_state=random_state)    ,
  &#34;LassoLars&#34;    :                LassoLars(random_state=random_state)    ,
  &#34;LassoLarsIC&#34;  :                LassoLarsIC()  ,
  
  &#39;RandomForest_model1&#39;:          RandomForestRegressor(random_state=random_state),
  &#39;RandomForest_model_n200&#39;:      RandomForestRegressor(n_estimators=200, random_state=random_state),
  &#39;RandomForest_model_n300&#39;:      RandomForestRegressor(n_estimators=300, random_state=random_state),
  &#39;Xgboost_n200&#39; :                XGBRegressor(n_estimators=200, random_state=random_state),
  &#39;Xgboost_n200_dp10&#39;:            XGBRegressor(n_estimators=200, max_depth=10, random_state=random_state) ,

  &#39;LightGBM&#39;:                     LGBMRegressor(random_state=random_state),
  &#39;LightGBM_n200&#39;:                LGBMRegressor(n_estimators=200, random_state=random_state),
  &#39;LightGBM_n400&#39;:                LGBMRegressor(n_estimators=300, random_state=random_state),
  &#39;LightGBM_n1000&#39;:               LGBMRegressor(n_estimators=1000, random_state=random_state),
  &#39;LightGBM_n200_dp10&#39;:           LGBMRegressor(max_depth=10,  n_estimators=200, random_state=random_state),
  &#39;LightGBM_n300_dp10&#39;:           LGBMRegressor(max_depth=10,  n_estimators=300, random_state=random_state),

  &#39;MLPRegressor1&#39;   :             MLPRegressor(alpha=1, max_iter=1000, random_state=random_state),
  &#39;MLPRegressor_early_stopping&#39;:  MLPRegressor(alpha=1, max_iter=1000, early_stopping=True, random_state=random_state),
  &#39;MLPRegressor3&#39;   :             MLPRegressor(alpha=1, max_iter=1000, solver=&#39;sgd&#39;, early_stopping=True, random_state=random_state),

  # &#34;Linear_SVM&#34;:                  SVR(kernel=&#34;linear&#34;, C=0.025, probability=True, random_state=random_state),
  # &#34;RBF_SVM&#34;:                     SVR(kernel=&#34;rbf&#34;, C=0.025, probability=True, random_state=random_state),
  # &#34;NuSVR&#34;:                       NuSVR(probability=True, random_state=random_state),

  &#34;AdaBoost&#34;:                     AdaBoostRegressor(random_state=random_state),
  # &#39;bagging&#39;:                      BaggingRegressor(estimator=DecisionTreeRegressor(), n_estimators=10, random_state=random_state),

  }

  basic_params = {&#34;random_state&#34;:random_state}

  regressors2={}
  for name, Regressor in Regressors.items():
    ##TODO: find a way to add randomstate here
    # params={**basic_params,**Regressor.get_params()}
    # print(params)
    if any([x in name for x in [&#39;pca&#39;,&#39;DiscriminantAnalysis&#39;]]):  
        regressors2[name]= Pipeline(steps=[(&#34;imputer&#34;, SimpleImputer(strategy=&#34;median&#34;)),
                                            (&#34;scaler&#34;, StandardScaler()),
                                            (&#34;reduce_dims&#34;, PCA(n_components=20)),
                                            (name, Regressor)])
        
    elif (&#39;xgb&#39; not in name.lower()) &amp; (&#39;gbm&#39; not in name.lower()) :
        regressors2[name]= Pipeline(steps=[(&#34;imputer&#34;, SimpleImputer(strategy=&#34;median&#34;)),
                                            (&#34;scaler&#34;, StandardScaler()),
                                            (name, Regressor)])
    else:
        regressors2[name]=Regressor
  return regressors2</code></pre>
</details>
</dd>
<dt id="ml_funcs.reliability_diagram"><code class="name flex">
<span>def <span class="ident">reliability_diagram</span></span>(<span>y, model_prob, pos_label, outputFile, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def reliability_diagram(y, model_prob, pos_label, outputFile, **kwargs):
    from sklearn.calibration import calibration_curve
    prob_true, prob_pred= calibration_curve(y_true=y, y_prob=model_prob, n_bins=50, normalize=False, **kwargs)  # pos_label=pos_label, 
    prob_true_norm, prob_pred_norm= calibration_curve(y_true=y, y_prob=model_prob,  n_bins=50,normalize=True, **kwargs)  # pos_label=pos_label, 

    fig, ax = plt.subplots(figsize=(20, 10))
    plt.plot([0,1],[0,1]) 
    plt.plot(prob_pred_norm, prob_true_norm, label=&#39;Normlized&#39;) 
    plt.plot(prob_pred, prob_true, label=&#39;Original&#39;) 
    plt.grid() 
    plt.xlabel(&#34;Average probability&#34;)
    plt.ylabel(&#34;Fraction of positive&#34;)
    plt.title(&#34;Reliability diagram&#34;) 
    ax.legend(loc=&#39;upper right&#39;, frameon=True)
    
    if outputFile is not None:
      plt.savefig(outputFile, bbox_inches=&#39;tight&#39;)
      plt.close(&#39;all&#39;)
      
    return prob_true, prob_pred, prob_true_norm, prob_pred_norm</code></pre>
</details>
</dd>
<dt id="ml_funcs.roc_curve2"><code class="name flex">
<span>def <span class="ident">roc_curve2</span></span>(<span>y, model_prob, pos_label, outputFile, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def roc_curve2(y, model_prob, pos_label, outputFile, **kwargs):
    ##TODO: add **kwargs to roc_curve, after sepration augs
    from sklearn.metrics import roc_auc_score

    model_auc = roc_auc_score(y_true=y, y_score=model_prob, **kwargs)
    ## NOTE:    Different result with roc_auc_score() and auc()
    # : https://stackoverflow.com/questions/31159157/different-result-with-roc-auc-score-and-auc
    # from sklearn.metrics import roc_curve,auc
    # model_fpr, model_tpr, _ = roc_curve(y.map(map_lbls_inv), model_prob)
    # model_auc2 = auc(model_fpr, model_tpr)

    tmpTxt=&#39;ROC AUC=%.3f (random selection=.5)&#39; % (model_auc)
#     print(tmpTxt+&#39;\n&#39;,&#39;green&#39;)
    from sklearn.metrics import roc_curve
    model_fpr, model_tpr, thresholds = roc_curve(y_true=y, y_score=model_prob, pos_label=pos_label)

    thresholds[0]=1
    df_roc=pd.DataFrame([model_fpr,
                      model_tpr,
                      thresholds], index=[&#39;False_Positive_Rate&#39;, &#39;True_Positive_Rate&#39;, &#39;thresholds&#39;]).T

    fig, ax = plt.subplots(figsize=(20, 10))
    plt.plot([0,1],[0,1], linestyle=&#39;--&#39;, label=&#39;No Skill&#39;)    ###or ns_fpr, ns_tpr =[0,1],[0,1]= roc_curve(y, [0 for _ in range(len(y))], pos_label)
    plt.plot(model_fpr, model_tpr, marker=&#39;.&#39;, label=&#39;Model&#39;)
    plt.xlabel(&#39;False Positive Rate&#39;)
    plt.ylabel(&#39;True Positive Rate&#39;)
    plt.title(&#39;ROC curve&#39;)
    ax.legend(loc=&#39;upper right&#39;, frameon=True)
    ax.annotate(&#39;ROC AUC=%.3f (random selection=.5)&#39; % (model_auc),
                xy=(1, 0), xycoords=&#39;axes fraction&#39;,
                xytext=(-20, 20),
                textcoords=&#39;offset pixels&#39;,
                horizontalalignment=&#39;right&#39;,
                verticalalignment=&#39;bottom&#39;)

    df_roc_tmp=df_roc.drop_duplicates(subset=[&#39;False_Positive_Rate&#39;]).reset_index()
    interval_no=min(15,df_roc_tmp.shape[0])
    idx=list(np.linspace(df_roc_tmp.index.min(),df_roc_tmp.index.max(),interval_no,endpoint=True,dtype=&#39;int&#39;))
    plt.xticks(df_roc_tmp.iloc[idx][&#39;False_Positive_Rate&#39;])
    ticks_loc = ax.get_xticks().tolist()
    threshs=(df_roc_tmp.iloc[idx][&#39;thresholds&#39;].round(3).astype(str)) 

    ax.set_xticks(ax.get_xticks().tolist())
    ax.set_xticklabels([str(round(x,2))+&#34;(t=&#34;+y+&#34;)&#34; for x,y in zip(ticks_loc,threshs)])
    plt.xticks(rotation=90)

    if outputFile is not None:
      plt.savefig(outputFile, bbox_inches=&#39;tight&#39;)
      plt.close(&#39;all&#39;)

    return df_roc, model_auc</code></pre>
</details>
</dd>
<dt id="ml_funcs.shap_plots_batch"><code class="name flex">
<span>def <span class="ident">shap_plots_batch</span></span>(<span>X, y, umodel, random_state=100)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def shap_plots_batch(X, y, umodel, random_state=100):
    import shap
    from sklearn.model_selection import RandomizedSearchCV, train_test_split

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=random_state
    )
    # X_train, X_test= X, X

    ###https://www.kaggle.com/code/hwwang98/shapley-value-feature-research#kln-19:
    # rather than use the whole training set to estimate expected values, we summarize with
    # a set of weighted kmeans, each weighted by the number of points they represent.
    X_train_summary = shap.kmeans(X_train, 10)
    ###TODO: why it is working only in xgboost:
    # explainer = shap.Explainer(umodel, X)
    # shap_values = explainer(X)
    # # shap.plots.beeswarm(shap_values)
    # shap.plots.bar(shap_values)
    # shap.summary_plot(shap_values, plot_type=&#39;violin&#39;)

    explainer = shap.KernelExplainer(
        model=umodel.predict,
        data=X_train_summary,
        # link=&#34;identity&#34;
    )
    shap_values = explainer.shap_values(
        X_test,
        #  nsamples=1000
    )

    # shap.plots.bar(shap_values)##not working
    shap.summary_plot(shap_values, X_test, plot_type=&#34;violin&#34;)
    # shap.force_plot(explainer.expected_value, shap_values, X_test) ##not working
    # shap.dependence_plot(&#34;NOH&#34;, shap_values, X_test)##not working

    return shap_values

####------------------------------Xgboost HyperParameter tuning----------------------------------------------------------------------------------------
##-----------------------------------------------------------------------------------------------------------------------------------------------
##-----------------------------------------------------------------------------------------------------------------------------------------------
##-----------------------------------------------------------------------------------------------------------------------------------------------
# When working with imbalanced data sets, accuracy is not always the best metric to evaluate the performance of a classifier, because it can be misleading. Some alternative metrics that are better suited for imbalanced data sets are:

# Precision: the proportion of true positive predictions among all positive predictions. It measures the ability of the classifier to avoid false positives.
# Recall (Sensitivity or TPR): the proportion of true positive predictions among all actual positive observations. It measures the ability of the classifier to detect all positive observations.
# F1-score: the harmonic mean of precision and recall. It is a balance between precision and recall.
# AUC-ROC: the area under the Receiver Operating Characteristic curve. It measures the ability of the classifier to distinguish between positive and negative observations.
# AUC-PR: the area under the precision-recall curve. It also measures the ability of the classifier to distinguish between positive and negative observations, but it puts more emphasis on the true positive rate.
# G-mean: the geometric mean of recall and specificity. It is a balance between recall and specificity and is sensitive to imbalanced data.
# These metrics can be calculated using the following sklearn functions:

# Precision: sklearn.metrics.precision_score(y_true, y_pred)
# Recall: sklearn.metrics.recall_score(y_true, y_pred)
# F1-score: sklearn.metrics.f1_score(y_true, y_pred)
# AUC-ROC: sklearn.metrics.roc_auc_score(y_true, y_score)
# AUC-PR: sklearn.metrics.average_precision_score(y_true, y_score) or use    precision, recall, _ = precision_recall_curve(y_test, y_pred) and AUC_PR = auc(recall, precision)
# G-mean: sklearn.metrics.geometric_mean_score(y_true, y_pred)


### NOTE: xgboost parameters: https://xgboost.readthedocs.io/en/latest/parameter.html
# https://scikit-learn.org/stable/modules/generated/sklearn.metrics.average_precision_score.html#sklearn.metrics.average_precision_score5 

### NOTE: Handle Imbalanced Dataset:
### https://xgboost.readthedocs.io/en/latest/tutorials/param_tuning.html:
### For common cases such as ads clickthrough log, the dataset is extremely imbalanced. This can affect the training of XGBoost ucase, and there are two ways to improve it:
### If you care only about the overall performance metric (AUC) of your prediction:
    ### Balance the positive and negative weights via scale_pos_weight
    ### Use AUC for evaluation
### If you care about predicting the right probability:
    ### In such a case, you cannot re-balance the dataset
    ### Set parameter max_delta_step to a finite number (say 1) to help convergence
## AUPRC and Average Precision:
###https://glassboxmedicine.com/2019/03/02/measuring-performance-auprc/
## The baseline of AUPRC is equal to the fraction of positives. If a dataset consists of 8% cancer examples and 92% healthy examples, the baseline AUPRC is 0.08, so obtaining an AUPRC of 0.40 in this scenario is good! AUPRC is most useful when you care a lot about your model handling the positive examples correctly.

###using scale_pos_weight and sample_weight  leads to almost the same auc:
# when  &#34;tree_method&#34;:&#34;gpu_hist&#34; and &#34;predictor&#34;:&#34;gpu_predictor&#34; :  (0.842775:0.842165)
# without using gpu results                                      :  (0.842609:0.842686)

# The hyperparameters that have the greatest effect on XGBoost objective metrics are: alpha, min_child_weight, subsample, eta, and num_round.

# Fill reasonable values for key inputs:
# learning_rate: 0.01
# n_estimators: 100 if the size of your data is high, 1000 is if it is medium-low
# max_depth: 3
# subsample: 0.8
# colsample_bytree: 1
# gamma: 1

###  I usually use 50 rounds for early stopping with 1000 trees in the model. I’ve seen in many places recommendation to use about 10% of total number of trees for early stopping
### early_stopping_rounds offers a way to automatically find the ideal value for n_estimators. Early stopping causes the model to stop iterating when the validation score stops improving, even if we aren&#39;t at the hard stop for n_estimators. It&#39;s smart to set a high value for n_estimators and then use early_stopping_rounds to find the optimal time to stop iterating.

##xgboost GPU parameter:
            ##NOte: Gpu is very fast, but it doesnot give good model performance like cpu, so tune your model with gpu and use cpu to generate final prediction on CPU 
#             &#34;tree_method&#34;:&#34;gpu_hist&#34;,
            ## &#34;gpu_id&#34;:1,</code></pre>
</details>
</dd>
<dt id="ml_funcs.xgb_tuner"><code class="name flex">
<span>def <span class="ident">xgb_tuner</span></span>(<span>X_train, y_train, X_test, y_test, random_state, metric=&lt;function roc_auc_score&gt;, stepWise=True)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def xgb_tuner(X_train, y_train,
              X_test,  y_test ,
              random_state,
              metric=roc_auc_score,
              stepWise=True):
  import xgboost as xgb 
  from sklearn.metrics import accuracy_score, roc_auc_score, average_precision_score

  from hyperopt import fmin, tpe, hp, Trials, STATUS_OK
  params = {&#39;random_state&#39;: random_state}
  rounds = [{&#39;max_depth&#39;      : hp.quniform(&#39;max_depth&#39;, 1, 8, 1), # tree
            &#39;min_child_weight&#39;: hp.loguniform(&#39;min_child_weight&#39;, -2, 3)},

            {&#39;subsample&#39;      : hp.uniform(&#39;subsample&#39;, .5, 1), # stochastic
            &#39;colsample_bytree&#39;: hp.uniform(&#39;colsample_bytree&#39;, .5, 1)},

            {&#39;reg_alpha&#39;      : hp.uniform(&#39;reg_alpha&#39;, 0, 10),
            &#39;reg_lambda&#39;     : hp.uniform(&#39;reg_lambda&#39;, 1, 10),},
            
            {&#39;gamma&#39;          : hp.loguniform(&#39;gamma&#39;, -10, 10)}, # regularization
            {&#39;learning_rate&#39;  : hp.loguniform(&#39;learning_rate&#39;, -7, 0)} # boosting
            ]
  if not stepWise:
    rounds = [{&#39;max_depth&#39;      : hp.quniform(&#39;max_depth&#39;, 1, 8, 1), # tree
            &#39;min_child_weight&#39;: hp.loguniform(&#39;min_child_weight&#39;, -2, 3),

            &#39;subsample&#39;      : hp.uniform(&#39;subsample&#39;, .5, 1), # stochastic
            &#39;colsample_bytree&#39;: hp.uniform(&#39;colsample_bytree&#39;, .5, 1),

            &#39;reg_alpha&#39;      : hp.uniform(&#39;reg_alpha&#39;, 0, 10),
            &#39;reg_lambda&#39;     : hp.uniform(&#39;reg_lambda&#39;, 1, 10),
            
            &#39;gamma&#39;          : hp.loguniform(&#39;gamma&#39;, -10, 10), # regularization
            &#39;learning_rate&#39;  : hp.loguniform(&#39;learning_rate&#39;, -7, 0)} # boosting
            ]
  for round in rounds:
    params = {**params, **round}
    trials = Trials()
    best = fmin(fn=lambda space: hyperparameter_tuning(space,
                                                        X_train,y_train,
                                                        X_test, y_test,
                                                        metric=metric),
                space=params,
                algo=tpe.suggest,
                max_evals=200 if stepWise else 1500,
                trials=trials,
                )
  params = {**params, **best}

  params[&#39;max_depth&#39;]=int(params[&#39;max_depth&#39;])

  return params, trials</code></pre>
</details>
</dd>
</dl>
</section>
<section>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="ml_funcs.class_weight2" href="#ml_funcs.class_weight2">class_weight2</a></code></li>
<li><code><a title="ml_funcs.classifer_performance_batch" href="#ml_funcs.classifer_performance_batch">classifer_performance_batch</a></code></li>
<li><code><a title="ml_funcs.classifiers_template" href="#ml_funcs.classifiers_template">classifiers_template</a></code></li>
<li><code><a title="ml_funcs.feature_importance_batch" href="#ml_funcs.feature_importance_batch">feature_importance_batch</a></code></li>
<li><code><a title="ml_funcs.gainNlift" href="#ml_funcs.gainNlift">gainNlift</a></code></li>
<li><code><a title="ml_funcs.hyperparameter_tuning" href="#ml_funcs.hyperparameter_tuning">hyperparameter_tuning</a></code></li>
<li><code><a title="ml_funcs.learning_curve_early_stopping" href="#ml_funcs.learning_curve_early_stopping">learning_curve_early_stopping</a></code></li>
<li><code><a title="ml_funcs.ml_comparison" href="#ml_funcs.ml_comparison">ml_comparison</a></code></li>
<li><code><a title="ml_funcs.ml_comparison_plot" href="#ml_funcs.ml_comparison_plot">ml_comparison_plot</a></code></li>
<li><code><a title="ml_funcs.ml_prediction" href="#ml_funcs.ml_prediction">ml_prediction</a></code></li>
<li><code><a title="ml_funcs.ml_prediction_sub_epochs" href="#ml_funcs.ml_prediction_sub_epochs">ml_prediction_sub_epochs</a></code></li>
<li><code><a title="ml_funcs.ml_prediction_xValNest" href="#ml_funcs.ml_prediction_xValNest">ml_prediction_xValNest</a></code></li>
<li><code><a title="ml_funcs.ml_scores" href="#ml_funcs.ml_scores">ml_scores</a></code></li>
<li><code><a title="ml_funcs.ml_scores_crossvalidate" href="#ml_funcs.ml_scores_crossvalidate">ml_scores_crossvalidate</a></code></li>
<li><code><a title="ml_funcs.ml_tuner" href="#ml_funcs.ml_tuner">ml_tuner</a></code></li>
<li><code><a title="ml_funcs.pca_explainedVar" href="#ml_funcs.pca_explainedVar">pca_explainedVar</a></code></li>
<li><code><a title="ml_funcs.pca_important_features" href="#ml_funcs.pca_important_features">pca_important_features</a></code></li>
<li><code><a title="ml_funcs.pca_ortho_rotation" href="#ml_funcs.pca_ortho_rotation">pca_ortho_rotation</a></code></li>
<li><code><a title="ml_funcs.pdp_plot_batch" href="#ml_funcs.pdp_plot_batch">pdp_plot_batch</a></code></li>
<li><code><a title="ml_funcs.plot_confusion_matrix2" href="#ml_funcs.plot_confusion_matrix2">plot_confusion_matrix2</a></code></li>
<li><code><a title="ml_funcs.precision_recall_curve2" href="#ml_funcs.precision_recall_curve2">precision_recall_curve2</a></code></li>
<li><code><a title="ml_funcs.regressors_template" href="#ml_funcs.regressors_template">regressors_template</a></code></li>
<li><code><a title="ml_funcs.reliability_diagram" href="#ml_funcs.reliability_diagram">reliability_diagram</a></code></li>
<li><code><a title="ml_funcs.roc_curve2" href="#ml_funcs.roc_curve2">roc_curve2</a></code></li>
<li><code><a title="ml_funcs.shap_plots_batch" href="#ml_funcs.shap_plots_batch">shap_plots_batch</a></code></li>
<li><code><a title="ml_funcs.xgb_tuner" href="#ml_funcs.xgb_tuner">xgb_tuner</a></code></li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>